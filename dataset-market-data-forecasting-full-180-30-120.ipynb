{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport polars as pl\n\ndef generate_test_data(\n    base_path: str,\n    id_col: str, \n    date_id: str,  # 例: (\"date_id\", int)\n    time_id: str,  # 例: (\"time_id\", int)\n    pred_cols: list, # 例: [(\"responder_0\", float)]\n    input_cols: list,# 例: [(\"feature_00\", float)]\n    num_partitions=4,\n    num_id=2,\n    date_per_partition=2\n):\n    \"\"\"\n    id_col, date_id, time_id, pred_cols, input_colsをもとにテストデータを作成し、\n    partition_id=0～(num_partitions-1)までのParquetファイルに出力する。\n\n    各パーティションは、以下の条件でデータを生成：\n      - id_col: 0～(num_id-1)までのIDを用意\n      - date_id: partition i は [i*date_per_partition, (i+1)*date_per_partition-1]\n      - time_id: 0～24 (25個)\n    よって1パーティションあたり: date_per_partition * num_id * 25行。\n\n    pred_cols, input_colsはタプルのリスト [(col_name, type), ...]で指定し、\n    値は日付・ID・time_idに基づき決定的に割り当てる。\n    \"\"\"\n    \n    os.makedirs(base_path, exist_ok=True)\n\n    # カラム名抽出\n    id_col_name = id_col[0]\n    date_col_name = date_id[0]\n    time_col_name = time_id[0]\n\n    # 1パーティションあたりの行数\n    time_per_day = 25\n    rows_per_partition = date_per_partition * num_id * time_per_day\n\n    # 全ての列名\n    all_col_names = [id_col_name, date_col_name, time_col_name] + [c[0] for c in pred_cols] + [c[0] for c in input_cols]\n\n    for i in range(num_partitions):\n        date_start = i * date_per_partition\n        date_end = date_start + date_per_partition\n\n        # データ生成\n        # 二重三重ループで決定的に割り当て\n        date_ids_list = []\n        id_list = []\n        time_ids_list = []\n\n        for d in range(date_start, date_end):\n            for sid in range(num_id):\n                for t in range(time_per_day):\n                    date_ids_list.append(d)\n                    id_list.append(sid)\n                    time_ids_list.append(t)\n\n        # ベースとなる数値計算用\n        # responder_0やfeature_00に割り当てるための基礎値\n        # 例：val = d*10000 + sid*100 + t\n        base_vals = []\n        for d_id, s_id, t_id in zip(date_ids_list, id_list, time_ids_list):\n            val = d_id * 10000 + s_id * 100 + t_id\n            base_vals.append(val)\n\n        # DataFrame用辞書\n        data = {\n            id_col_name: id_list,\n            date_col_name: date_ids_list,\n            time_col_name: time_ids_list,\n        }\n\n        # pred_cols割り当て（例：responder_0 = base_val + index_of_col）\n        for idx, (c_name, c_type) in enumerate(pred_cols):\n            # 例: responder_0 = base_val + idx\n            c_values = [v + idx for v in base_vals]\n            data[c_name] = c_values\n\n        # input_cols割り当て（例：feature_00 = base_val/1000）\n        for idx, (c_name, c_type) in enumerate(input_cols):\n            # 例: feature_00 = base_val / 1000.0\n            c_values = [v / 1000.0 + idx for v in base_vals]\n            data[c_name] = c_values\n\n        df = pl.DataFrame(data)\n        # ソート（id_col_name, date_col_name, time_col_name順）\n        df = df.sort([id_col_name, date_col_name, time_col_name])\n\n        partition_dir = os.path.join(base_path, f\"partition_id={i}\")\n        os.makedirs(partition_dir, exist_ok=True)\n        df.write_parquet(os.path.join(partition_dir, \"part-0.parquet\"))\n\n    print(\"Test data generated without rows_per_partition, using given columns.\")","metadata":{"_uuid":"f68866a2-e910-4cd1-b613-709d600662f0","_cell_guid":"d56e7f40-d49e-4a03-89df-31470a3737ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.336829Z","iopub.execute_input":"2024-12-23T09:42:28.337248Z","iopub.status.idle":"2024-12-23T09:42:28.349643Z","shell.execute_reply.started":"2024-12-23T09:42:28.337212Z","shell.execute_reply":"2024-12-23T09:42:28.348391Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# 入力変数選択\nid_col = (\"symbol_id\", int)\ndate_id = (\"date_id\", int)\ntime_id = (\"time_id\", int)\n\n#preds_cols = [f\"responder_{i}\" for i in range(9)] # 目的変数\npred_cols = [(\"responder_0\", float)]\ninput_cols = [(\"feature_00\", float)] # 説明変数\n\ntarget_cols = [id_col, date_id, time_id] + pred_cols + input_cols\ntarget_cols","metadata":{"_uuid":"6f344a3d-07fd-47b5-94c0-d2957fb84ecd","_cell_guid":"1283520c-2194-4f05-8452-190318a611f7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.367833Z","iopub.execute_input":"2024-12-23T09:42:28.368228Z","iopub.status.idle":"2024-12-23T09:42:28.377257Z","shell.execute_reply.started":"2024-12-23T09:42:28.368191Z","shell.execute_reply":"2024-12-23T09:42:28.376019Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[('symbol_id', int),\n ('date_id', int),\n ('time_id', int),\n ('responder_0', float),\n ('feature_00', float)]"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"generate_test_data(\"test\", id_col, date_id, time_id, pred_cols, input_cols, num_partitions=2, num_id=1, date_per_partition=3)","metadata":{"_uuid":"1978295c-7d27-45ba-a755-0a53e8f742f6","_cell_guid":"cb46278d-0a29-4f3a-8a8b-973239921d9f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.379237Z","iopub.execute_input":"2024-12-23T09:42:28.380320Z","iopub.status.idle":"2024-12-23T09:42:28.395829Z","shell.execute_reply.started":"2024-12-23T09:42:28.380266Z","shell.execute_reply":"2024-12-23T09:42:28.394543Z"}},"outputs":[{"name":"stdout","text":"Test data generated without rows_per_partition, using given columns.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"df = (\n    pl.scan_parquet(\"/kaggle/working/test/partition_id=*/part-*.parquet\", glob=True)\n      .filter(pl.col(\"symbol_id\") == 0)\n      .collect()\n)\ndf","metadata":{"_uuid":"203cd0c7-f167-40be-b6e6-55415500f526","_cell_guid":"43f495ad-8380-4271-97c3-9e8d32ed4666","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.407276Z","iopub.execute_input":"2024-12-23T09:42:28.407698Z","iopub.status.idle":"2024-12-23T09:42:28.417718Z","shell.execute_reply.started":"2024-12-23T09:42:28.407660Z","shell.execute_reply":"2024-12-23T09:42:28.416692Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"shape: (150, 5)\n┌───────────┬─────────┬─────────┬─────────────┬────────────┐\n│ symbol_id ┆ date_id ┆ time_id ┆ responder_0 ┆ feature_00 │\n│ ---       ┆ ---     ┆ ---     ┆ ---         ┆ ---        │\n│ i64       ┆ i64     ┆ i64     ┆ i64         ┆ f64        │\n╞═══════════╪═════════╪═════════╪═════════════╪════════════╡\n│ 0         ┆ 0       ┆ 0       ┆ 0           ┆ 0.0        │\n│ 0         ┆ 0       ┆ 1       ┆ 1           ┆ 0.001      │\n│ 0         ┆ 0       ┆ 2       ┆ 2           ┆ 0.002      │\n│ 0         ┆ 0       ┆ 3       ┆ 3           ┆ 0.003      │\n│ 0         ┆ 0       ┆ 4       ┆ 4           ┆ 0.004      │\n│ …         ┆ …       ┆ …       ┆ …           ┆ …          │\n│ 0         ┆ 5       ┆ 20      ┆ 50020       ┆ 50.02      │\n│ 0         ┆ 5       ┆ 21      ┆ 50021       ┆ 50.021     │\n│ 0         ┆ 5       ┆ 22      ┆ 50022       ┆ 50.022     │\n│ 0         ┆ 5       ┆ 23      ┆ 50023       ┆ 50.023     │\n│ 0         ┆ 5       ┆ 24      ┆ 50024       ┆ 50.024     │\n└───────────┴─────────┴─────────┴─────────────┴────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (150, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>0.001</td></tr><tr><td>0</td><td>0</td><td>2</td><td>2</td><td>0.002</td></tr><tr><td>0</td><td>0</td><td>3</td><td>3</td><td>0.003</td></tr><tr><td>0</td><td>0</td><td>4</td><td>4</td><td>0.004</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>5</td><td>20</td><td>50020</td><td>50.02</td></tr><tr><td>0</td><td>5</td><td>21</td><td>50021</td><td>50.021</td></tr><tr><td>0</td><td>5</td><td>22</td><td>50022</td><td>50.022</td></tr><tr><td>0</td><td>5</td><td>23</td><td>50023</td><td>50.023</td></tr><tr><td>0</td><td>5</td><td>24</td><td>50024</td><td>50.024</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import os\nimport glob\nimport polars as pl\nimport numpy as np\nimport gc\nimport time\n\ndef create_metadata(input_base):\n    parquet_files = sorted(glob.glob(os.path.join(input_base, \"partition_id=*\", \"*.parquet\")))\n    meta = []\n    for pi, f in enumerate(parquet_files):\n        df_all = pl.read_parquet(f, columns=[\"date_id\"])\n        min_id = df_all.select(pl.col(\"date_id\").min()).item()\n        max_id = df_all.select(pl.col(\"date_id\").max()).item()\n        meta.append((pi, min_id, max_id))\n    print(\"meta: partition, min date, max date\", meta)\n    return meta\n\ndef partitions_for_range(meta, start_id: int, end_id: int):\n    needed = []\n    for (pi, mini, maxi) in meta:\n        if maxi >= start_id and mini <= end_id:\n            needed.append(pi)\n    return needed","metadata":{"_uuid":"d64383d6-e1e7-4708-8dce-e78792b2ea30","_cell_guid":"5e21ac12-188d-47e5-99f7-254e38eeb624","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.431889Z","iopub.execute_input":"2024-12-23T09:42:28.432433Z","iopub.status.idle":"2024-12-23T09:42:28.440628Z","shell.execute_reply.started":"2024-12-23T09:42:28.432368Z","shell.execute_reply":"2024-12-23T09:42:28.439404Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nimport glob\nimport gc\nimport polars as pl\n\ndef run_walk_forward(\n    input_base: str,\n    output_base: str,\n    id_col, date_id, time_id, pred_cols, input_cols,\n    train_length, valid_length, train_shift, retroactive_size,\n    add_feature_func\n):\n    \"\"\"\n    1) Kaggleパーティションファイルを読み込み、指定期間でtrainとvalidを作る。\n    2) add_feature_funcの処理の前にgc.collect()でメモリ解放を試みる。\n    3) 前回と今回の generated_feature, generated_predsに差分がある場合にExceptionを投げる。\n    4) 書き出し時に zstd圧縮(level=9)を指定。\n    \n    Returns:\n        (last_generated_feature, last_generated_preds)\n    \"\"\"\n    original_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n    print(\"original_cols\", original_cols)\n\n    meta = create_metadata(input_base)\n    max_date_id_global = max(m[2] for m in meta)\n\n    start_of_train = 0\n    sprint_num = 1\n\n    parquet_files = sorted(glob.glob(os.path.join(input_base, \"partition_id=*\", \"*.parquet\")))\n\n    # 前回のgeneratedを記録する変数(Noneで初期化)\n    prev_gen_feature_str = None\n    prev_gen_preds_str = None\n\n    generated_feature = None\n    generated_preds = None\n\n    while True:\n        if start_of_train > max_date_id_global:\n            break\n\n        train_start = start_of_train\n        train_end = start_of_train + train_length\n        valid_start = train_end\n        valid_end = train_end + valid_length\n\n        if valid_start > max_date_id_global:\n            break\n        if valid_end > max_date_id_global + 1:\n            valid_end = max_date_id_global + 1\n\n        extended_start = max(train_start - retroactive_size, 0)\n        required_start = extended_start\n        required_end = valid_end - 1\n\n        needed_partitions = partitions_for_range(meta, required_start, required_end)\n        if len(needed_partitions) == 0:\n            break\n\n        df_current = None\n        for p in needed_partitions:\n            parquet_files_matched = [f for f in parquet_files if f\"partition_id={p}\" in f]\n            for fpart in parquet_files_matched:\n                print(\"read_parquet\", fpart)\n                # 存在しない列がある場合に備えてスキーマ確認\n                df_schema = pl.read_parquet(fpart, n_rows=1).schema\n                actual_cols = df_schema.keys()\n                cols_to_read = []\n                for col in original_cols:\n                    if isinstance(col, tuple):\n                        col_name = col[0]\n                    else:\n                        col_name = col\n                    if col_name in actual_cols:\n                        cols_to_read.append(col_name)\n                \n                df_part = pl.read_parquet(fpart, columns=cols_to_read)\n                if df_current is None:\n                    df_current = df_part\n                else:\n                    df_current = pl.concat([df_current, df_part], how=\"vertical\")\n\n        if df_current is None:\n            break\n\n        print(\"extended_start\", extended_start)\n        print(\"valid_end\", valid_end)\n\n        df_current = df_current.filter((pl.col(\"date_id\") >= extended_start) & (pl.col(\"date_id\") < valid_end))\n        \n        # 1) add_feature_func の実行前にメモリ解放\n        gc.collect()\n\n        # 2) 特徴量生成\n        start_time = time.time()\n        df_current, generated_feature, generated_preds = add_feature_func(\n            df_current,\n            pred_cols=pred_cols,\n            input_cols=input_cols,\n            original_cols=[col[0] if isinstance(col, tuple) else col for col in original_cols]\n        )\n        end_time = time.time()\n        print(f\"add_feature_func Execution time: {end_time - start_time:.4f} seconds\")\n\n        # 3) 前回と今回の generated_feature/generated_predsを比較\n        # リストのリストであるため、repr()で簡易比較\n        print(\"Check generated col diff.\")\n        current_gen_feature_str = repr(generated_feature)\n        current_gen_preds_str   = repr(generated_preds)\n        if prev_gen_feature_str is not None:\n            # 差分があるかどうか判定\n            if current_gen_feature_str != prev_gen_feature_str:\n                raise Exception(\"generated_feature differs from previous sprint!\")\n        if prev_gen_preds_str is not None:\n            if current_gen_preds_str != prev_gen_preds_str:\n                raise Exception(\"generated_preds differs from previous sprint!\")\n\n        # 今回を前回として保存\n        prev_gen_feature_str = current_gen_feature_str\n        prev_gen_preds_str   = current_gen_preds_str\n\n        # train/valid抽出\n        print(\"Check generated col diff.\")\n        train_df = df_current.filter((pl.col(\"date_id\") >= train_start) & (pl.col(\"date_id\") < train_end))\n        valid_df = df_current.filter((pl.col(\"date_id\") >= valid_start) & (pl.col(\"date_id\") < valid_end))\n\n        sprint_dir = os.path.join(output_base, f\"sprint{sprint_num}\")\n        os.makedirs(sprint_dir, exist_ok=True)\n\n        train_path = os.path.join(sprint_dir, \"train.parquet\")\n        valid_path = os.path.join(sprint_dir, \"valid.parquet\")\n\n        print(\"write_parquet start.\")\n        gc.collect()\n        start_time = time.time()\n        train_df.collect().write_parquet(train_path, compression=\"zstd\")\n        #train_df.sink_parquet(train_path, compression=\"snappy\")\n        gc.collect()\n\n        valid_df.collect().write_parquet(valid_path, compression=\"zstd\")\n        #valid_df.sink_parquet(valid_path, compression=\"snappy\")\n        end_time = time.time()\n        print(f\"  write_parquet Execution time: {end_time - start_time:.4f} seconds\")\n\n        print(f\"Sprint {sprint_num}:\")\n        print(f\"  Train: date_id in [{train_start}, {train_end}) -> {train_path}\")\n        print(f\"  Valid: date_id in [{valid_start}, {valid_end}) -> {valid_path}\")\n        print(f\"  Saved to {sprint_dir}\\n\")\n\n        del df_current, train_df, valid_df\n        gc.collect()\n\n        start_of_train += train_shift\n        sprint_num += 1\n\n    print(\"Done.\")\n    gc.collect()\n\n    return generated_feature, generated_preds","metadata":{"_uuid":"58fd0698-e4de-4725-92fd-0844c061c56a","_cell_guid":"c6439978-96fc-4a1b-9a91-7d79e70c1db2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.564429Z","iopub.execute_input":"2024-12-23T09:42:28.564847Z","iopub.status.idle":"2024-12-23T09:42:28.584474Z","shell.execute_reply.started":"2024-12-23T09:42:28.564810Z","shell.execute_reply":"2024-12-23T09:42:28.583483Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def add_rolling_preds(df: pl.DataFrame, pred_cols: list[str]) -> pl.DataFrame:\n    \"\"\"\n    \"\"\"\n    agg_expr = []\n    for c in pred_cols:\n        agg_expr.append(pl.col(c).first().alias(f\"{c}_prev_first\"))\n        agg_expr.append(pl.col(c).last().alias(f\"{c}_prev_last\"))\n        agg_expr.append(pl.col(c).max().alias(f\"{c}_prev_max\"))\n        agg_expr.append(pl.col(c).min().alias(f\"{c}_prev_min\"))\n        agg_expr.append(pl.col(c).std().alias(f\"{c}_prev_std\"))\n\n    return (\n        df.group_by([\"symbol_id\", \"date_id\"])\n        .agg(agg_expr)\n        .with_columns((pl.col(\"date_id\") + 1).alias(\"date_id_next\"))\n        .drop(\"date_id\")\n        .rename({\"date_id_next\": \"date_id\"})\n    )\n\nwindow_size = 7\ndef add_feature(\n    df: pl.DataFrame, \n    pred_cols, \n    input_cols,\n    original_cols\n) -> (pl.DataFrame, pl.DataFrame):\n    # add_rolling_preds_funcを使用してprev_day_aggs相当を計算\n\n    df = df.lazy()\n    \n    df_prev = add_rolling_preds(df, pred_cols)\n    df = df.join(df_prev, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n    df = df.sort([\"symbol_id\",\"date_id\",\"time_id\"])\n\n    # pred_cols[0]でrolling_meanする例\n    first_pred = pred_cols[0]\n    rolling_col = f\"{first_pred}_prev_last\"\n\n    df = df.with_columns(\n        pl.col(rolling_col)\n        .rolling_mean(window_size=window_size)\n        .over(\"symbol_id\")\n        .alias(f\"{rolling_col}_{window_size}day_mean\")\n    )\n    return df, [], []","metadata":{"_uuid":"ea4430b5-8fd6-47f2-b300-191a399462ad","_cell_guid":"adfe1266-3f38-4319-8ea8-5e9ce9effc64","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.601589Z","iopub.execute_input":"2024-12-23T09:42:28.602663Z","iopub.status.idle":"2024-12-23T09:42:28.612136Z","shell.execute_reply.started":"2024-12-23T09:42:28.602585Z","shell.execute_reply":"2024-12-23T09:42:28.611051Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"output_base = \"test_datasets\"\nos.makedirs(output_base, exist_ok=True)\n\ntest_train_length = 2\ntest_valid_length = 1\ntest_train_shift = 1\ntest_retroactive_size = 1\n\nrun_walk_forward(\n    input_base=\"/kaggle/working/test\",\n    output_base=output_base,\n    id_col=id_col[0],\n    date_id=date_id[0], \n    time_id=time_id[0],\n    pred_cols=[c[0] for c in pred_cols],\n    input_cols=[c[0] for c in input_cols],\n    train_length=test_train_length,\n    valid_length=test_valid_length,\n    train_shift=test_train_shift, \n    retroactive_size=test_retroactive_size,\n    add_feature_func=add_feature\n)","metadata":{"_uuid":"aa78cf22-4193-49f0-887c-a37ad37cb705","_cell_guid":"2ae1ade6-192a-48d4-b44e-7076d361c859","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:28.643521Z","iopub.execute_input":"2024-12-23T09:42:28.644189Z","iopub.status.idle":"2024-12-23T09:42:29.766625Z","shell.execute_reply.started":"2024-12-23T09:42:28.644152Z","shell.execute_reply":"2024-12-23T09:42:29.765362Z"}},"outputs":[{"name":"stdout","text":"original_cols ['symbol_id', 'date_id', 'time_id', 'responder_0', 'feature_00']\nmeta: partition, min date, max date [(0, 0, 2), (1, 3, 5)]\nread_parquet /kaggle/working/test/partition_id=0/part-0.parquet\nextended_start 0\nvalid_end 3\nadd_feature_func Execution time: 0.0003 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n  write_parquet Execution time: 0.0740 seconds\nSprint 1:\n  Train: date_id in [0, 2) -> test_datasets/sprint1/train.parquet\n  Valid: date_id in [2, 3) -> test_datasets/sprint1/valid.parquet\n  Saved to test_datasets/sprint1\n\nread_parquet /kaggle/working/test/partition_id=0/part-0.parquet\nread_parquet /kaggle/working/test/partition_id=1/part-0.parquet\nextended_start 0\nvalid_end 4\nadd_feature_func Execution time: 0.0004 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n  write_parquet Execution time: 0.0781 seconds\nSprint 2:\n  Train: date_id in [1, 3) -> test_datasets/sprint2/train.parquet\n  Valid: date_id in [3, 4) -> test_datasets/sprint2/valid.parquet\n  Saved to test_datasets/sprint2\n\nread_parquet /kaggle/working/test/partition_id=0/part-0.parquet\nread_parquet /kaggle/working/test/partition_id=1/part-0.parquet\nextended_start 1\nvalid_end 5\nadd_feature_func Execution time: 0.0004 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n  write_parquet Execution time: 0.0707 seconds\nSprint 3:\n  Train: date_id in [2, 4) -> test_datasets/sprint3/train.parquet\n  Valid: date_id in [4, 5) -> test_datasets/sprint3/valid.parquet\n  Saved to test_datasets/sprint3\n\nread_parquet /kaggle/working/test/partition_id=0/part-0.parquet\nread_parquet /kaggle/working/test/partition_id=1/part-0.parquet\nextended_start 2\nvalid_end 6\nadd_feature_func Execution time: 0.0003 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n  write_parquet Execution time: 0.0706 seconds\nSprint 4:\n  Train: date_id in [3, 5) -> test_datasets/sprint4/train.parquet\n  Valid: date_id in [5, 6) -> test_datasets/sprint4/valid.parquet\n  Saved to test_datasets/sprint4\n\nDone.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"([], [])"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"train1 = pl.read_parquet(\"/kaggle/working/test_datasets/sprint1/train.parquet\")\ntrain1.filter(pl.col(\"symbol_id\") == 0)","metadata":{"_uuid":"c1c00a90-23c0-439c-9930-7f01e1fef04d","_cell_guid":"4c43c028-81db-4f39-8081-2b87498dfd65","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:29.768364Z","iopub.execute_input":"2024-12-23T09:42:29.768752Z","iopub.status.idle":"2024-12-23T09:42:29.779858Z","shell.execute_reply.started":"2024-12-23T09:42:29.768717Z","shell.execute_reply":"2024-12-23T09:42:29.778338Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"shape: (50, 11)\n┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ symbol_id ┆ date_id ┆ time_id ┆ responder_ ┆ … ┆ responder_ ┆ responder_ ┆ responder ┆ responder │\n│ ---       ┆ ---     ┆ ---     ┆ 0          ┆   ┆ 0_prev_max ┆ 0_prev_min ┆ _0_prev_s ┆ _0_prev_l │\n│ i64       ┆ i64     ┆ i64     ┆ ---        ┆   ┆ ---        ┆ ---        ┆ td        ┆ ast_7day_ │\n│           ┆         ┆         ┆ i64        ┆   ┆ i64        ┆ i64        ┆ ---       ┆ mea…      │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆ f64       ┆ ---       │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ f64       │\n╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 0         ┆ 0       ┆ 0       ┆ 0          ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n│ 0         ┆ 0       ┆ 1       ┆ 1          ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n│ 0         ┆ 0       ┆ 2       ┆ 2          ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n│ 0         ┆ 0       ┆ 3       ┆ 3          ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n│ 0         ┆ 0       ┆ 4       ┆ 4          ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n│ …         ┆ …       ┆ …       ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 0         ┆ 1       ┆ 20      ┆ 10020      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ 24.0      │\n│ 0         ┆ 1       ┆ 21      ┆ 10021      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ 24.0      │\n│ 0         ┆ 1       ┆ 22      ┆ 10022      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ 24.0      │\n│ 0         ┆ 1       ┆ 23      ┆ 10023      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ 24.0      │\n│ 0         ┆ 1       ┆ 24      ┆ 10024      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ 24.0      │\n└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (50, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th><th>responder_0_prev_first</th><th>responder_0_prev_last</th><th>responder_0_prev_max</th><th>responder_0_prev_min</th><th>responder_0_prev_std</th><th>responder_0_prev_last_7day_mean</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>0.001</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>2</td><td>2</td><td>0.002</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>3</td><td>3</td><td>0.003</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0</td><td>0</td><td>4</td><td>4</td><td>0.004</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>1</td><td>20</td><td>10020</td><td>10.02</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>24.0</td></tr><tr><td>0</td><td>1</td><td>21</td><td>10021</td><td>10.021</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>24.0</td></tr><tr><td>0</td><td>1</td><td>22</td><td>10022</td><td>10.022</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>24.0</td></tr><tr><td>0</td><td>1</td><td>23</td><td>10023</td><td>10.023</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>24.0</td></tr><tr><td>0</td><td>1</td><td>24</td><td>10024</td><td>10.024</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>24.0</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"valid1 = pl.read_parquet(\"/kaggle/working/test_datasets/sprint1/valid.parquet\")\nvalid1.filter(pl.col(\"symbol_id\") == 0)","metadata":{"_uuid":"424586fd-d96f-4b13-9952-6c7e8d836142","_cell_guid":"80e4d853-ce65-4507-8b22-bd761ee0e20a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:29.780996Z","iopub.execute_input":"2024-12-23T09:42:29.781434Z","iopub.status.idle":"2024-12-23T09:42:29.800144Z","shell.execute_reply.started":"2024-12-23T09:42:29.781399Z","shell.execute_reply":"2024-12-23T09:42:29.798923Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"shape: (25, 11)\n┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ symbol_id ┆ date_id ┆ time_id ┆ responder_ ┆ … ┆ responder_ ┆ responder_ ┆ responder ┆ responder │\n│ ---       ┆ ---     ┆ ---     ┆ 0          ┆   ┆ 0_prev_max ┆ 0_prev_min ┆ _0_prev_s ┆ _0_prev_l │\n│ i64       ┆ i64     ┆ i64     ┆ ---        ┆   ┆ ---        ┆ ---        ┆ td        ┆ ast_7day_ │\n│           ┆         ┆         ┆ i64        ┆   ┆ i64        ┆ i64        ┆ ---       ┆ mea…      │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆ f64       ┆ ---       │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ f64       │\n╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 0         ┆ 2       ┆ 0       ┆ 20000      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 1452.5714 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 29        │\n│ 0         ┆ 2       ┆ 1       ┆ 20001      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 2881.1428 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 57        │\n│ 0         ┆ 2       ┆ 2       ┆ 20002      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 4309.7142 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 86        │\n│ 0         ┆ 2       ┆ 3       ┆ 20003      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 5738.2857 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 14        │\n│ 0         ┆ 2       ┆ 4       ┆ 20004      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 7166.8571 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 43        │\n│ …         ┆ …       ┆ …       ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 0         ┆ 2       ┆ 20      ┆ 20020      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 21      ┆ 20021      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 22      ┆ 20022      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 23      ┆ 20023      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 24      ┆ 20024      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (25, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th><th>responder_0_prev_first</th><th>responder_0_prev_last</th><th>responder_0_prev_max</th><th>responder_0_prev_min</th><th>responder_0_prev_std</th><th>responder_0_prev_last_7day_mean</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>2</td><td>0</td><td>20000</td><td>20.0</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>1452.571429</td></tr><tr><td>0</td><td>2</td><td>1</td><td>20001</td><td>20.001</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>2881.142857</td></tr><tr><td>0</td><td>2</td><td>2</td><td>20002</td><td>20.002</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>4309.714286</td></tr><tr><td>0</td><td>2</td><td>3</td><td>20003</td><td>20.003</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>5738.285714</td></tr><tr><td>0</td><td>2</td><td>4</td><td>20004</td><td>20.004</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>7166.857143</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>2</td><td>20</td><td>20020</td><td>20.02</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>21</td><td>20021</td><td>20.021</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>22</td><td>20022</td><td>20.022</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>23</td><td>20023</td><td>20.023</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>24</td><td>20024</td><td>20.024</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"train2 = pl.read_parquet(\"/kaggle/working/test_datasets/sprint2/train.parquet\")\ntrain2.filter(pl.col(\"symbol_id\") == 0)","metadata":{"_uuid":"2365b884-1a2d-4465-905a-88916e8968fa","_cell_guid":"e299b898-b9b2-4c31-9b3e-b078ab4154ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:29.801540Z","iopub.execute_input":"2024-12-23T09:42:29.801882Z","iopub.status.idle":"2024-12-23T09:42:29.815853Z","shell.execute_reply.started":"2024-12-23T09:42:29.801850Z","shell.execute_reply":"2024-12-23T09:42:29.814709Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"shape: (50, 11)\n┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ symbol_id ┆ date_id ┆ time_id ┆ responder_ ┆ … ┆ responder_ ┆ responder_ ┆ responder ┆ responder │\n│ ---       ┆ ---     ┆ ---     ┆ 0          ┆   ┆ 0_prev_max ┆ 0_prev_min ┆ _0_prev_s ┆ _0_prev_l │\n│ i64       ┆ i64     ┆ i64     ┆ ---        ┆   ┆ ---        ┆ ---        ┆ td        ┆ ast_7day_ │\n│           ┆         ┆         ┆ i64        ┆   ┆ i64        ┆ i64        ┆ ---       ┆ mea…      │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆ f64       ┆ ---       │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ f64       │\n╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 0         ┆ 1       ┆ 0       ┆ 10000      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ null      │\n│ 0         ┆ 1       ┆ 1       ┆ 10001      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ null      │\n│ 0         ┆ 1       ┆ 2       ┆ 10002      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ null      │\n│ 0         ┆ 1       ┆ 3       ┆ 10003      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ null      │\n│ 0         ┆ 1       ┆ 4       ┆ 10004      ┆ … ┆ 24         ┆ 0          ┆ 7.359801  ┆ null      │\n│ …         ┆ …       ┆ …       ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 0         ┆ 2       ┆ 20      ┆ 20020      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 21      ┆ 20021      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 22      ┆ 20022      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 23      ┆ 20023      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n│ 0         ┆ 2       ┆ 24      ┆ 20024      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ 10024.0   │\n└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (50, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th><th>responder_0_prev_first</th><th>responder_0_prev_last</th><th>responder_0_prev_max</th><th>responder_0_prev_min</th><th>responder_0_prev_std</th><th>responder_0_prev_last_7day_mean</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>1</td><td>0</td><td>10000</td><td>10.0</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>1</td><td>1</td><td>10001</td><td>10.001</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>1</td><td>2</td><td>10002</td><td>10.002</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>1</td><td>3</td><td>10003</td><td>10.003</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>1</td><td>4</td><td>10004</td><td>10.004</td><td>0</td><td>24</td><td>24</td><td>0</td><td>7.359801</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>2</td><td>20</td><td>20020</td><td>20.02</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>21</td><td>20021</td><td>20.021</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>22</td><td>20022</td><td>20.022</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>23</td><td>20023</td><td>20.023</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr><tr><td>0</td><td>2</td><td>24</td><td>20024</td><td>20.024</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>10024.0</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"valid2 = pl.read_parquet(\"/kaggle/working/test_datasets/sprint2/valid.parquet\")\nvalid2.filter(pl.col(\"symbol_id\") == 0)","metadata":{"_uuid":"c71efc4d-2e63-422e-bbd0-53432b081b1f","_cell_guid":"aaa0d5fb-a90e-45a0-b161-9f9b034f988c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:29.818565Z","iopub.execute_input":"2024-12-23T09:42:29.818991Z","iopub.status.idle":"2024-12-23T09:42:29.833946Z","shell.execute_reply.started":"2024-12-23T09:42:29.818955Z","shell.execute_reply":"2024-12-23T09:42:29.832710Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"shape: (25, 11)\n┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ symbol_id ┆ date_id ┆ time_id ┆ responder_ ┆ … ┆ responder_ ┆ responder_ ┆ responder ┆ responder │\n│ ---       ┆ ---     ┆ ---     ┆ 0          ┆   ┆ 0_prev_max ┆ 0_prev_min ┆ _0_prev_s ┆ _0_prev_l │\n│ i64       ┆ i64     ┆ i64     ┆ ---        ┆   ┆ ---        ┆ ---        ┆ td        ┆ ast_7day_ │\n│           ┆         ┆         ┆ i64        ┆   ┆ i64        ┆ i64        ┆ ---       ┆ mea…      │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆ f64       ┆ ---       │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ f64       │\n╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 0         ┆ 3       ┆ 0       ┆ 30000      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 11452.571 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 429       │\n│ 0         ┆ 3       ┆ 1       ┆ 30001      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 12881.142 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 857       │\n│ 0         ┆ 3       ┆ 2       ┆ 30002      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 14309.714 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 286       │\n│ 0         ┆ 3       ┆ 3       ┆ 30003      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 15738.285 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 714       │\n│ 0         ┆ 3       ┆ 4       ┆ 30004      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 17166.857 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 143       │\n│ …         ┆ …       ┆ …       ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 0         ┆ 3       ┆ 20      ┆ 30020      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 21      ┆ 30021      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 22      ┆ 30022      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 23      ┆ 30023      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 24      ┆ 30024      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (25, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th><th>responder_0_prev_first</th><th>responder_0_prev_last</th><th>responder_0_prev_max</th><th>responder_0_prev_min</th><th>responder_0_prev_std</th><th>responder_0_prev_last_7day_mean</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>3</td><td>0</td><td>30000</td><td>30.0</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>11452.571429</td></tr><tr><td>0</td><td>3</td><td>1</td><td>30001</td><td>30.001</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>12881.142857</td></tr><tr><td>0</td><td>3</td><td>2</td><td>30002</td><td>30.002</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>14309.714286</td></tr><tr><td>0</td><td>3</td><td>3</td><td>30003</td><td>30.003</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>15738.285714</td></tr><tr><td>0</td><td>3</td><td>4</td><td>30004</td><td>30.004</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>17166.857143</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>3</td><td>20</td><td>30020</td><td>30.02</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>21</td><td>30021</td><td>30.021</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>22</td><td>30022</td><td>30.022</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>23</td><td>30023</td><td>30.023</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>24</td><td>30024</td><td>30.024</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"train3 = pl.read_parquet(\"/kaggle/working/test_datasets/sprint3/train.parquet\")\ntrain3.filter(pl.col(\"symbol_id\") == 0)","metadata":{"_uuid":"37a4e2b7-b215-4062-afb6-6f33cefbaf13","_cell_guid":"245578a2-538a-4f33-8aec-185d84b21c39","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:29.835285Z","iopub.execute_input":"2024-12-23T09:42:29.835663Z","iopub.status.idle":"2024-12-23T09:42:29.853193Z","shell.execute_reply.started":"2024-12-23T09:42:29.835589Z","shell.execute_reply":"2024-12-23T09:42:29.851962Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"shape: (50, 11)\n┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ symbol_id ┆ date_id ┆ time_id ┆ responder_ ┆ … ┆ responder_ ┆ responder_ ┆ responder ┆ responder │\n│ ---       ┆ ---     ┆ ---     ┆ 0          ┆   ┆ 0_prev_max ┆ 0_prev_min ┆ _0_prev_s ┆ _0_prev_l │\n│ i64       ┆ i64     ┆ i64     ┆ ---        ┆   ┆ ---        ┆ ---        ┆ td        ┆ ast_7day_ │\n│           ┆         ┆         ┆ i64        ┆   ┆ i64        ┆ i64        ┆ ---       ┆ mea…      │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆ f64       ┆ ---       │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ f64       │\n╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 0         ┆ 2       ┆ 0       ┆ 20000      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ null      │\n│ 0         ┆ 2       ┆ 1       ┆ 20001      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ null      │\n│ 0         ┆ 2       ┆ 2       ┆ 20002      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ null      │\n│ 0         ┆ 2       ┆ 3       ┆ 20003      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ null      │\n│ 0         ┆ 2       ┆ 4       ┆ 20004      ┆ … ┆ 10024      ┆ 10000      ┆ 7.359801  ┆ null      │\n│ …         ┆ …       ┆ …       ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 0         ┆ 3       ┆ 20      ┆ 30020      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 21      ┆ 30021      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 22      ┆ 30022      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 23      ┆ 30023      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n│ 0         ┆ 3       ┆ 24      ┆ 30024      ┆ … ┆ 20024      ┆ 20000      ┆ 7.359801  ┆ 20024.0   │\n└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (50, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th><th>responder_0_prev_first</th><th>responder_0_prev_last</th><th>responder_0_prev_max</th><th>responder_0_prev_min</th><th>responder_0_prev_std</th><th>responder_0_prev_last_7day_mean</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>2</td><td>0</td><td>20000</td><td>20.0</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>2</td><td>1</td><td>20001</td><td>20.001</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>2</td><td>2</td><td>20002</td><td>20.002</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>2</td><td>3</td><td>20003</td><td>20.003</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>null</td></tr><tr><td>0</td><td>2</td><td>4</td><td>20004</td><td>20.004</td><td>10000</td><td>10024</td><td>10024</td><td>10000</td><td>7.359801</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>3</td><td>20</td><td>30020</td><td>30.02</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>21</td><td>30021</td><td>30.021</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>22</td><td>30022</td><td>30.022</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>23</td><td>30023</td><td>30.023</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr><tr><td>0</td><td>3</td><td>24</td><td>30024</td><td>30.024</td><td>20000</td><td>20024</td><td>20024</td><td>20000</td><td>7.359801</td><td>20024.0</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"valid3 = pl.read_parquet(\"/kaggle/working/test_datasets/sprint3/valid.parquet\")\nvalid3.filter(pl.col(\"symbol_id\") == 0)","metadata":{"_uuid":"f949f70c-f309-4e15-af1f-8f30785bed9b","_cell_guid":"ddccb027-f79a-4e7e-a3ff-c95c7fa0980d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:29.854675Z","iopub.execute_input":"2024-12-23T09:42:29.855034Z","iopub.status.idle":"2024-12-23T09:42:29.867932Z","shell.execute_reply.started":"2024-12-23T09:42:29.855002Z","shell.execute_reply":"2024-12-23T09:42:29.866547Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"shape: (25, 11)\n┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ symbol_id ┆ date_id ┆ time_id ┆ responder_ ┆ … ┆ responder_ ┆ responder_ ┆ responder ┆ responder │\n│ ---       ┆ ---     ┆ ---     ┆ 0          ┆   ┆ 0_prev_max ┆ 0_prev_min ┆ _0_prev_s ┆ _0_prev_l │\n│ i64       ┆ i64     ┆ i64     ┆ ---        ┆   ┆ ---        ┆ ---        ┆ td        ┆ ast_7day_ │\n│           ┆         ┆         ┆ i64        ┆   ┆ i64        ┆ i64        ┆ ---       ┆ mea…      │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆ f64       ┆ ---       │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ f64       │\n╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 0         ┆ 4       ┆ 0       ┆ 40000      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 21452.571 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 429       │\n│ 0         ┆ 4       ┆ 1       ┆ 40001      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 22881.142 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 857       │\n│ 0         ┆ 4       ┆ 2       ┆ 40002      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 24309.714 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 286       │\n│ 0         ┆ 4       ┆ 3       ┆ 40003      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 25738.285 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 714       │\n│ 0         ┆ 4       ┆ 4       ┆ 40004      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 27166.857 │\n│           ┆         ┆         ┆            ┆   ┆            ┆            ┆           ┆ 143       │\n│ …         ┆ …       ┆ …       ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 0         ┆ 4       ┆ 20      ┆ 40020      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 30024.0   │\n│ 0         ┆ 4       ┆ 21      ┆ 40021      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 30024.0   │\n│ 0         ┆ 4       ┆ 22      ┆ 40022      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 30024.0   │\n│ 0         ┆ 4       ┆ 23      ┆ 40023      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 30024.0   │\n│ 0         ┆ 4       ┆ 24      ┆ 40024      ┆ … ┆ 30024      ┆ 30000      ┆ 7.359801  ┆ 30024.0   │\n└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (25, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th><th>responder_0_prev_first</th><th>responder_0_prev_last</th><th>responder_0_prev_max</th><th>responder_0_prev_min</th><th>responder_0_prev_std</th><th>responder_0_prev_last_7day_mean</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>4</td><td>0</td><td>40000</td><td>40.0</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>21452.571429</td></tr><tr><td>0</td><td>4</td><td>1</td><td>40001</td><td>40.001</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>22881.142857</td></tr><tr><td>0</td><td>4</td><td>2</td><td>40002</td><td>40.002</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>24309.714286</td></tr><tr><td>0</td><td>4</td><td>3</td><td>40003</td><td>40.003</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>25738.285714</td></tr><tr><td>0</td><td>4</td><td>4</td><td>40004</td><td>40.004</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>27166.857143</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>4</td><td>20</td><td>40020</td><td>40.02</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>30024.0</td></tr><tr><td>0</td><td>4</td><td>21</td><td>40021</td><td>40.021</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>30024.0</td></tr><tr><td>0</td><td>4</td><td>22</td><td>40022</td><td>40.022</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>30024.0</td></tr><tr><td>0</td><td>4</td><td>23</td><td>40023</td><td>40.023</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>30024.0</td></tr><tr><td>0</td><td>4</td><td>24</td><td>40024</td><td>40.024</td><td>30000</td><td>30024</td><td>30024</td><td>30000</td><td>7.359801</td><td>30024.0</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"import os\nimport glob\nimport polars as pl\nimport numpy as np\n\noutput_base = \"datasets\"\nos.makedirs(output_base, exist_ok=True)\n\ndata_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\"\nparquet_files = sorted(glob.glob(os.path.join(data_path, \"partition_id=*\", \"*.parquet\")))\n\n# 入力変数選択（事前定義）\nid_col = \"symbol_id\"    # ID列\ndate_id = \"date_id\"     # 日付列\ntime_id = \"time_id\"     # 時間列\npred_cols = [f\"responder_{i}\" for i in range(9)]  # 目的変数\n\n# 最初のファイルからカラム一覧とdtype取得\nif not parquet_files:\n    raise FileNotFoundError(\"No parquet files found in the specified data path.\")\n\ndf_test = pl.read_parquet(parquet_files[9], n_rows=1)\nall_cols = df_test.columns\nschema = df_test.schema  # {col_name: polars.DataType}\n\n# 除外する列\nexclude_cols = {id_col, date_id, time_id} | set(pred_cols)\n\n# input_colsを自動的に決定\ninput_cols = []\nfor col in all_cols:\n    if col not in exclude_cols:\n        polars_dtype = schema[col]\n        print(f\"{col}, {polars_dtype}\")\n        input_cols.append(col)\n\n# target_cols作成\ntarget_cols = [id_col, date_id, time_id] + pred_cols + input_cols\nprint(\"target_cols:\", target_cols)","metadata":{"_uuid":"d05a7c5f-9179-44c0-9479-18ee8d8af501","_cell_guid":"724c68b8-b99e-45b1-83e7-1585737a6791","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:29.869678Z","iopub.execute_input":"2024-12-23T09:42:29.870111Z","iopub.status.idle":"2024-12-23T09:42:31.160224Z","shell.execute_reply.started":"2024-12-23T09:42:29.870064Z","shell.execute_reply":"2024-12-23T09:42:31.158944Z"}},"outputs":[{"name":"stdout","text":"weight, Float32\nfeature_00, Float32\nfeature_01, Float32\nfeature_02, Float32\nfeature_03, Float32\nfeature_04, Float32\nfeature_05, Float32\nfeature_06, Float32\nfeature_07, Float32\nfeature_08, Float32\nfeature_09, Int8\nfeature_10, Int8\nfeature_11, Int16\nfeature_12, Float32\nfeature_13, Float32\nfeature_14, Float32\nfeature_15, Float32\nfeature_16, Float32\nfeature_17, Float32\nfeature_18, Float32\nfeature_19, Float32\nfeature_20, Float32\nfeature_21, Float32\nfeature_22, Float32\nfeature_23, Float32\nfeature_24, Float32\nfeature_25, Float32\nfeature_26, Float32\nfeature_27, Float32\nfeature_28, Float32\nfeature_29, Float32\nfeature_30, Float32\nfeature_31, Float32\nfeature_32, Float32\nfeature_33, Float32\nfeature_34, Float32\nfeature_35, Float32\nfeature_36, Float32\nfeature_37, Float32\nfeature_38, Float32\nfeature_39, Float32\nfeature_40, Float32\nfeature_41, Float32\nfeature_42, Float32\nfeature_43, Float32\nfeature_44, Float32\nfeature_45, Float32\nfeature_46, Float32\nfeature_47, Float32\nfeature_48, Float32\nfeature_49, Float32\nfeature_50, Float32\nfeature_51, Float32\nfeature_52, Float32\nfeature_53, Float32\nfeature_54, Float32\nfeature_55, Float32\nfeature_56, Float32\nfeature_57, Float32\nfeature_58, Float32\nfeature_59, Float32\nfeature_60, Float32\nfeature_61, Float32\nfeature_62, Float32\nfeature_63, Float32\nfeature_64, Float32\nfeature_65, Float32\nfeature_66, Float32\nfeature_67, Float32\nfeature_68, Float32\nfeature_69, Float32\nfeature_70, Float32\nfeature_71, Float32\nfeature_72, Float32\nfeature_73, Float32\nfeature_74, Float32\nfeature_75, Float32\nfeature_76, Float32\nfeature_77, Float32\nfeature_78, Float32\ntarget_cols: ['symbol_id', 'date_id', 'time_id', 'responder_0', 'responder_1', 'responder_2', 'responder_3', 'responder_4', 'responder_5', 'responder_6', 'responder_7', 'responder_8', 'weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import polars as pl\n\n# Globalデータ\n# max_time_idを取得\nmax_time_id = pl.read_parquet(parquet_files[9]).select(pl.col(\"time_id\").max()).item()\nsegment_size = (max_time_id + 1) // 4  # 4等分のサイズを計算\n\n# 4分割の閾値を計算\nthreshold1 = segment_size\nthreshold2 = segment_size * 2\nthreshold3 = segment_size * 3\n\nmax_time_id, threshold1, threshold2, threshold3","metadata":{"_uuid":"18a504db-21e6-4d3e-826a-68346059f339","_cell_guid":"6a08a6c4-7df0-4f3e-943f-88f5cabb0c2a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:31.161634Z","iopub.execute_input":"2024-12-23T09:42:31.162053Z","iopub.status.idle":"2024-12-23T09:42:37.843296Z","shell.execute_reply.started":"2024-12-23T09:42:31.162019Z","shell.execute_reply":"2024-12-23T09:42:37.841903Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(967, 242, 484, 726)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"def add_prev_feature(ldf: pl.LazyFrame, cols: list[str]) -> tuple[pl.LazyFrame, list[str]]:\n    \"\"\"\n    前日集計(_prev_*)を生成する関数 (LazyFrame版).\n    \n    Args:\n        ldf: LazyFrame\n        cols: 前日分の集計対象となる列（文字列のみ）\n    Returns:\n        (ldf_joined, generated_cols): \n          ldf_joined → ldfに_prev_*列をleft joinしたLazyFrame\n          generated_cols → 生成された_prev_*列の名前一覧\n    \"\"\"\n    agg_expr = []\n    for c in cols:\n        agg_expr.extend([\n            pl.col(c).first().alias(f\"{c}_prev_first\"),\n            pl.col(c).last().alias(f\"{c}_prev_last\"),\n            pl.col(c).max().alias(f\"{c}_prev_max\"),\n            pl.col(c).min().alias(f\"{c}_prev_min\"),\n            pl.col(c).mean().alias(f\"{c}_prev_mean\"),\n            pl.col(c).std().alias(f\"{c}_prev_std\")\n        ])\n\n    ldf_agg = (\n        ldf\n        .group_by([\"symbol_id\", \"date_id\"])\n        .agg(agg_expr)\n        .with_columns((pl.col(\"date_id\") + 1).alias(\"date_id_next\"))\n        .drop(\"date_id\")\n        .rename({\"date_id_next\": \"date_id\"})\n    )\n\n    # 生成された列名(除く symbol_id, date_id)\n    generated_cols = [\n        c for c in ldf_agg.collect_schema().keys()\n        if c not in (\"symbol_id\",\"date_id\")\n    ]\n\n    # 元の ldf に left join\n    ldf_joined = ldf.join(ldf_agg, on=[\"symbol_id\",\"date_id\"], how=\"left\")\n    return ldf_joined, generated_cols\n\ndef add_diff_n_by_id_date_time(ldf: pl.LazyFrame, n: int, cols: list[str]) -> tuple[pl.LazyFrame, list[str]]:\n    \"\"\"\n    symbol_id, date_id, time_id でソートした時系列上で、\n    n行前との差分(col - shift(n))を計算。time_id=0は前のdate_id末尾が参照対象.\n    \n    Args:\n        ldf: LazyFrame\n        n: 何行前とのdiffをとるか\n        cols: 差分を計算したい列（文字列リスト）\n    Returns:\n        (ldf_with_diff, diff_col_names): \n          ldf_with_diff → diff列を追加したLazyFrame\n          diff_col_names → 生成されたdiff列名\n    \"\"\"\n    ldf_sorted = ldf.sort([\"symbol_id\",\"date_id\",\"time_id\"])\n\n    diff_exprs = []\n    diff_col_names = []\n\n    for c in cols:\n        diff_name = f\"{c}_diff{n}\"\n        expr = (\n            pl.col(c) - pl.col(c).shift(n)\n        ).over(\"symbol_id\").alias(diff_name)\n        diff_exprs.append(expr)\n        diff_col_names.append(diff_name)\n\n    ldf_with_diff = ldf_sorted.with_columns(diff_exprs)\n    return ldf_with_diff, diff_col_names\n\ndef add_id_time_id_group_feature(ldf: pl.LazyFrame, base_features: list[str]) -> tuple[pl.LazyFrame, list[str]]:\n    \"\"\"\n    time_id を threshold1,2,3 で4分割し、\n    group_first_ratio, group_expanding_mean(60)を追加.\n    \n    Returns:\n        (ldf_with_cols, generated_cols)\n    \"\"\"\n    ldf_sorted = ldf.sort([\"symbol_id\",\"date_id\",\"time_id\"])\n\n    # time_id_group 列を追加\n    ldf_grouped = ldf_sorted.with_columns(\n        pl.when(pl.col(\"time_id\") < threshold1).then(0)\n        .when(pl.col(\"time_id\") < threshold2).then(1)\n        .when(pl.col(\"time_id\") < threshold3).then(2)\n        .otherwise(3)\n        .cast(pl.Int32)\n        .alias(\"time_id_group\")\n    )\n\n    group_first_ratio_exprs = []\n    group_expanding_mean_exprs = []\n    generated_cols = []\n\n    for col in base_features:\n        gfr_name = f\"{col}_group_first_ratio\"\n        gem_name = f\"{col}_group_expanding_mean60\"\n        expr_gfr = (\n            (pl.col(col).first() / pl.col(col))\n            .over(['date_id','time_id_group','symbol_id'])\n            .cast(pl.Float32)\n            .alias(gfr_name)\n        )\n        expr_gem = (\n            (pl.col(col).rolling_mean(60, min_periods=1) / pl.col(col))\n            .over(['date_id','time_id_group','symbol_id'])\n            .cast(pl.Float32)\n            .alias(gem_name)\n        )\n        group_first_ratio_exprs.append(expr_gfr)\n        group_expanding_mean_exprs.append(expr_gem)\n        generated_cols.extend([gfr_name, gem_name])\n\n    ldf_with_cols = ldf_grouped.with_columns(group_first_ratio_exprs + group_expanding_mean_exprs)\n    return ldf_with_cols, generated_cols\n\ndef add_date_time_feature(ldf: pl.LazyFrame, base_features: list[str]) -> tuple[pl.LazyFrame, list[str]]:\n    \"\"\"\n    date_id, time_id ごとの mean ratio / rank を追加\n    Returns:\n        (ldf_with_cols, generated_cols)\n    \"\"\"\n    group_mean_ratio_exprs = []\n    group_rank_exprs = []\n    generated_cols = []\n\n    for col in base_features:\n        gmr_name = f\"{col}_time_id_group_mean_ratio\"\n        grk_name = f\"{col}_time_id_group_rank\"\n\n        expr_gmr = (\n            (pl.col(col).mean() / pl.col(col))\n            .over(['date_id','time_id'])\n            .cast(pl.Float32)\n            .alias(gmr_name)\n        )\n        expr_grk = (\n            (pl.col(col).rank(descending=True, method='ordinal') / pl.col(col).count())\n            .over(['date_id','time_id'])\n            .cast(pl.Float32)\n            .alias(grk_name)\n        )\n        group_mean_ratio_exprs.append(expr_gmr)\n        group_rank_exprs.append(expr_grk)\n        generated_cols.extend([gmr_name, grk_name])\n\n    ldf_with_cols = ldf.with_columns(group_mean_ratio_exprs + group_rank_exprs)\n    return ldf_with_cols, generated_cols","metadata":{"_uuid":"d69b1a0f-cf8e-437d-a86e-169fff722201","_cell_guid":"99331b8e-ec22-4522-a338-a5102a9b53bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:37.845101Z","iopub.execute_input":"2024-12-23T09:42:37.845724Z","iopub.status.idle":"2024-12-23T09:42:37.865768Z","shell.execute_reply.started":"2024-12-23T09:42:37.845676Z","shell.execute_reply":"2024-12-23T09:42:37.864504Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import polars as pl\nfrom typing import List, Tuple\n\n\ndef add_symbol_date_lag(\n    ldf: pl.LazyFrame, \n    cols: list[str],\n    n: int = 1\n) -> tuple[pl.LazyFrame, list[str]]:\n    \"\"\"\n    symbol_id × date_id でgroupbyし、colsの統計量を計算して\n    \"date_id + n\" に対応付ける -> 次の日(あるいは n日後)が参照する形でリークを防ぐ。\n    \n    戻り値:\n      (ldf_joined, generated_cols)\n        ldf_joined: ldfに統計量をleft joinしたLazyFrame\n        generated_cols: 新たに生成された列名一覧\n    \"\"\"\n    agg_expr = []\n    generated_cols = []\n    for c in cols:\n        # 例として mean, std, min, maxあたりを計算\n        agg_expr.extend([\n            pl.col(c).mean().alias(f\"{c}_lag{n}_mean\"),\n            pl.col(c).std().alias(f\"{c}_lag{n}_std\"),\n            pl.col(c).min().alias(f\"{c}_lag{n}_min\"),\n            pl.col(c).max().alias(f\"{c}_lag{n}_max\"),\n        ])\n        generated_cols.extend([\n            f\"{c}_lag{n}_mean\",\n            f\"{c}_lag{n}_std\",\n            f\"{c}_lag{n}_min\",\n            f\"{c}_lag{n}_max\",\n        ])\n    \n    # groupby symbol_id, date_id\n    ldf_agg = (\n        ldf.group_by([\"symbol_id\",\"date_id\"])\n           .agg(agg_expr)\n           .with_columns((pl.col(\"date_id\") + n).alias(\"date_id_next\"))\n           .drop(\"date_id\")\n           .rename({\"date_id_next\":\"date_id\"})\n    )\n    # これで \"n日後\" の行に統計量をjoin可能\n\n    # left join\n    ldf_joined = ldf.join(ldf_agg, on=[\"symbol_id\",\"date_id\"], how=\"left\")\n    return ldf_joined, generated_cols\n\ndef add_symbol_timeidgroup_lag(\n    ldf: pl.LazyFrame,\n    cols: List[str],\n    agg_funcs: List[str] = [\"mean\",\"std\"],\n    group_size: int = 4  # time_id_group が [0,1,2,3] の場合\n) -> Tuple[pl.LazyFrame, List[str]]:\n    \"\"\"\n    time_id_groupが 0 の行は (date_id-1, group=group_size-1) を“直前”として参照し、\n    group>0 の行は (date_id, group-1) を“直前”として参照。\n    これを実現するために:\n      1) ldfに date_id_prev, time_id_group_prev を計算\n      2) group_by([\"symbol_id\",\"date_id_prev\",\"time_id_group_prev\"]) で集計 (cols, agg_funcs)\n      3) main ldf に on=[\"symbol_id\",\"date_id\",\"time_id_group\"] で left join\n\n    戻り値:\n      (ldf_joined, generated_cols)\n    \"\"\"\n\n    # 1) date_id_prev, time_id_group_prev を列として作る\n    #    time_id_group=0 → (date_id-1, group=3)\n    #    time_id_group>0 → (date_id, group=time_id_group-1)\n    # LazyFrameに列を追加\n    ldf_prep = ldf.with_columns([\n        # date_id_prev\n        pl.when(pl.col(\"time_id_group\") == 0)\n          .then(pl.col(\"date_id\") - 1)\n          .otherwise(pl.col(\"date_id\"))\n          .alias(\"date_id_prev\"),\n\n        # time_id_group_prev\n        pl.when(pl.col(\"time_id_group\") == 0)\n          .then(group_size - 1)  # group=3 if group_size=4\n          .otherwise(pl.col(\"time_id_group\") - 1)\n          .alias(\"time_id_group_prev\")\n    ])\n\n    # 2) group_by([\"symbol_id\",\"date_id_prev\",\"time_id_group_prev\"]) で集計\n    agg_exprs = []\n    generated_cols = []\n    for c in cols:\n        for f in agg_funcs:\n            alias_name = f\"{c}_lag_{f}\"  # 例: c_lag_mean, c_lag_std\n            generated_cols.append(alias_name)\n            if f==\"mean\":\n                agg_exprs.append(pl.col(c).mean().alias(alias_name))\n            elif f==\"std\":\n                agg_exprs.append(pl.col(c).std().alias(alias_name))\n            elif f==\"min\":\n                alias_name = f\"{c}_lag_min\"\n                generated_cols.append(alias_name)\n                agg_exprs.append(pl.col(c).min().alias(alias_name))\n            elif f==\"max\":\n                alias_name = f\"{c}_lag_max\"\n                generated_cols.append(alias_name)\n                agg_exprs.append(pl.col(c).max().alias(alias_name))\n            # 必要に応じて他の集約関数も追加\n\n    ldf_agg = (\n        ldf_prep.group_by([\"symbol_id\",\"date_id_prev\",\"time_id_group_prev\"])\n                .agg(agg_exprs)\n    )\n\n    # 3) main ldf と join\n    # main ldf のキー: (symbol_id, date_id, time_id_group)\n    # ldf_agg のキー:  (symbol_id, date_id_prev, time_id_group_prev)\n    # => join するには ldf_prep と rename などで合わせる\n    #  あるいは on left side: \"symbol_id\", \"date_id_prev as date_id\", \"time_id_group_prev as time_id_group\"\n    #   → ここは left sideを下準備 or right side rename\n    #   ここでは ldf_agg 側を rename して \"date_id_prev->date_id\", \"time_id_group_prev->time_id_group\"\n    ldf_agg_renamed = ldf_agg.rename({\"date_id_prev\":\"date_id\",\"time_id_group_prev\":\"time_id_group\"})\n\n    # now we can join on (symbol_id, date_id, time_id_group)\n    ldf_joined = ldf_prep.join(\n        ldf_agg_renamed,\n        on=[\"symbol_id\",\"date_id\",\"time_id_group\"],\n        how=\"left\"\n    )\n\n    return ldf_joined, generated_cols","metadata":{"_uuid":"af6198cc-8872-477f-bc88-32d3910612d2","_cell_guid":"15451375-151c-4a8c-92df-3085231c3b40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-23T09:42:37.867376Z","iopub.execute_input":"2024-12-23T09:42:37.867848Z","iopub.status.idle":"2024-12-23T09:42:37.891691Z","shell.execute_reply.started":"2024-12-23T09:42:37.867800Z","shell.execute_reply":"2024-12-23T09:42:37.890532Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# 汎用特徴量作成関数","metadata":{}},{"cell_type":"code","source":"import re\nimport polars as pl\nfrom typing import List, Tuple\n\ndef create_stat_features_by(\n    ldf: pl.LazyFrame,\n    cols: List[str],\n    key_by: List[str],\n    agg_funcs: List[str],\n) -> Tuple[pl.LazyFrame, List[str]]:\n    \"\"\"\n    ldf を key_by で group_by し、cols に対して指定された agg_funcs を実行する汎用関数。\n    例:\n      agg_funcs に [\"mean\",\"std\",\"min\",\"max\",\"median\",\"sum\",\"count\",\"n_unique\",\n                    \"last\",\"first\",\"skew\",\"kurtosis\",\"cv\", \"q0.25\",\"q0.75\" ...]\n      のような文字列を指定できる。\n\n    戻り値:\n      (ldf_agg, generated_cols):\n        ldf_agg : group_by(key_by).agg(...) の結果 (key_by + 各集計列 を持つ LazyFrame)\n        generated_cols : 新しく生成された列名のリスト\n\n    注意:\n      - \"q0.25\" 等のquantile形式は \"q0.x\" のフォーマットで xをfloatとして解釈し expr.quantile(x)。\n      - \"cv\" (coefficient of variation) = std/mean (mean=0に注意)\n      - skew, kurtosis は Polars のバージョンによっては使えない場合あり\n      - rolling_meanなどのウィンドウ関数はこのgroup_by集約とは別物\n\n    例:\n      aggregator_mapに含まれる文字列一覧を \"agg_funcs\" で指定\n        -> \"mean\",\"std\",\"cv\",\"q0.25\",\"skew\"など\n    \"\"\"\n\n    # Polarsでサポートする単一スカラー集約を文字列→lambda で定義\n    def parse_quantile(alias: str, cexpr: pl.Expr) -> pl.Expr:\n        # 例: alias == \"q0.25\" → 0.25\n        #     alias == \"q0.75\" → 0.75\n        m = re.match(r\"q0\\.(\\d+)\", alias)  # 例: \"q0.25\" -> group(1)==\"25\"\n        if not m:\n            # 不正形式ならそのまま col\n            return cexpr\n        float_str = \"0.\" + m.group(1)\n        q = float(float_str)\n        return cexpr.quantile(q)  # quantile(0.25 / 0.75 etc.)\n\n    aggregator_map = {\n        \"mean\":      lambda cexpr: cexpr.mean(),\n        \"std\":       lambda cexpr: cexpr.std(),\n        \"min\":       lambda cexpr: cexpr.min(),\n        \"max\":       lambda cexpr: cexpr.max(),\n        \"median\":    lambda cexpr: cexpr.median(),\n        \"sum\":       lambda cexpr: cexpr.sum(),\n        \"count\":     lambda cexpr: cexpr.count(),\n        \"n_unique\":  lambda cexpr: cexpr.n_unique(),\n        \"last\":      lambda cexpr: cexpr.last(),\n        \"first\":     lambda cexpr: cexpr.first(),\n        \"skew\":      lambda cexpr: cexpr.skew(),\n        \"kurtosis\":  lambda cexpr: cexpr.kurtosis(),\n        # 変動係数 (coefficient of variation)\n        \"cv\":        lambda cexpr: (cexpr.std() / cexpr.mean()),  # mean=0注意\n    }\n\n    agg_exprs = []\n    generated_cols: List[str] = []\n\n    for c in cols:\n        for f in agg_funcs:\n            alias_name = f\"{c}_{'_'.join(key_by)}_{f}\"\n\n            # 1) quantile系:  \"q0.25\", \"q0.50\" etc.\n            if f.startswith(\"q0.\"):\n                expr = parse_quantile(f, pl.col(c)).alias(alias_name)\n                agg_exprs.append(expr)\n                generated_cols.append(alias_name)\n                continue\n\n            # 2) aggregator_map にあるか？\n            aggregator = aggregator_map.get(f, None)\n            if aggregator is not None:\n                expr = aggregator(pl.col(c)).alias(alias_name)\n                agg_exprs.append(expr)\n                generated_cols.append(alias_name)\n            else:\n                raise Exception(f\"create_stat_features_by: not defined {f}\")\n\n    # group_by and agg\n    ldf_agg = ldf.group_by(key_by).agg(agg_exprs)\n\n    return ldf_agg, generated_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T09:42:37.893242Z","iopub.execute_input":"2024-12-23T09:42:37.893576Z","iopub.status.idle":"2024-12-23T09:42:37.915141Z","shell.execute_reply.started":"2024-12-23T09:42:37.893542Z","shell.execute_reply":"2024-12-23T09:42:37.913951Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# rolling/window系特徴量","metadata":{}},{"cell_type":"code","source":"import polars as pl\nfrom typing import List, Tuple\n\n\ndef add_avg_change_and_volatility(\n    ldf: pl.LazyFrame,\n    cols: List[str],\n    sort_keys: List[str],\n    group_keys: List[str],\n    n: int = 5,\n    use_log_return: bool = False,\n) -> Tuple[pl.LazyFrame, List[str]]:\n    \"\"\"\n    LazyFrameに対して:\n      (1) t-nとの変化率 (log/ratio) ×100\n      (2) 過去n期間 rolling_std(ボラティリティ)\n    をcolsの各カラムについて計算し、(ldf_out, generated_cols)を返す。\n\n    Parameters\n    ----------\n    ldf : pl.LazyFrame\n        処理対象のLazyFrame\n    cols : List[str]\n        計算対象の数値カラム ([\"feature_00\",\"feature_01\"]など)\n    n : int\n        shiftやrollingのウィンドウサイズ\n    use_log_return : bool\n        Trueならlogリターンで計算\n    sort_keys : List[str], optional\n        時系列順を保証するためにソートするカラムのリスト\n        例: [\"symbol_id\",\"date_id\",\"time_id\"]\n    group_keys : List[str], optional\n        shiftやrollingを「どのカラム単位でパーティション切る」か\n        例: [\"symbol_id\"]\n\n    Returns\n    -------\n    (ldf_out, generated_cols):\n      ldf_out       : 新しい列を追加したLazyFrame\n      generated_cols: 作成された列の名前一覧\n\n    Note\n    ----\n    - sort_keys が指定されれば ldf.sort(by=sort_keys) で並べ替えてから計算\n    - group_keys が指定されれば shift(n).over(group_keys), rolling_std(...).over(group_keys)\n      → 銘柄単位、などで独立計算が可能\n    \"\"\"\n\n    # 1) ソート\n    if sort_keys:\n        ldf = ldf.sort(by=sort_keys)\n\n    # group_keys が空でなければ partition 単位で over() を使う\n    def shift_n_expr(col: str, periods: int):\n        if group_keys:\n            return pl.col(col).shift(periods).over(group_keys)\n        else:\n            return pl.col(col).shift(periods)\n\n    def rolling_std_expr(col: str, window_size: int):\n        if group_keys:\n            return pl.col(col).rolling_std(window_size=window_size).over(group_keys)\n        else:\n            return pl.col(col).rolling_std(window_size=window_size)\n\n    generated_cols: List[str] = []\n    tmp_cols: List[str] = []\n\n    # (A) まず、一時列(1期間リターン)を作成\n    exprs_step1 = []\n    for col_name in cols:\n        tmp_return_col = f\"__tmp_return_{col_name}_{n}_{'log' if use_log_return else 'ratio'}\"\n        tmp_cols.append(tmp_return_col)\n\n        # 1期前の log 差 or ratio\n        if use_log_return:\n            # (log(col) - log(col.shift(1))) * 100\n            exprs_step1.append(\n                (\n                    (pl.col(col_name).log() - shift_n_expr(col_name, 1).log()) * 100\n                ).alias(tmp_return_col)\n            )\n        else:\n            # ((col / col.shift(1)) - 1) * 100\n            exprs_step1.append(\n                (\n                    ((pl.col(col_name) / shift_n_expr(col_name, 1)) - 1) * 100\n                ).alias(tmp_return_col)\n            )\n\n    # 一時列を先に追加\n    ldf = ldf.with_columns(exprs_step1)\n\n    # (B) 追加で (a) t-n との変化率, (b) rolling_std を計算\n    exprs_step2 = []\n    for col_name in cols:\n        avg_change_col = f\"{col_name}_avg_change_{n}\"\n        vol_col = f\"{col_name}_volatility_{n}\"\n        generated_cols.extend([avg_change_col, vol_col])\n\n        tmp_return_col = f\"__tmp_return_{col_name}_{n}_{'log' if use_log_return else 'ratio'}\"\n\n        # (a) t-n との変化率\n        if use_log_return:\n            # log リターン: (log(col) - log(col.shift(n))) * 100\n            exprs_step2.append(\n                (\n                    (pl.col(col_name).log() - shift_n_expr(col_name, n).log()) * 100\n                ).alias(avg_change_col)\n            )\n        else:\n            # ratio リターン: ((col / col.shift(n)) - 1) * 100\n            exprs_step2.append(\n                (\n                    ((pl.col(col_name) / shift_n_expr(col_name, n)) - 1) * 100\n                ).alias(avg_change_col)\n            )\n\n        # (b) rolling_std (過去 n 期間)\n        exprs_step2.append(\n            rolling_std_expr(tmp_return_col, n).alias(vol_col)\n        )\n\n    # 二段階目の計算を実行し、一時列を drop\n    ldf_out = (\n        ldf\n        .with_columns(exprs_step2)\n        .drop(tmp_cols)\n    )\n\n    return ldf_out, generated_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T09:42:37.916633Z","iopub.execute_input":"2024-12-23T09:42:37.916995Z","iopub.status.idle":"2024-12-23T09:42:37.936479Z","shell.execute_reply.started":"2024-12-23T09:42:37.916950Z","shell.execute_reply":"2024-12-23T09:42:37.935452Z"},"scrolled":true},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# 特徴量処理","metadata":{}},{"cell_type":"code","source":"train_length = 180\nvalid_length = 30\ntrain_shift = 120\nretroactive_size = 60 # rollingやwindowの最大lags数","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T09:42:37.941207Z","iopub.execute_input":"2024-12-23T09:42:37.941585Z","iopub.status.idle":"2024-12-23T09:42:37.954304Z","shell.execute_reply.started":"2024-12-23T09:42:37.941551Z","shell.execute_reply":"2024-12-23T09:42:37.953213Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Fature Today Rolling 処理","metadata":{}},{"cell_type":"code","source":"def feature_today_rolling_func(\n    df: pl.DataFrame,\n    pred_cols: List[str],\n    input_cols: List[str],\n    original_cols: List[str] | None = None\n) -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n    \"\"\"\n    featureに関する特徴量を計算\n    \"\"\"\n    generated_features: List[str] = []\n    generated_preds: List[str] = []\n\n    # Lazy化\n    ldf = df.lazy()\n\n    ldf.drop(pred_cols)\n\n    #  sort\n    ldf = ldf.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n\n    # 当日特徴量\n    ## rolling特徴量\n    ldf, generated_cols = add_avg_change_and_volatility(ldf, input_cols, n = 4, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id\"])\n    generated_features.extend(generated_cols)\n\n    ldf, generated_cols = add_avg_change_and_volatility(ldf, input_cols, n = 28, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id\"])\n    generated_features.extend(generated_cols)\n\n    # drop\n    ldf.drop(input_cols)\n\n    return ldf, generated_features, generated_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T09:42:37.955862Z","iopub.execute_input":"2024-12-23T09:42:37.956420Z","iopub.status.idle":"2024-12-23T09:42:37.967052Z","shell.execute_reply.started":"2024-12-23T09:42:37.956375Z","shell.execute_reply":"2024-12-23T09:42:37.965977Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"output_base = \"feature_today_rolling_datasets\"\nos.makedirs(output_base, exist_ok=True)\n\ngenerated_feature_lag, _ = run_walk_forward(\n    input_base=data_path,\n    output_base=output_base,\n    id_col=id_col,\n    date_id=date_id,\n    time_id=time_id,\n    pred_cols=pred_cols,\n    input_cols=input_cols,\n    train_length=train_length,\n    valid_length=valid_length,\n    train_shift=train_shift, \n    retroactive_size=retroactive_size,\n    add_feature_func=feature_today_rolling_func\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T09:42:37.968295Z","iopub.execute_input":"2024-12-23T09:42:37.968583Z","execution_failed":"2024-12-23T13:01:10.723Z"}},"outputs":[{"name":"stdout","text":"original_cols ['symbol_id', 'date_id', 'time_id', 'responder_0', 'responder_1', 'responder_2', 'responder_3', 'responder_4', 'responder_5', 'responder_6', 'responder_7', 'responder_8', 'weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\nmeta: partition, min date, max date [(0, 0, 169), (1, 170, 339), (2, 340, 509), (3, 510, 679), (4, 680, 849), (5, 850, 1019), (6, 1020, 1189), (7, 1190, 1359), (8, 1360, 1529), (9, 1530, 1698)]\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\nextended_start 0\nvalid_end 210\nadd_feature_func Execution time: 0.0124 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n  write_parquet Execution time: 177.4828 seconds\nSprint 1:\n  Train: date_id in [0, 180) -> feature_today_rolling_datasets/sprint1/train.parquet\n  Valid: date_id in [180, 210) -> feature_today_rolling_datasets/sprint1/valid.parquet\n  Saved to feature_today_rolling_datasets/sprint1\n\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\nextended_start 60\nvalid_end 330\nadd_feature_func Execution time: 0.0126 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n  write_parquet Execution time: 285.2313 seconds\nSprint 2:\n  Train: date_id in [120, 300) -> feature_today_rolling_datasets/sprint2/train.parquet\n  Valid: date_id in [300, 330) -> feature_today_rolling_datasets/sprint2/valid.parquet\n  Saved to feature_today_rolling_datasets/sprint2\n\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\nextended_start 180\nvalid_end 450\nadd_feature_func Execution time: 0.0123 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n  write_parquet Execution time: 321.6182 seconds\nSprint 3:\n  Train: date_id in [240, 420) -> feature_today_rolling_datasets/sprint3/train.parquet\n  Valid: date_id in [420, 450) -> feature_today_rolling_datasets/sprint3/valid.parquet\n  Saved to feature_today_rolling_datasets/sprint3\n\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\nread_parquet /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\nextended_start 300\nvalid_end 570\nadd_feature_func Execution time: 0.0127 seconds\nCheck generated col diff.\nCheck generated col diff.\nwrite_parquet start.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Feature Today Stat処理","metadata":{}},{"cell_type":"code","source":"def feature_today_stat_func(\n    df: pl.DataFrame,\n    pred_cols: List[str],\n    input_cols: List[str],\n    original_cols: List[str] | None = None\n) -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n    \"\"\"\n    featureに関する特徴量を計算\n    \"\"\"\n    generated_features: List[str] = []\n    generated_preds: List[str] = []\n\n    # Lazy化\n    ldf = df.lazy()\n\n    #  sort\n    ldf = ldf.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n    \n    # date_id, time_id単位\n    df_datetime, datetime_feature_cols = create_stat_features_by(ldf=ldf, cols=input_cols, key_by=[\"date_id\", \"time_id\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(datetime_feature_cols)\n    \n    df_datetime, datetime_feature_cols = add_avg_change_and_volatility(df_datetime, datetime_feature_cols, n = 7, group_keys=[\"date_id\"], sort_keys=[\"time_id\"])\n    generated_features.extend(datetime_feature_cols)\n    \n    return df_datetime, generated_features, generated_preds","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_base = \"feature_today_stat_datasets\"\nos.makedirs(output_base, exist_ok=True)\n\ngenerated_feature_lag, _ = run_walk_forward(\n    input_base=data_path,\n    output_base=output_base,\n    id_col=id_col,\n    date_id=date_id,\n    time_id=time_id,\n    pred_cols=pred_cols,\n    input_cols=input_cols,\n    train_length=train_length,\n    valid_length=valid_length,\n    train_shift=train_shift, \n    retroactive_size=retroactive_size,\n    add_feature_func=feature_today_stat_func\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Responder処理","metadata":{"_uuid":"be2982c7-0d83-4e65-9885-d7d1c21abde5","_cell_guid":"d1e3fc19-70a7-46d4-8b07-e4794582ec91","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import polars as pl\nfrom typing import List, Tuple\n\ndef create_raw_responder_lag(\n    df: pl.DataFrame,\n    pred_cols: List[str]\n) -> Tuple[pl.LazyFrame, List[str]]:\n    \"\"\"\n    1) 'symbol_id', 'date_id', 'w' と pred_cols の列のみ取り出す\n    2) w * responder_6 の列を新たに作る (\"w_responder_6\")\n    3) w を削除\n    4) date_id を (date_id - 1) シフト\n    5) pred_cols + [\"w_responder_6\"] を \"_lag_1\" にリネーム\n    戻り値:\n      (lazy_frame, generated_cols):\n        lazy_frame : 上記処理結果をLazyFrameに変換したもの\n        generated_cols : リネーム後の列名(例: \"responder_6_lag_1\", \"w_responder_6_lag_1\"など)\n    \"\"\"\n\n    # 1) 必要列のみ抽出 (Eager DataFrame)\n    target_cols = [\"symbol_id\", \"date_id\", \"time_id_group\", \"weight\"] + pred_cols\n    df_lags = df.select(target_cols)\n\n    # 2) w * responder を作成\n    w_responder_cols = []\n    for c in pred_cols:\n        w_c = f\"w_{c}\"\n        w_responder_cols.append(w_c)\n        df_lags = df_lags.with_columns(\n            (pl.col(\"weight\") * pl.col(c)).alias(w_c)\n        )\n    new_pred_cols = pred_cols + w_responder_cols\n    \n    # 3) wを削除\n    df_lags = df_lags.drop([\"weight\"])\n\n    # 4) date_id = date_id + 1\n    df_lags = df_lags.with_columns(\n        (pl.col(\"date_id\") + 1).alias(\"date_id\")\n    )\n\n    # 5) 上記 new_pred_cols を \"colName_lag_1\" にリネーム\n    rename_map = {}\n    for c in new_pred_cols:\n        rename_map[c] = f\"{c}_lag_1\"\n\n    df_lags = df_lags.rename(rename_map)\n\n    # リネーム後の列名一覧\n    generated_cols = list(rename_map.values())\n    \n    keys = [\"symbol_id\", \"date_id\", \"time_id_group\"]\n    return df_lags, generated_cols, keys","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_responder_lag_feature(ldf, cols):\n    generated_features = []\n    df_raw_lags, lags_cols, raw_lags_keys = create_raw_responder_lag(ldf, cols)\n\n    # symbol_id, date_id, time_id_group 単位\n    df_lags, gen_cols_symdate_group = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"symbol_id\", \"date_id\", \"time_id_group\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_symdate_group)\n\n    ## rolling特徴量\n    ### 4 time_id_groupまで見る\n    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 4, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n    generated_features.extend(generated_cols)\n\n    ### 4 * 7 time_id_groupまで見る\n    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 28, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n    generated_features.extend(generated_cols)\n\n    # date_id単位でラグ列を集計\n    df_lags_dateid, gen_cols_date = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_date)\n\n    df_lags_dateid, generated_cols = add_avg_change_and_volatility(df_lags_dateid, gen_cols_date, n = 7, group_keys=[\"date_id\"], sort_keys=[])\n    generated_features.extend(generated_cols)\n    \n    df_lags = df_lags.join(df_lags_dateid, on=[\"date_id\"], how=\"left\")\n\n    # symbol_id, date_id単位\n    df_lags_symdate, gen_cols_symdate = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols,key_by=[\"symbol_id\", \"date_id\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_symdate)\n    \n    df_lags_symdate, generated_cols = add_avg_change_and_volatility(df_lags_symdate, gen_cols_symdate, n = 7, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\"])\n    generated_features.extend(generated_cols)\n\n    df_lags = df_lags.join(df_lags_symdate, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n\n    # date_id, time_group_id単位\n    df_lags_datetime, gen_cols_datetime = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\", \"time_id_group\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_datetime)\n   \n    df_lags_datetime, generated_cols = add_avg_change_and_volatility(df_lags_datetime, gen_cols_datetime, n = 7, group_keys=[\"date_id\"], sort_keys=[\"time_id_group\"])\n    generated_features.extend(generated_cols)\n    \n    df_lags = df_lags.join(df_lags_datetime, on=[\"date_id\",\"time_id_group\"], how=\"left\")\n\n    return df_lags, generated_features","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def responder_lag_feature_func(\n    df: pl.DataFrame,\n    pred_cols: List[str],\n    input_cols: List[str],\n    original_cols: List[str] | None = None\n) -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n    ## [lags.parquetの扱い]\n    ## 当日と前日のtiem_idが異なる可能性なので、単純なdate_id, time_idのlagは有用ではない\n    ## また、前日の全てのlagは当日のtime_id == 0のタイミングで提供される\n    ## すなわち、lagsが提供された場合で統計量の処理を行い、date_id - 1のresponderの統計量として扱う。\n    \n    generated_features: List[str] = []\n    generated_preds: List[str] = []\n\n    # Lazy化\n    ldf = df.lazy()\n\n    # 1) sort\n    ldf = ldf.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n\n    # 2) time_id_group (4分割)\n    ldf = ldf.with_columns(\n        pl.when(pl.col(\"time_id\") < threshold1).then(0)\n          .when(pl.col(\"time_id\") < threshold2).then(1)\n          .when(pl.col(\"time_id\") < threshold3).then(2)\n          .otherwise(3)\n          .cast(pl.Int32)\n          .alias(\"time_id_group\")\n    )\n    generated_features.extend([\"time_id_group\"])\n\n    # responder-lag特徴量\n    ldf, cols = create_responder_lag_feature(ldf, pred_cols)\n    generated_features.extend(cols)\n\n    return result_df, generated_features, generated_preds","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_base = \"responder_lag_datasets\"\nos.makedirs(output_base, exist_ok=True)\n\nresponder_lag_features, generated_preds = run_walk_forward(\n    input_base=data_path,\n    output_base=output_base,\n    id_col=id_col,\n    date_id=date_id,\n    time_id=time_id,\n    pred_cols=pred_cols,\n    input_cols=input_cols,\n    train_length=train_length,\n    valid_length=valid_length,\n    train_shift=train_shift, \n    retroactive_size=retroactive_size,\n    add_feature_func=responder_lag_feature_func\n)","metadata":{"_uuid":"d95e4503-b430-4555-8679-ad2736f51eae","_cell_guid":"9188bf52-7089-4831-9dd3-5ee3b3e4aff2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2024-12-23T13:01:10.726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"responder_lag_features","metadata":{"_uuid":"a5837f26-013d-4de0-8593-287630e07960","_cell_guid":"85ee318e-cb8c-44a1-8cc4-75f164f946dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2024-12-23T13:01:10.728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature(LAG)処理","metadata":{}},{"cell_type":"code","source":"import polars as pl\nfrom typing import List, Tuple\n\ndef create_raw_feature_lag(\n    df: pl.DataFrame,\n    pred_cols: List[str]\n) -> Tuple[pl.LazyFrame, List[str]]:\n    target_cols = [\"symbol_id\", \"date_id\", \"time_id_group\"] + pred_cols\n    df_lags = df.select(target_cols)\n\n    # 4) date_id = date_id + 1\n    df_lags = df_lags.with_columns(\n        (pl.col(\"date_id\") + 1).alias(\"date_id\")\n    )\n    rename_map = {}\n    for c in pred_cols:\n        rename_map[c] = f\"{c}_lag_1\"\n\n    df_lags = df_lags.rename(rename_map)\n\n    # リネーム後の列名一覧\n    generated_cols = list(rename_map.values())\n    \n    keys = [\"symbol_id\", \"date_id\", \"time_id_group\"]\n    return df_lags, generated_cols, keys\n\ndef create_feature_lag(ldf, cols):\n    generated_features = []\n    df_raw_lags, lags_cols, raw_lags_keys = create_raw_feature_lag(ldf, cols)\n\n    # symbol_id, date_id, time_id_group 単位\n    df_lags, gen_cols_symdate_group = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"symbol_id\", \"date_id\", \"time_id_group\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_symdate_group)\n\n    ## rolling特徴量\n    ### 4 time_id_groupまで見る\n    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 4, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n    generated_features.extend(generated_cols)\n\n    ### 4 * 7 time_id_groupまで見る\n    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 28, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n    generated_features.extend(generated_cols)\n\n    # date_id単位でラグ列を集計\n    df_lags_dateid, gen_cols_date = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_date)\n\n    df_lags_dateid, generated_cols = add_avg_change_and_volatility(df_lags_dateid, gen_cols_date, n = 7, group_keys=[\"date_id\"], sort_keys=[])\n    generated_features.extend(generated_cols)\n    \n    df_lags = df_lags.join(df_lags_dateid, on=[\"date_id\"], how=\"left\")\n\n    # symbol_id, date_id単位\n    df_lags_symdate, gen_cols_symdate = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols,key_by=[\"symbol_id\", \"date_id\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_symdate)\n    \n    df_lags_symdate, generated_cols = add_avg_change_and_volatility(df_lags_symdate, gen_cols_symdate, n = 7, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\"])\n    generated_features.extend(generated_cols)\n\n    df_lags = df_lags.join(df_lags_symdate, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n\n    # date_id, time_group_id単位\n    df_lags_datetime, gen_cols_datetime = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\", \"time_id_group\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n    generated_features.extend(gen_cols_datetime)\n   \n    df_lags_datetime, generated_cols = add_avg_change_and_volatility(df_lags_datetime, gen_cols_datetime, n = 7, group_keys=[\"date_id\"], sort_keys=[\"time_id_group\"])\n    generated_features.extend(generated_cols)\n    \n    df_lags = df_lags.join(df_lags_datetime, on=[\"date_id\",\"time_id_group\"], how=\"left\")\n\n    return df_lags, generated_features","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_lag_func(\n    df: pl.DataFrame,\n    pred_cols: List[str],\n    input_cols: List[str],\n    original_cols: List[str] | None = None\n) -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n    \"\"\"\n    featureに関する特徴量を計算\n    \"\"\"\n    generated_features: List[str] = []\n    generated_preds: List[str] = []\n\n    # Lazy化\n    ldf = df.lazy()\n\n    # 1) sort\n    ldf = ldf.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n\n    # 2) time_id_group (4分割)\n    ldf = ldf.with_columns(\n        pl.when(pl.col(\"time_id\") < threshold1).then(0)\n          .when(pl.col(\"time_id\") < threshold2).then(1)\n          .when(pl.col(\"time_id\") < threshold3).then(2)\n          .otherwise(3)\n          .cast(pl.Int32)\n          .alias(\"time_id_group\")\n    )\n    generated_features.extend([\"time_id_group\"])\n\n    #前日特徴量(symbol_id, date_id -1, time_group_id)\n    ldf, generated_cols = create_feature_lag(ldf, input_cols)\n    generated_features.extend(generated_cols)\n\n    return result_df, generated_features, generated_preds","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_base = \"feature_lag_datasets\"\nos.makedirs(output_base, exist_ok=True)\n\ngenerated_feature_lag, _ = run_walk_forward(\n    input_base=data_path,\n    output_base=output_base,\n    id_col=id_col,\n    date_id=date_id,\n    time_id=time_id,\n    pred_cols=pred_cols,\n    input_cols=input_cols,\n    train_length=train_length,\n    valid_length=valid_length,\n    train_shift=train_shift, \n    retroactive_size=retroactive_size,\n    add_feature_func=feature_lag_func\n)","metadata":{"_uuid":"ae1263be-b4da-481f-bbb3-276ed8ecf695","_cell_guid":"511871b6-87b4-439e-99f6-22c98faa3b2f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2024-12-23T13:01:10.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_feature_lag","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-23T13:01:10.730Z"}},"outputs":[],"execution_count":null}]}
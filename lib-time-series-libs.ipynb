{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f68ed4",
   "metadata": {
    "_cell_guid": "d56e7f40-d49e-4a03-89df-31470a3737ef",
    "_uuid": "f68866a2-e910-4cd1-b613-709d600662f0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:46.572000Z",
     "iopub.status.busy": "2024-12-26T05:22:46.571613Z",
     "iopub.status.idle": "2024-12-26T05:22:46.861924Z",
     "shell.execute_reply": "2024-12-26T05:22:46.860794Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.300983,
     "end_time": "2024-12-26T05:22:46.864433",
     "exception": false,
     "start_time": "2024-12-26T05:22:46.563450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "def generate_test_data(\n",
    "    base_path: str,\n",
    "    id_col: str, \n",
    "    date_id: str,  # 例: (\"date_id\", int)\n",
    "    time_id: str,  # 例: (\"time_id\", int)\n",
    "    pred_cols: list, # 例: [(\"responder_0\", float)]\n",
    "    input_cols: list,# 例: [(\"feature_00\", float)]\n",
    "    num_partitions=4,\n",
    "    num_id=2,\n",
    "    date_per_partition=2\n",
    "):\n",
    "    \"\"\"\n",
    "    id_col, date_id, time_id, pred_cols, input_colsをもとにテストデータを作成し、\n",
    "    partition_id=0～(num_partitions-1)までのParquetファイルに出力する。\n",
    "\n",
    "    各パーティションは、以下の条件でデータを生成：\n",
    "      - id_col: 0～(num_id-1)までのIDを用意\n",
    "      - date_id: partition i は [i*date_per_partition, (i+1)*date_per_partition-1]\n",
    "      - time_id: 0～24 (25個)\n",
    "    よって1パーティションあたり: date_per_partition * num_id * 25行。\n",
    "\n",
    "    pred_cols, input_colsはタプルのリスト [(col_name, type), ...]で指定し、\n",
    "    値は日付・ID・time_idに基づき決定的に割り当てる。\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # カラム名抽出\n",
    "    id_col_name = id_col[0]\n",
    "    date_col_name = date_id[0]\n",
    "    time_col_name = time_id[0]\n",
    "\n",
    "    # 1パーティションあたりの行数\n",
    "    time_per_day = 25\n",
    "    rows_per_partition = date_per_partition * num_id * time_per_day\n",
    "\n",
    "    # 全ての列名\n",
    "    all_col_names = [id_col_name, date_col_name, time_col_name] + [c[0] for c in pred_cols] + [c[0] for c in input_cols]\n",
    "\n",
    "    for i in range(num_partitions):\n",
    "        date_start = i * date_per_partition\n",
    "        date_end = date_start + date_per_partition\n",
    "\n",
    "        # データ生成\n",
    "        # 二重三重ループで決定的に割り当て\n",
    "        date_ids_list = []\n",
    "        id_list = []\n",
    "        time_ids_list = []\n",
    "\n",
    "        for d in range(date_start, date_end):\n",
    "            for sid in range(num_id):\n",
    "                for t in range(time_per_day):\n",
    "                    date_ids_list.append(d)\n",
    "                    id_list.append(sid)\n",
    "                    time_ids_list.append(t)\n",
    "\n",
    "        # ベースとなる数値計算用\n",
    "        # responder_0やfeature_00に割り当てるための基礎値\n",
    "        # 例：val = d*10000 + sid*100 + t\n",
    "        base_vals = []\n",
    "        for d_id, s_id, t_id in zip(date_ids_list, id_list, time_ids_list):\n",
    "            val = d_id * 10000 + s_id * 100 + t_id\n",
    "            base_vals.append(val)\n",
    "\n",
    "        # DataFrame用辞書\n",
    "        data = {\n",
    "            id_col_name: id_list,\n",
    "            date_col_name: date_ids_list,\n",
    "            time_col_name: time_ids_list,\n",
    "        }\n",
    "\n",
    "        # pred_cols割り当て（例：responder_0 = base_val + index_of_col）\n",
    "        for idx, (c_name, c_type) in enumerate(pred_cols):\n",
    "            # 例: responder_0 = base_val + idx\n",
    "            c_values = [v + idx for v in base_vals]\n",
    "            data[c_name] = c_values\n",
    "\n",
    "        # input_cols割り当て（例：feature_00 = base_val/1000）\n",
    "        for idx, (c_name, c_type) in enumerate(input_cols):\n",
    "            # 例: feature_00 = base_val / 1000.0\n",
    "            c_values = [v / 1000.0 + idx for v in base_vals]\n",
    "            data[c_name] = c_values\n",
    "\n",
    "        df = pl.DataFrame(data)\n",
    "        # ソート（id_col_name, date_col_name, time_col_name順）\n",
    "        df = df.sort([id_col_name, date_col_name, time_col_name])\n",
    "\n",
    "        partition_dir = os.path.join(base_path, f\"partition_id={i}\")\n",
    "        os.makedirs(partition_dir, exist_ok=True)\n",
    "        df.write_parquet(os.path.join(partition_dir, \"part-0.parquet\"))\n",
    "\n",
    "    print(\"Test data generated without rows_per_partition, using given columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fef50ec",
   "metadata": {
    "_cell_guid": "1283520c-2194-4f05-8452-190318a611f7",
    "_uuid": "6f344a3d-07fd-47b5-94c0-d2957fb84ecd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:46.876829Z",
     "iopub.status.busy": "2024-12-26T05:22:46.876460Z",
     "iopub.status.idle": "2024-12-26T05:22:46.884848Z",
     "shell.execute_reply": "2024-12-26T05:22:46.883882Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016891,
     "end_time": "2024-12-26T05:22:46.886976",
     "exception": false,
     "start_time": "2024-12-26T05:22:46.870085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('symbol_id', int),\n",
       " ('date_id', int),\n",
       " ('time_id', int),\n",
       " ('responder_0', float),\n",
       " ('feature_00', float)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力変数選択\n",
    "id_col = (\"symbol_id\", int)\n",
    "date_id = (\"date_id\", int)\n",
    "time_id = (\"time_id\", int)\n",
    "\n",
    "#preds_cols = [f\"responder_{i}\" for i in range(9)] # 目的変数\n",
    "pred_cols = [(\"responder_0\", float)]\n",
    "input_cols = [(\"feature_00\", float)] # 説明変数\n",
    "\n",
    "target_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0458fca",
   "metadata": {
    "_cell_guid": "cb46278d-0a29-4f3a-8a8b-973239921d9f",
    "_uuid": "1978295c-7d27-45ba-a755-0a53e8f742f6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:46.899355Z",
     "iopub.status.busy": "2024-12-26T05:22:46.898953Z",
     "iopub.status.idle": "2024-12-26T05:22:47.022681Z",
     "shell.execute_reply": "2024-12-26T05:22:47.021520Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.132541,
     "end_time": "2024-12-26T05:22:47.024976",
     "exception": false,
     "start_time": "2024-12-26T05:22:46.892435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data generated without rows_per_partition, using given columns.\n"
     ]
    }
   ],
   "source": [
    "generate_test_data(\"test\", id_col, date_id, time_id, pred_cols, input_cols, num_partitions=2, num_id=2, date_per_partition=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd826a0",
   "metadata": {
    "_cell_guid": "43f495ad-8380-4271-97c3-9e8d32ed4666",
    "_uuid": "203cd0c7-f167-40be-b6e6-55415500f526",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:47.037442Z",
     "iopub.status.busy": "2024-12-26T05:22:47.037057Z",
     "iopub.status.idle": "2024-12-26T05:22:47.102325Z",
     "shell.execute_reply": "2024-12-26T05:22:47.101224Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.07407,
     "end_time": "2024-12-26T05:22:47.104514",
     "exception": false,
     "start_time": "2024-12-26T05:22:47.030444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (150, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>0.001</td></tr><tr><td>0</td><td>0</td><td>2</td><td>2</td><td>0.002</td></tr><tr><td>0</td><td>0</td><td>3</td><td>3</td><td>0.003</td></tr><tr><td>0</td><td>0</td><td>4</td><td>4</td><td>0.004</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>5</td><td>20</td><td>50020</td><td>50.02</td></tr><tr><td>0</td><td>5</td><td>21</td><td>50021</td><td>50.021</td></tr><tr><td>0</td><td>5</td><td>22</td><td>50022</td><td>50.022</td></tr><tr><td>0</td><td>5</td><td>23</td><td>50023</td><td>50.023</td></tr><tr><td>0</td><td>5</td><td>24</td><td>50024</td><td>50.024</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (150, 5)\n",
       "┌───────────┬─────────┬─────────┬─────────────┬────────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ responder_0 ┆ feature_00 │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ ---         ┆ ---        │\n",
       "│ i64       ┆ i64     ┆ i64     ┆ i64         ┆ f64        │\n",
       "╞═══════════╪═════════╪═════════╪═════════════╪════════════╡\n",
       "│ 0         ┆ 0       ┆ 0       ┆ 0           ┆ 0.0        │\n",
       "│ 0         ┆ 0       ┆ 1       ┆ 1           ┆ 0.001      │\n",
       "│ 0         ┆ 0       ┆ 2       ┆ 2           ┆ 0.002      │\n",
       "│ 0         ┆ 0       ┆ 3       ┆ 3           ┆ 0.003      │\n",
       "│ 0         ┆ 0       ┆ 4       ┆ 4           ┆ 0.004      │\n",
       "│ …         ┆ …       ┆ …       ┆ …           ┆ …          │\n",
       "│ 0         ┆ 5       ┆ 20      ┆ 50020       ┆ 50.02      │\n",
       "│ 0         ┆ 5       ┆ 21      ┆ 50021       ┆ 50.021     │\n",
       "│ 0         ┆ 5       ┆ 22      ┆ 50022       ┆ 50.022     │\n",
       "│ 0         ┆ 5       ┆ 23      ┆ 50023       ┆ 50.023     │\n",
       "│ 0         ┆ 5       ┆ 24      ┆ 50024       ┆ 50.024     │\n",
       "└───────────┴─────────┴─────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pl.scan_parquet(\"/kaggle/working/test/partition_id=*/part-*.parquet\", glob=True)\n",
    "      .filter(pl.col(\"symbol_id\") == 0)\n",
    "      .collect()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b08bbac",
   "metadata": {
    "_cell_guid": "c6439978-96fc-4a1b-9a91-7d79e70c1db2",
    "_uuid": "58fd0698-e4de-4725-92fd-0844c061c56a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:47.117690Z",
     "iopub.status.busy": "2024-12-26T05:22:47.117349Z",
     "iopub.status.idle": "2024-12-26T05:22:47.129551Z",
     "shell.execute_reply": "2024-12-26T05:22:47.128580Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021575,
     "end_time": "2024-12-26T05:22:47.131674",
     "exception": false,
     "start_time": "2024-12-26T05:22:47.110099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing make_walk_forward_feature.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_walk_forward_feature.py\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "import polars as pl\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# =============================================================================\n",
    "# メタ情報を作る関数: 指定のファイル一覧から min/max(date_id) を取得\n",
    "# =============================================================================\n",
    "def create_metadata_from_files(\n",
    "    files: List[str],\n",
    "    date_id: str = \"date_id\",\n",
    "):\n",
    "    \"\"\"\n",
    "    files の一覧を走査し、各ファイルから date_id の min/max を取得。\n",
    "    返り値: List[ (index, min_date, max_date) ]\n",
    "    \"\"\"\n",
    "    meta = []\n",
    "    for i, fpath in enumerate(sorted(files)):\n",
    "        # Parquet読み込み（形式固定例）\n",
    "        df_all = pl.read_parquet(fpath, columns=[date_id])\n",
    "        min_id = df_all.select(pl.col(date_id).min()).item()\n",
    "        max_id = df_all.select(pl.col(date_id).max()).item()\n",
    "        meta.append((i, min_id, max_id))\n",
    "\n",
    "    print(\"meta: partition_index, min_date, max_date =\", meta)\n",
    "    return meta\n",
    "\n",
    "def partitions_for_range(\n",
    "    meta: List[tuple],\n",
    "    start_id: int,\n",
    "    end_id: int\n",
    "):\n",
    "    \"\"\"\n",
    "    meta: [(index, min_date, max_date), ...]\n",
    "    指定した日付範囲 [start_id, end_id] と重なるファイルの index を返す\n",
    "    \"\"\"\n",
    "    needed = []\n",
    "    for (idx, mini, maxi) in meta:\n",
    "        if maxi >= start_id and mini <= end_id:\n",
    "            needed.append(idx)\n",
    "    return needed\n",
    "\n",
    "# =============================================================================\n",
    "# シンプルにParquetだけ読み書きするヘルパー\n",
    "# =============================================================================\n",
    "def _read_parquet_file(\n",
    "    fpath: str,\n",
    "    cols_to_read: List[str],\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Parquetを指定列だけ読み込む\n",
    "    \"\"\"\n",
    "    return pl.read_parquet(fpath, columns=cols_to_read)\n",
    "\n",
    "def _write_parquet_file(\n",
    "    df: pl.DataFrame,\n",
    "    out_path: str,\n",
    "    compression: str = \"zstd\",\n",
    "    compression_level: int = 9\n",
    "):\n",
    "    \"\"\"\n",
    "    Parquet書き込み\n",
    "    \"\"\"\n",
    "    df.write_parquet(\n",
    "        out_path,\n",
    "        compression=compression,\n",
    "        compression_level=compression_level\n",
    "    )\n",
    "\n",
    "def make_feature_walk_forward(\n",
    "    files: List[str],\n",
    "    output_base: str,\n",
    "    id_col: str,\n",
    "    date_id: str,\n",
    "    time_id: str,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    train_length: int,\n",
    "    retroactive_size: int,\n",
    "    add_feature_func\n",
    "):\n",
    "    original_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "    print(\"original_cols:\", original_cols)\n",
    "\n",
    "    # メタ情報作成\n",
    "    meta = create_metadata_from_files(files, date_id=date_id)\n",
    "    max_date_id_global = max(m[2] for m in meta)\n",
    "\n",
    "    # 【変更点】metaから全パーティションの最小date_idを取り、そこからスタート\n",
    "    min_date_id_global = min(m[1] for m in meta)\n",
    "    # start_of_train = 0  # 従来\n",
    "    start_of_train = min_date_id_global  # 修正後\n",
    "\n",
    "    sprint_num = 1\n",
    "\n",
    "    gen_feat = None\n",
    "    while True:\n",
    "        if start_of_train > max_date_id_global:\n",
    "            print(\"break because start_of_train > max_date_id_global\")\n",
    "            break\n",
    "\n",
    "        train_start = start_of_train\n",
    "        train_end = start_of_train + train_length\n",
    "        if train_end > max_date_id_global + 1:\n",
    "            train_end = max_date_id_global + 1\n",
    "\n",
    "        extended_start = max(train_start - retroactive_size, 0)\n",
    "        required_start = extended_start\n",
    "        required_end   = train_end - 1\n",
    "\n",
    "        needed_partitions = partitions_for_range(meta, required_start, required_end)\n",
    "        if len(needed_partitions) == 0:\n",
    "            print(\"break because len(needed_partitions) == 0\")\n",
    "            break\n",
    "\n",
    "        df_current = None\n",
    "        for idx in needed_partitions:\n",
    "            fpath = files[idx]\n",
    "            print(\"read\", fpath)\n",
    "            df_part = _read_parquet_file(fpath, cols_to_read=[\n",
    "                c for c in original_cols if c in pl.read_parquet(fpath, n_rows=1).columns\n",
    "            ])\n",
    "            if df_current is None:\n",
    "                df_current = df_part\n",
    "            else:\n",
    "                df_current = pl.concat([df_current, df_part], how=\"vertical\")\n",
    "\n",
    "        if df_current is None or len(df_current) == 0:\n",
    "            print(\"break because df_current is None or len(df_current) == 0\")\n",
    "            break\n",
    "\n",
    "        # フィルタ\n",
    "        df_current = df_current.filter(\n",
    "            (pl.col(date_id) >= extended_start) & (pl.col(date_id) < train_end)\n",
    "        )\n",
    "\n",
    "        print(f\"Sprint {sprint_num}: extended_start={extended_start}, train_end={train_end}, rows={len(df_current)}\")\n",
    "\n",
    "        # 特徴量作成\n",
    "        df_out, gen_feat, gen_pred = add_feature_func(\n",
    "            df_current,\n",
    "            pred_cols=pred_cols,\n",
    "            input_cols=input_cols,\n",
    "            original_cols=original_cols\n",
    "        )\n",
    "\n",
    "        # train_df\n",
    "        train_df = df_out.filter(\n",
    "            (pl.col(date_id) >= train_start) & (pl.col(date_id) < train_end)\n",
    "        )\n",
    "\n",
    "        # 出力先\n",
    "        sprint_dir = os.path.join(output_base, f\"sprint{sprint_num}\")\n",
    "        os.makedirs(sprint_dir, exist_ok=True)\n",
    "        train_path = os.path.join(sprint_dir, \"train.parquet\")\n",
    "\n",
    "        train_df = train_df.collect()\n",
    "        _write_parquet_file(train_df, train_path)\n",
    "        print(f\"  -> wrote {train_path}, rows={len(train_df)}\")\n",
    "\n",
    "        del df_current, df_out, train_df\n",
    "        gc.collect()\n",
    "\n",
    "        # 次のスプリントへ\n",
    "        start_of_train = train_end\n",
    "        sprint_num += 1\n",
    "\n",
    "    print(\"Done (make_feature_walk_forward).\")\n",
    "    gc.collect()\n",
    "    return gen_feat\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) train+valid分割 + train_shiftありのウォークフォワード\n",
    "# =============================================================================\n",
    "def make_feature_falk_forward_train_valid(\n",
    "    # 入力\n",
    "    files: List[str],\n",
    "    output_base: str,\n",
    "    # カラム指定\n",
    "    id_col: str,\n",
    "    date_id: str,\n",
    "    time_id: str,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    # パラメータ\n",
    "    train_length: int,\n",
    "    valid_length: int,\n",
    "    train_shift: int,\n",
    "    retroactive_size: int,\n",
    "    # 特徴量作成関数\n",
    "    add_feature_func\n",
    "):\n",
    "    \"\"\"\n",
    "    従来の train+valid + train_shift があるウォークフォワード版。\n",
    "    \"\"\"\n",
    "    original_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "    print(\"original_cols:\", original_cols)\n",
    "\n",
    "    meta = create_metadata_from_files(files, date_id=date_id)\n",
    "    max_date_id_global = max(m[2] for m in meta)\n",
    "\n",
    "    start_of_train = 0\n",
    "    sprint_num = 1\n",
    "\n",
    "    prev_gen_feature_str = None\n",
    "    prev_gen_preds_str   = None\n",
    "\n",
    "    while True:\n",
    "        if start_of_train > max_date_id_global:\n",
    "            break\n",
    "\n",
    "        train_start = start_of_train\n",
    "        train_end   = start_of_train + train_length\n",
    "        valid_start = train_end\n",
    "        valid_end   = train_end + valid_length\n",
    "\n",
    "        if valid_start > max_date_id_global:\n",
    "            break\n",
    "        if valid_end > max_date_id_global + 1:\n",
    "            valid_end = max_date_id_global + 1\n",
    "\n",
    "        extended_start = max(train_start - retroactive_size, 0)\n",
    "        required_start = extended_start\n",
    "        required_end   = valid_end - 1\n",
    "\n",
    "        needed_partitions = partitions_for_range(meta, required_start, required_end)\n",
    "        if len(needed_partitions) == 0:\n",
    "            break\n",
    "\n",
    "        df_current = None\n",
    "        for idx in needed_partitions:\n",
    "            fpath = files[idx]\n",
    "            df_part = _read_parquet_file(fpath, cols_to_read=[\n",
    "                c for c in original_cols if c in pl.read_parquet(fpath, n_rows=1).columns\n",
    "            ])\n",
    "            if df_current is None:\n",
    "                df_current = df_part\n",
    "            else:\n",
    "                df_current = pl.concat([df_current, df_part], how=\"vertical\")\n",
    "\n",
    "        if df_current is None or len(df_current) == 0:\n",
    "            break\n",
    "\n",
    "        # filter\n",
    "        df_current = df_current.filter(\n",
    "            (pl.col(date_id) >= extended_start) & (pl.col(date_id) < valid_end)\n",
    "        )\n",
    "        print(f\"Retroactive={extended_start}, Train=[{train_start},{train_end}), Valid=[{valid_start},{valid_end}), rows={len(df_current)}\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        df_out, generated_feature, generated_preds = add_feature_func(\n",
    "            df_current,\n",
    "            pred_cols=pred_cols,\n",
    "            input_cols=input_cols,\n",
    "            original_cols=original_cols\n",
    "        )\n",
    "\n",
    "        # 前回と比較\n",
    "        current_gen_feature_str = repr(generated_feature)\n",
    "        current_gen_preds_str   = repr(generated_preds)\n",
    "        if prev_gen_feature_str is not None:\n",
    "            if current_gen_feature_str != prev_gen_feature_str:\n",
    "                raise Exception(\"generated_feature differs from previous sprint!\")\n",
    "        if prev_gen_preds_str is not None:\n",
    "            if current_gen_preds_str != prev_gen_preds_str:\n",
    "                raise Exception(\"generated_preds differs from previous sprint!\")\n",
    "        prev_gen_feature_str = current_gen_feature_str\n",
    "        prev_gen_preds_str   = current_gen_preds_str\n",
    "\n",
    "        train_df = df_out.filter((pl.col(date_id) >= train_start) & (pl.col(date_id) < train_end))\n",
    "        valid_df = df_out.filter((pl.col(date_id) >= valid_start) & (pl.col(date_id) < valid_end))\n",
    "\n",
    "        sprint_dir = os.path.join(output_base, f\"sprint{sprint_num}\")\n",
    "        os.makedirs(sprint_dir, exist_ok=True)\n",
    "\n",
    "        train_path = os.path.join(sprint_dir, \"train.parquet\")\n",
    "        valid_path = os.path.join(sprint_dir, \"valid.parquet\")\n",
    "\n",
    "        train_df = train_df.collect()\n",
    "        valid_df = valid_df.collect()\n",
    "        _write_parquet_file(train_df, train_path)\n",
    "        _write_parquet_file(valid_df, valid_path)\n",
    "        print(f\"Sprint {sprint_num}: Train=[{train_start},{train_end}), Valid=[{valid_start},{valid_end})\")\n",
    "        print(f\"  -> train={train_path}, rows={len(train_df)}\")\n",
    "        print(f\"  -> valid={valid_path}, rows={len(valid_df)}\")\n",
    "\n",
    "        del df_current, df_out, train_df, valid_df\n",
    "        gc.collect()\n",
    "\n",
    "        start_of_train += train_shift\n",
    "        sprint_num += 1\n",
    "\n",
    "    print(\"Done (make_feature_falk_forward_train_valid).\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3ec605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:47.145491Z",
     "iopub.status.busy": "2024-12-26T05:22:47.145029Z",
     "iopub.status.idle": "2024-12-26T05:22:47.158766Z",
     "shell.execute_reply": "2024-12-26T05:22:47.157717Z"
    },
    "papermill": {
     "duration": 0.023674,
     "end_time": "2024-12-26T05:22:47.161026",
     "exception": false,
     "start_time": "2024-12-26T05:22:47.137352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing time_serise_feature.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile time_serise_feature.py\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "\n",
    "def create_stat_features_by(\n",
    "    ldf: pl.LazyFrame,\n",
    "    cols: List[str],\n",
    "    key_by: List[str],\n",
    "    agg_funcs: List[str],\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    ldf を key_by で group_by し、cols に対して指定された agg_funcs を実行する汎用関数。\n",
    "    例:\n",
    "      agg_funcs に [\"mean\",\"std\",\"min\",\"max\",\"median\",\"sum\",\"count\",\"n_unique\",\n",
    "                    \"last\",\"first\",\"skew\",\"kurtosis\",\"cv\", \"q0.25\",\"q0.75\" ...]\n",
    "      のような文字列を指定できる。\n",
    "\n",
    "    戻り値:\n",
    "      (ldf_agg, generated_cols):\n",
    "        ldf_agg : group_by(key_by).agg(...) の結果 (key_by + 各集計列 を持つ LazyFrame)\n",
    "        generated_cols : 新しく生成された列名のリスト\n",
    "\n",
    "    注意:\n",
    "      - \"q0.25\" 等のquantile形式は \"q0.x\" のフォーマットで xをfloatとして解釈し expr.quantile(x)。\n",
    "      - \"cv\" (coefficient of variation) = std/mean (mean=0に注意)\n",
    "      - skew, kurtosis は Polars のバージョンによっては使えない場合あり\n",
    "      - rolling_meanなどのウィンドウ関数はこのgroup_by集約とは別物\n",
    "\n",
    "    例:\n",
    "      aggregator_mapに含まれる文字列一覧を \"agg_funcs\" で指定\n",
    "        -> \"mean\",\"std\",\"cv\",\"q0.25\",\"skew\"など\n",
    "    \"\"\"\n",
    "\n",
    "    # Polarsでサポートする単一スカラー集約を文字列→lambda で定義\n",
    "    def parse_quantile(alias: str, cexpr: pl.Expr) -> pl.Expr:\n",
    "        # 例: alias == \"q0.25\" → 0.25\n",
    "        #     alias == \"q0.75\" → 0.75\n",
    "        m = re.match(r\"q0\\.(\\d+)\", alias)  # 例: \"q0.25\" -> group(1)==\"25\"\n",
    "        if not m:\n",
    "            # 不正形式ならそのまま col\n",
    "            return cexpr\n",
    "        float_str = \"0.\" + m.group(1)\n",
    "        q = float(float_str)\n",
    "        return cexpr.quantile(q)  # quantile(0.25 / 0.75 etc.)\n",
    "\n",
    "    aggregator_map = {\n",
    "        \"mean\":      lambda cexpr: cexpr.mean(),\n",
    "        \"std\":       lambda cexpr: cexpr.std(),\n",
    "        \"min\":       lambda cexpr: cexpr.min(),\n",
    "        \"max\":       lambda cexpr: cexpr.max(),\n",
    "        \"median\":    lambda cexpr: cexpr.median(),\n",
    "        \"sum\":       lambda cexpr: cexpr.sum(),\n",
    "        \"count\":     lambda cexpr: cexpr.count(),\n",
    "        \"n_unique\":  lambda cexpr: cexpr.n_unique(),\n",
    "        \"last\":      lambda cexpr: cexpr.last(),\n",
    "        \"first\":     lambda cexpr: cexpr.first(),\n",
    "        \"skew\":      lambda cexpr: cexpr.skew(),\n",
    "        \"kurtosis\":  lambda cexpr: cexpr.kurtosis(),\n",
    "        # 変動係数 (coefficient of variation)\n",
    "        \"cv\":        lambda cexpr: (cexpr.std() / cexpr.mean()),  # mean=0注意\n",
    "    }\n",
    "\n",
    "    agg_exprs = []\n",
    "    generated_cols: List[str] = []\n",
    "\n",
    "    for c in cols:\n",
    "        for f in agg_funcs:\n",
    "            alias_name = f\"{c}_{'_'.join(key_by)}_{f}\"\n",
    "\n",
    "            # 1) quantile系:  \"q0.25\", \"q0.50\" etc.\n",
    "            if f.startswith(\"q0.\"):\n",
    "                expr = parse_quantile(f, pl.col(c)).alias(alias_name)\n",
    "                agg_exprs.append(expr)\n",
    "                generated_cols.append(alias_name)\n",
    "                continue\n",
    "\n",
    "            # 2) aggregator_map にあるか？\n",
    "            aggregator = aggregator_map.get(f, None)\n",
    "            if aggregator is not None:\n",
    "                expr = aggregator(pl.col(c)).alias(alias_name)\n",
    "                agg_exprs.append(expr)\n",
    "                generated_cols.append(alias_name)\n",
    "            else:\n",
    "                raise Exception(f\"create_stat_features_by: not defined {f}\")\n",
    "\n",
    "    # group_by and agg\n",
    "    ldf_agg = ldf.group_by(key_by).agg(agg_exprs)\n",
    "\n",
    "    return ldf_agg, generated_cols\n",
    "\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "\n",
    "def add_rolling_stats(\n",
    "    ldf: pl.LazyFrame,\n",
    "    cols: List[str],\n",
    "    sort_keys: List[str],\n",
    "    group_keys: List[str],\n",
    "    n: int = 5,\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    過去n期間の rolling_std(ボラティリティ) と rolling_mean を計算し、新しい列を追加する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.LazyFrame\n",
    "        処理対象のLazyFrame\n",
    "    cols : List[str]\n",
    "        計算対象の数値カラム ([\"feature_00\",\"feature_01\"]など)\n",
    "    sort_keys : List[str]\n",
    "        時系列順を保証するためにソートするカラムのリスト\n",
    "        例: [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "    group_keys : List[str]\n",
    "        rolling計算を「どのカラム単位でパーティション切る」か\n",
    "        例: [\"symbol_id\"]\n",
    "    n : int, optional\n",
    "        rollingのウィンドウサイズ (デフォルト=5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf_out, generated_cols):\n",
    "      ldf_out       : 新しい列を追加したLazyFrame\n",
    "      generated_cols: 作成された列の名前一覧\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ソート (時系列順を正しく)\n",
    "    if sort_keys:\n",
    "        ldf = ldf.sort(by=sort_keys)\n",
    "\n",
    "    # 2) group_keys があれば over() でパーティション分割\n",
    "    def rolling_std_expr(col: str):\n",
    "        if group_keys:\n",
    "            return pl.col(col).rolling_std(window_size=n).over(group_keys)\n",
    "        else:\n",
    "            return pl.col(col).rolling_std(window_size=n)\n",
    "\n",
    "    def rolling_mean_expr(col: str):\n",
    "        if group_keys:\n",
    "            return pl.col(col).rolling_mean(window_size=n).over(group_keys)\n",
    "        else:\n",
    "            return pl.col(col).rolling_mean(window_size=n)\n",
    "\n",
    "    new_cols = []\n",
    "    exprs = []\n",
    "\n",
    "    for col_name in cols:\n",
    "        # 出力列名\n",
    "        std_col_name = f\"{col_name}_rolling_std_{n}\"\n",
    "        mean_col_name = f\"{col_name}_rolling_mean_{n}\"\n",
    "        new_cols.extend([std_col_name, mean_col_name])\n",
    "\n",
    "        # rolling_std\n",
    "        exprs.append(\n",
    "            rolling_std_expr(col_name).alias(std_col_name)\n",
    "        )\n",
    "        # rolling_mean\n",
    "        exprs.append(\n",
    "            rolling_mean_expr(col_name).alias(mean_col_name)\n",
    "        )\n",
    "\n",
    "    ldf_out = ldf.with_columns(exprs)\n",
    "\n",
    "    return ldf_out, new_cols\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "\n",
    "def add_rolling_stats(\n",
    "    ldf: pl.LazyFrame,\n",
    "    cols: List[str],\n",
    "    sort_keys: List[str],\n",
    "    group_keys: List[str],\n",
    "    n: int = 5,\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    過去n期間の rolling_std(ボラティリティ) と rolling_mean を計算し、新しい列を追加する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.LazyFrame\n",
    "        処理対象のLazyFrame\n",
    "    cols : List[str]\n",
    "        計算対象の数値カラム ([\"feature_00\",\"feature_01\"]など)\n",
    "    sort_keys : List[str]\n",
    "        時系列順を保証するためにソートするカラムのリスト\n",
    "        例: [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "    group_keys : List[str]\n",
    "        rolling計算を「どのカラム単位でパーティション切る」か\n",
    "        例: [\"symbol_id\"]\n",
    "    n : int, optional\n",
    "        rollingのウィンドウサイズ (デフォルト=5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf_out, generated_cols):\n",
    "      ldf_out       : 新しい列を追加したLazyFrame\n",
    "      generated_cols: 作成された列の名前一覧\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ソート (時系列順を正しく)\n",
    "    if sort_keys:\n",
    "        ldf = ldf.sort(by=sort_keys)\n",
    "\n",
    "    # 2) group_keys があれば over() でパーティション分割\n",
    "    def rolling_std_expr(col: str):\n",
    "        if group_keys:\n",
    "            return pl.col(col).rolling_std(window_size=n).over(group_keys)\n",
    "        else:\n",
    "            return pl.col(col).rolling_std(window_size=n)\n",
    "\n",
    "    def rolling_mean_expr(col: str):\n",
    "        if group_keys:\n",
    "            return pl.col(col).rolling_mean(window_size=n).over(group_keys)\n",
    "        else:\n",
    "            return pl.col(col).rolling_mean(window_size=n)\n",
    "\n",
    "    new_cols = []\n",
    "    exprs = []\n",
    "\n",
    "    for col_name in cols:\n",
    "        # 出力列名\n",
    "        std_col_name = f\"{col_name}_rolling_std_{n}\"\n",
    "        mean_col_name = f\"{col_name}_rolling_mean_{n}\"\n",
    "        new_cols.extend([std_col_name, mean_col_name])\n",
    "\n",
    "        # rolling_std\n",
    "        exprs.append(\n",
    "            rolling_std_expr(col_name).alias(std_col_name)\n",
    "        )\n",
    "        # rolling_mean\n",
    "        exprs.append(\n",
    "            rolling_mean_expr(col_name).alias(mean_col_name)\n",
    "        )\n",
    "\n",
    "    ldf_out = ldf.with_columns(exprs)\n",
    "\n",
    "    return ldf_out, new_cols\n",
    "\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "\n",
    "def add_change_rate(\n",
    "    ldf: pl.LazyFrame,\n",
    "    cols: List[str],\n",
    "    sort_keys: List[str],\n",
    "    group_keys: List[str],\n",
    "    n: int = 5,\n",
    "    use_log_return: bool = False,\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    t-nとの変化率 (log/ratio) * 100 を計算し、新しい列を追加する。\n",
    "\n",
    "    例:\n",
    "      - use_log_return=True  -> (log(current) - log(shift(n))) * 100\n",
    "      - use_log_return=False -> ((current / shift(n)) - 1) * 100\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.LazyFrame\n",
    "        処理対象のLazyFrame\n",
    "    cols : List[str]\n",
    "        計算対象の数値カラム ([\"feature_00\",\"feature_01\"]など)\n",
    "    sort_keys : List[str]\n",
    "        時系列順を保証するためにソートするカラムのリスト\n",
    "        例: [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "    group_keys : List[str]\n",
    "        shiftを「どのカラム単位でパーティション切る」か\n",
    "        例: [\"symbol_id\"]\n",
    "    n : int, optional\n",
    "        t-n の期間差 (デフォルト=5)\n",
    "    use_log_return : bool, optional\n",
    "        Trueならlogリターンで計算 (デフォルト=False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf_out, generated_cols):\n",
    "      ldf_out       : 新しい列を追加したLazyFrame\n",
    "      generated_cols: 作成された列の名前一覧\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ソート (時系列順を正しく)\n",
    "    if sort_keys:\n",
    "        ldf = ldf.sort(by=sort_keys)\n",
    "\n",
    "    # 2) group_keys があれば over() でパーティション分割\n",
    "    def shift_n_expr(col: str, periods: int):\n",
    "        if group_keys:\n",
    "            return pl.col(col).shift(periods).over(group_keys)\n",
    "        else:\n",
    "            return pl.col(col).shift(periods)\n",
    "\n",
    "    # 3) カラムごとに t-n との変化率列を追加\n",
    "    new_cols = []\n",
    "    exprs = []\n",
    "    for col_name in cols:\n",
    "        # 出力列名\n",
    "        rate_col_name = f\"{col_name}_change_rate_{n}\"\n",
    "        new_cols.append(rate_col_name)\n",
    "\n",
    "        # (log(current) - log(shift(n))) * 100\n",
    "        #  or\n",
    "        # ((current / shift(n)) - 1) * 100\n",
    "        if use_log_return:\n",
    "            exprs.append(\n",
    "                (\n",
    "                    (pl.col(col_name).log() - shift_n_expr(col_name, n).log()) * 100\n",
    "                ).alias(rate_col_name)\n",
    "            )\n",
    "        else:\n",
    "            exprs.append(\n",
    "                (\n",
    "                    ((pl.col(col_name) / shift_n_expr(col_name, n)) - 1) * 100\n",
    "                ).alias(rate_col_name)\n",
    "            )\n",
    "\n",
    "    ldf_out = ldf.with_columns(exprs)\n",
    "\n",
    "    return ldf_out, new_cols\n",
    "\n",
    "\n",
    "def add_rolling_feature(\n",
    "    ldf: pl.DataFrame,\n",
    "    cols: List[str],\n",
    "    group_keys: List[str],\n",
    "    sort_keys: List[str],\n",
    "    lags: List[int],\n",
    "    use_log_return: bool=False,\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    複数のウィンドウサイズ(lags)を使って Rolling 系特徴量と変化率を追加する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        処理対象のDataFrame\n",
    "    cols : List[str]\n",
    "        計算対象となるカラム一覧\n",
    "    n : int\n",
    "        (必要に応じて使う) 単一ウィンドウサイズ\n",
    "    group_keys : List[str]\n",
    "        RollingやShiftをグルーピング単位で計算するためのキー\n",
    "    sort_keys : List[str]\n",
    "        ソート順序を保障するためのキー\n",
    "    use_log_return : bool\n",
    "        Trueならlog変化率で計算\n",
    "    lags : List[int]\n",
    "        計算したい複数のウィンドウサイズ (例: [4, 28] など)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (df, generated_features, generated_preds):\n",
    "      df                : 新しい列を追加したDataFrame\n",
    "      generated_features: 今回生成した特徴量カラム名のリスト\n",
    "      generated_preds   : ここでは空、用途に合わせて拡張可能\n",
    "    \"\"\"\n",
    "\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "\n",
    "    for lag in lags:\n",
    "        # rolling std & mean\n",
    "        ldf, gen_cols = add_rolling_stats(\n",
    "            ldf=ldf,\n",
    "            cols=cols,\n",
    "            n=lag,\n",
    "            group_keys=group_keys,\n",
    "            sort_keys=sort_keys\n",
    "        )\n",
    "        generated_features.extend(gen_cols)\n",
    "\n",
    "        # t-n 変化率\n",
    "        ldf, gen_cols = add_change_rate(\n",
    "            ldf=ldf,\n",
    "            cols=cols,\n",
    "            n=lag,\n",
    "            group_keys=group_keys,\n",
    "            sort_keys=sort_keys,\n",
    "            use_log_return=use_log_return\n",
    "        )\n",
    "        generated_features.extend(gen_cols)\n",
    "\n",
    "    return ldf, generated_features, generated_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d31a6d",
   "metadata": {
    "_cell_guid": "adfe1266-3f38-4319-8ea8-5e9ce9effc64",
    "_uuid": "ea4430b5-8fd6-47f2-b300-191a399462ad",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:47.174623Z",
     "iopub.status.busy": "2024-12-26T05:22:47.173842Z",
     "iopub.status.idle": "2024-12-26T05:22:47.187410Z",
     "shell.execute_reply": "2024-12-26T05:22:47.186344Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022926,
     "end_time": "2024-12-26T05:22:47.189853",
     "exception": false,
     "start_time": "2024-12-26T05:22:47.166927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time_serise_feature import *\n",
    "\n",
    "def create_prev_features(df: pl.DataFrame, pred_cols: list[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    agg_expr = []\n",
    "    for c in pred_cols:\n",
    "        agg_expr.append(pl.col(c).first().alias(f\"{c}_prev_first\"))\n",
    "        agg_expr.append(pl.col(c).last().alias(f\"{c}_prev_last\"))\n",
    "        agg_expr.append(pl.col(c).max().alias(f\"{c}_prev_max\"))\n",
    "        agg_expr.append(pl.col(c).min().alias(f\"{c}_prev_min\"))\n",
    "        agg_expr.append(pl.col(c).std().alias(f\"{c}_prev_std\"))\n",
    "\n",
    "    df = (\n",
    "        df.group_by([\"symbol_id\", \"date_id\"])\n",
    "        .agg(agg_expr)\n",
    "        .with_columns((pl.col(\"date_id\") + 1).alias(\"date_id_next\"))\n",
    "        .drop(\"date_id\")\n",
    "        .rename({\"date_id_next\": \"date_id\"})\n",
    "    )\n",
    "    cols = df.collect_schema().keys() - [\"symbol_id\", \"date_id\"]\n",
    "    return df, list(cols)\n",
    "\n",
    "window_size = 7\n",
    "def add_feature(\n",
    "    df: pl.DataFrame, \n",
    "    pred_cols, \n",
    "    input_cols,\n",
    "    original_cols\n",
    ") -> (pl.DataFrame, pl.DataFrame):\n",
    "    # add_rolling_preds_funcを使用してprev_day_aggs相当を計算\n",
    "    generated_features = []\n",
    "    df = df.lazy()\n",
    "    \n",
    "    df_prev, generated_cols = create_prev_features(df, pred_cols)\n",
    "    generated_features.extend(generated_cols)\n",
    "    df = df.join(df_prev, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n",
    "    \n",
    "    df, generated_cols, _ = add_rolling_feature(df, pred_cols, lags = [7, 14], group_keys=[\"symbol_id\"], sort_keys=[\"symbol_id\", \"date_id\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "    \n",
    "    \n",
    "    return df, generated_features, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b22d3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:47.203220Z",
     "iopub.status.busy": "2024-12-26T05:22:47.202877Z",
     "iopub.status.idle": "2024-12-26T05:22:50.134747Z",
     "shell.execute_reply": "2024-12-26T05:22:50.133362Z"
    },
    "papermill": {
     "duration": 2.941034,
     "end_time": "2024-12-26T05:22:50.136773",
     "exception": false,
     "start_time": "2024-12-26T05:22:47.195739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files ['/kaggle/working/test/partition_id=0/part-0.parquet', '/kaggle/working/test/partition_id=1/part-0.parquet']\n",
      "original_cols: ['symbol_id', 'date_id', 'time_id', 'responder_0', 'feature_00']\n",
      "meta: partition_index, min_date, max_date = [(0, 0, 2), (1, 3, 5)]\n",
      "read /kaggle/working/test/partition_id=0/part-0.parquet\n",
      "Sprint 1: extended_start=0, train_end=2, rows=100\n",
      "  -> wrote test_make_feature_walk_forward/sprint1/train.parquet, rows=100\n",
      "read /kaggle/working/test/partition_id=0/part-0.parquet\n",
      "read /kaggle/working/test/partition_id=1/part-0.parquet\n",
      "Sprint 2: extended_start=1, train_end=4, rows=150\n",
      "  -> wrote test_make_feature_walk_forward/sprint2/train.parquet, rows=100\n",
      "read /kaggle/working/test/partition_id=1/part-0.parquet\n",
      "Sprint 3: extended_start=3, train_end=6, rows=150\n",
      "  -> wrote test_make_feature_walk_forward/sprint3/train.parquet, rows=100\n",
      "break because start_of_train > max_date_id_global\n",
      "Done (make_feature_walk_forward).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['responder_0_prev_first',\n",
       " 'responder_0_prev_last',\n",
       " 'responder_0_prev_max',\n",
       " 'responder_0_prev_min',\n",
       " 'responder_0_prev_std',\n",
       " 'responder_0_rolling_std_7',\n",
       " 'responder_0_rolling_mean_7',\n",
       " 'responder_0_change_rate_7',\n",
       " 'responder_0_rolling_std_14',\n",
       " 'responder_0_rolling_mean_14',\n",
       " 'responder_0_change_rate_14']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from make_walk_forward_feature import make_feature_walk_forward\n",
    "\n",
    "data_path = \"/kaggle/working/test\"\n",
    "parquet_files = glob.glob(os.path.join(data_path, \"partition_id=*\", \"*.parquet\"))\n",
    "\n",
    "output_base = \"test_make_feature_walk_forward\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "test_train_length = 2\n",
    "test_retroactive_size = 1\n",
    "\n",
    "print(\"parquet_files\", parquet_files)\n",
    "\n",
    "gen_feat = make_feature_walk_forward(\n",
    "    files=parquet_files,\n",
    "    output_base=output_base,\n",
    "    id_col=id_col[0],\n",
    "    date_id=date_id[0], \n",
    "    time_id=time_id[0],\n",
    "    pred_cols=[c[0] for c in pred_cols],\n",
    "    input_cols=[c[0] for c in input_cols],\n",
    "    train_length=test_train_length,\n",
    "    retroactive_size=test_retroactive_size,\n",
    "    add_feature_func=add_feature\n",
    ")\n",
    "gen_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0e1eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:50.151601Z",
     "iopub.status.busy": "2024-12-26T05:22:50.150433Z",
     "iopub.status.idle": "2024-12-26T05:22:50.161306Z",
     "shell.execute_reply": "2024-12-26T05:22:50.160302Z"
    },
    "papermill": {
     "duration": 0.020298,
     "end_time": "2024-12-26T05:22:50.163717",
     "exception": false,
     "start_time": "2024-12-26T05:22:50.143419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (50, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th><th>responder_0_prev_first</th><th>responder_0_prev_last</th><th>responder_0_prev_max</th><th>responder_0_prev_min</th><th>responder_0_prev_std</th><th>responder_0_rolling_std_7</th><th>responder_0_rolling_mean_7</th><th>responder_0_change_rate_7</th><th>responder_0_rolling_std_14</th><th>responder_0_rolling_mean_14</th><th>responder_0_change_rate_14</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>0</td><td>0</td><td>100</td><td>0.1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>0</td><td>1</td><td>101</td><td>0.101</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>0</td><td>2</td><td>102</td><td>0.102</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>0</td><td>3</td><td>103</td><td>0.103</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>0</td><td>4</td><td>104</td><td>0.104</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1</td><td>1</td><td>20</td><td>10120</td><td>10.12</td><td>100</td><td>124</td><td>124</td><td>100</td><td>7.359801</td><td>2.160247</td><td>10117.0</td><td>0.069218</td><td>4.1833</td><td>10113.5</td><td>0.138532</td></tr><tr><td>1</td><td>1</td><td>21</td><td>10121</td><td>10.121</td><td>100</td><td>124</td><td>124</td><td>100</td><td>7.359801</td><td>2.160247</td><td>10118.0</td><td>0.069211</td><td>4.1833</td><td>10114.5</td><td>0.138518</td></tr><tr><td>1</td><td>1</td><td>22</td><td>10122</td><td>10.122</td><td>100</td><td>124</td><td>124</td><td>100</td><td>7.359801</td><td>2.160247</td><td>10119.0</td><td>0.069204</td><td>4.1833</td><td>10115.5</td><td>0.138504</td></tr><tr><td>1</td><td>1</td><td>23</td><td>10123</td><td>10.123</td><td>100</td><td>124</td><td>124</td><td>100</td><td>7.359801</td><td>2.160247</td><td>10120.0</td><td>0.069197</td><td>4.1833</td><td>10116.5</td><td>0.13849</td></tr><tr><td>1</td><td>1</td><td>24</td><td>10124</td><td>10.124</td><td>100</td><td>124</td><td>124</td><td>100</td><td>7.359801</td><td>2.160247</td><td>10121.0</td><td>0.06919</td><td>4.1833</td><td>10117.5</td><td>0.138477</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (50, 16)\n",
       "┌───────────┬─────────┬─────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ responder_ ┆ … ┆ responder_ ┆ responder_ ┆ responder ┆ responder │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ 0          ┆   ┆ 0_change_r ┆ 0_rolling_ ┆ _0_rollin ┆ _0_change │\n",
       "│ i64       ┆ i64     ┆ i64     ┆ ---        ┆   ┆ ate_7      ┆ std_14     ┆ g_mean_14 ┆ _rate_14  │\n",
       "│           ┆         ┆         ┆ i64        ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│           ┆         ┆         ┆            ┆   ┆ f64        ┆ f64        ┆ f64       ┆ f64       │\n",
       "╞═══════════╪═════════╪═════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 1         ┆ 0       ┆ 0       ┆ 100        ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n",
       "│ 1         ┆ 0       ┆ 1       ┆ 101        ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n",
       "│ 1         ┆ 0       ┆ 2       ┆ 102        ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n",
       "│ 1         ┆ 0       ┆ 3       ┆ 103        ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n",
       "│ 1         ┆ 0       ┆ 4       ┆ 104        ┆ … ┆ null       ┆ null       ┆ null      ┆ null      │\n",
       "│ …         ┆ …       ┆ …       ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
       "│ 1         ┆ 1       ┆ 20      ┆ 10120      ┆ … ┆ 0.069218   ┆ 4.1833     ┆ 10113.5   ┆ 0.138532  │\n",
       "│ 1         ┆ 1       ┆ 21      ┆ 10121      ┆ … ┆ 0.069211   ┆ 4.1833     ┆ 10114.5   ┆ 0.138518  │\n",
       "│ 1         ┆ 1       ┆ 22      ┆ 10122      ┆ … ┆ 0.069204   ┆ 4.1833     ┆ 10115.5   ┆ 0.138504  │\n",
       "│ 1         ┆ 1       ┆ 23      ┆ 10123      ┆ … ┆ 0.069197   ┆ 4.1833     ┆ 10116.5   ┆ 0.13849   │\n",
       "│ 1         ┆ 1       ┆ 24      ┆ 10124      ┆ … ┆ 0.06919    ┆ 4.1833     ┆ 10117.5   ┆ 0.138477  │\n",
       "└───────────┴─────────┴─────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet(\"/kaggle/working/test_make_feature_walk_forward/sprint1/train.parquet\")\n",
    "df.filter(pl.col(\"symbol_id\") == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5d216",
   "metadata": {
    "papermill": {
     "duration": 0.005888,
     "end_time": "2024-12-26T05:22:50.175893",
     "exception": false,
     "start_time": "2024-12-26T05:22:50.170005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Jane Street Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b71570f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:50.190779Z",
     "iopub.status.busy": "2024-12-26T05:22:50.190368Z",
     "iopub.status.idle": "2024-12-26T05:22:50.207319Z",
     "shell.execute_reply": "2024-12-26T05:22:50.206221Z"
    },
    "papermill": {
     "duration": 0.027156,
     "end_time": "2024-12-26T05:22:50.209234",
     "exception": false,
     "start_time": "2024-12-26T05:22:50.182078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jane_street_real.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jane_street_real.py\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "\n",
    "from time_serise_feature import (\n",
    "    create_stat_features_by,  # おそらく外部で定義された関数をインポート\n",
    "    add_rolling_feature       # おそらく外部で定義された関数をインポート (例示)\n",
    ")\n",
    "\n",
    "target_9_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9\"\n",
    "max_time_id = pl.read_parquet(target_9_path).select(pl.col(\"time_id\").max()).item()\n",
    "segment_size = (max_time_id + 1) // 4  # 4等分のサイズ\n",
    "\n",
    "# 4分割の閾値を計算\n",
    "threshold1 = segment_size\n",
    "threshold2 = segment_size * 2\n",
    "threshold3 = segment_size * 3\n",
    "\n",
    "print(\"threshold\", threshold1, threshold2, threshold3)\n",
    "\n",
    "def feature_today_rolling_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    当日のローリング特徴量を作成する関数。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.DataFrame\n",
    "        入力データフレーム\n",
    "    pred_cols : List[str]\n",
    "        予測に使用する列（例: responder系など）\n",
    "    input_cols : List[str]\n",
    "        ローリングをかけるカラム一覧\n",
    "    original_cols : List[str] | None\n",
    "        オリジナル列の一覧\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf, generated_features, generated_preds):\n",
    "        ldf                : 特徴量を追加したDataFrame\n",
    "        generated_features : 生成した特徴量カラム名一覧\n",
    "        generated_preds    : 生成された予測カラム (ここでは空リスト)\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "    \n",
    "    # 当日特徴量（例: rolling特徴量）\n",
    "    ldf, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=ldf,\n",
    "        cols=input_cols,\n",
    "        lags=[10, 100, 1000],\n",
    "        group_keys=[\"symbol_id\"],\n",
    "        sort_keys=[\"symbol_id\", \"date_id\", \"time_id\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "  \n",
    "    return ldf, generated_features, generated_preds\n",
    "\n",
    "\n",
    "def feature_today_stat_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    当日データを date_id, time_id 単位で集計して特徴量を作成する関数。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.DataFrame\n",
    "        入力データフレーム\n",
    "    pred_cols : List[str]\n",
    "        予測に使用する列\n",
    "    input_cols : List[str]\n",
    "        集計対象となるカラム\n",
    "    original_cols : List[str] | None\n",
    "        オリジナル列の一覧\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf, generated_features, generated_preds):\n",
    "        ldf                : 特徴量を追加したDataFrame\n",
    "        generated_features : 生成した特徴量カラム名一覧\n",
    "        generated_preds    : 生成された予測カラム (ここでは空リスト)\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "\n",
    "    # date_id, time_id単位で統計量を作成\n",
    "    df_datetime, datetime_feature_cols = create_stat_features_by(\n",
    "        ldf=ldf,\n",
    "        cols=input_cols,\n",
    "        key_by=[\"date_id\", \"time_id\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(datetime_feature_cols)\n",
    "\n",
    "    # 更に rolling\n",
    "    df_datetime, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_datetime,\n",
    "        cols=datetime_feature_cols,\n",
    "        lags=[10, 100, 250, 500],\n",
    "        group_keys=[\"date_id\"],\n",
    "        sort_keys=[\"date_id\", \"time_id\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "    \n",
    "    return df_datetime, generated_features, generated_preds\n",
    "\n",
    "\n",
    "def create_raw_responder_lag(\n",
    "    df: pl.DataFrame,\n",
    "    pred_cols: List[str]\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Responderに対するラグを作成し、col_lag_1としてリネームしたDataFrameを作る。\n",
    "\n",
    "    1) 'symbol_id', 'date_id', 'time_id_group', 'weight' と pred_cols の列のみ抽出\n",
    "    2) w * responder_6 の列を新たに作成 (\"w_responder_6\"など)\n",
    "    3) weight を削除\n",
    "    4) date_id を (date_id + 1) シフト\n",
    "    5) pred_cols + [\"w_responder_6\"] を \"_lag_1\" にリネーム\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (df_lags, generated_cols, keys):\n",
    "        df_lags        : ラグ作成後のDataFrame\n",
    "        generated_cols : リネーム後の列名一覧\n",
    "        keys           : 主キー([\"symbol_id\", \"date_id\", \"time_id_group\"])\n",
    "    \"\"\"\n",
    "    target_cols = [\"symbol_id\", \"date_id\", \"time_id_group\", \"weight\"] + pred_cols\n",
    "    df_lags = df.select(target_cols)\n",
    "\n",
    "    # 2) w * responder\n",
    "    w_responder_cols = []\n",
    "    for c in pred_cols:\n",
    "        w_c = f\"w_{c}\"\n",
    "        w_responder_cols.append(w_c)\n",
    "        df_lags = df_lags.with_columns(\n",
    "            (pl.col(\"weight\") * pl.col(c)).alias(w_c)\n",
    "        )\n",
    "    new_pred_cols = pred_cols + w_responder_cols\n",
    "    \n",
    "    # 3) wを削除\n",
    "    df_lags = df_lags.drop([\"weight\"])\n",
    "\n",
    "    # 4) date_id = date_id + 1\n",
    "    df_lags = df_lags.with_columns(\n",
    "        (pl.col(\"date_id\") + 1).alias(\"date_id\")\n",
    "    )\n",
    "\n",
    "    # 5) new_pred_cols を \"colName_lag_1\" にリネーム\n",
    "    rename_map = {}\n",
    "    for c in new_pred_cols:\n",
    "        rename_map[c] = f\"{c}_lag_1\"\n",
    "\n",
    "    df_lags = df_lags.rename(rename_map)\n",
    "    generated_cols = list(rename_map.values())\n",
    "\n",
    "    keys = [\"symbol_id\", \"date_id\", \"time_id_group\"]\n",
    "    return df_lags, generated_cols, keys\n",
    "\n",
    "\n",
    "def create_responder_lag_feature(\n",
    "    ldf: pl.DataFrame,\n",
    "    cols: List[str]\n",
    ") -> Tuple[pl.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Responderのラグ列を作り、統計量 + rolling を計算したDataFrameを生成する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.DataFrame\n",
    "        入力のDataFrame\n",
    "    cols : List[str]\n",
    "        responder系列名など\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (df_lags, generated_features):\n",
    "        df_lags            : 作成した特徴量を追加したDataFrame\n",
    "        generated_features : 生成した列名一覧\n",
    "    \"\"\"\n",
    "    generated_features = []\n",
    "    df_raw_lags, lags_cols, raw_lags_keys = create_raw_responder_lag(ldf, cols)\n",
    "\n",
    "    # symbol_id, date_id, time_id_group 単位で集計\n",
    "    df_lags, gen_cols_symdate_group = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"symbol_id\", \"date_id\", \"time_id_group\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_symdate_group)\n",
    "\n",
    "    # rolling [4, 21]\n",
    "    df_lags, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_lags,\n",
    "        cols=gen_cols_symdate_group,\n",
    "        lags=[4, 21],\n",
    "        group_keys=[\"symbol_id\"],\n",
    "        sort_keys=[\"date_id\", \"time_id_group\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    # date_id単位でラグ列を集計\n",
    "    df_lags_dateid, gen_cols_date = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"date_id\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_date)\n",
    "\n",
    "    # rolling [7]\n",
    "    df_lags_dateid, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_lags_dateid,\n",
    "        cols=gen_cols_date,\n",
    "        lags=[7],\n",
    "        group_keys=[\"date_id\"],\n",
    "        sort_keys=[\"date_id\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    # 結合\n",
    "    df_lags = df_lags.join(df_lags_dateid, on=[\"date_id\"], how=\"left\")\n",
    "\n",
    "    # symbol_id, date_id単位\n",
    "    df_lags_symdate, gen_cols_symdate = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"symbol_id\", \"date_id\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_symdate)\n",
    "\n",
    "    # rolling [7]\n",
    "    df_lags_symdate, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_lags_symdate,\n",
    "        cols=gen_cols_symdate,\n",
    "        lags=[7],\n",
    "        group_keys=[\"symbol_id\"],\n",
    "        sort_keys=[\"symbol_id\", \"date_id\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    df_lags = df_lags.join(df_lags_symdate, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n",
    "\n",
    "    # date_id, time_group_id単位\n",
    "    df_lags_datetime, gen_cols_datetime = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"date_id\", \"time_id_group\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_datetime)\n",
    "\n",
    "    # rolling [7]\n",
    "    df_lags_datetime, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_lags_datetime,\n",
    "        cols=gen_cols_datetime,\n",
    "        lags=[7],\n",
    "        group_keys=[\"date_id\"],\n",
    "        sort_keys=[\"date_id\", \"time_id_group\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    df_lags = df_lags.join(df_lags_datetime, on=[\"date_id\", \"time_id_group\"], how=\"left\")\n",
    "\n",
    "    return df_lags, generated_features\n",
    "\n",
    "\n",
    "def responder_lag_feature_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    responder_lag_featureの作成を行うラッパ関数。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.DataFrame\n",
    "        入力のDataFrame\n",
    "    pred_cols : List[str]\n",
    "        responder系列など(実際にはこの例では create_responder_lag_feature内で参照)\n",
    "    input_cols : List[str]\n",
    "        未使用だがインターフェイスとして保持\n",
    "    original_cols : List[str] | None\n",
    "        未使用だがインターフェイスとして保持\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf_responder_lag_feature, generated_features, generated_preds):\n",
    "        ldf_responder_lag_feature : responder_lag_feature を追加した DataFrame\n",
    "        generated_features        : 生成した列名一覧\n",
    "        generated_preds           : 空リスト\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "\n",
    "    # responder-lag特徴量を生成\n",
    "    ldf_responder_lag, cols = create_responder_lag_feature(ldf, pred_cols)\n",
    "    generated_features.extend(cols)\n",
    "\n",
    "    return ldf_responder_lag, generated_features, generated_preds\n",
    "\n",
    "\n",
    "def create_raw_feature_lag(\n",
    "    df: pl.DataFrame,\n",
    "    pred_cols: List[str]\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    (date_id + 1)シフトを行い、カラムを col_lag_1 にリネームした DataFrame を作成。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        入力のDataFrame\n",
    "    pred_cols : List[str]\n",
    "        ラグを取りたいカラム一覧\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (df_lags, generated_cols, keys):\n",
    "        df_lags        : ラグを付与後のDataFrame\n",
    "        generated_cols : ラグカラム名一覧\n",
    "        keys           : [\"symbol_id\", \"date_id\", \"time_id_group\"] 主キー\n",
    "    \"\"\"\n",
    "    target_cols = [\"symbol_id\", \"date_id\", \"time_id_group\"] + pred_cols\n",
    "    df_lags = df.select(target_cols)\n",
    "\n",
    "    # date_idを+1\n",
    "    df_lags = df_lags.with_columns(\n",
    "        (pl.col(\"date_id\") + 1).alias(\"date_id\")\n",
    "    )\n",
    "\n",
    "    rename_map = {}\n",
    "    for c in pred_cols:\n",
    "        rename_map[c] = f\"{c}_lag_1\"\n",
    "\n",
    "    df_lags = df_lags.rename(rename_map)\n",
    "    generated_cols = list(rename_map.values())\n",
    "    keys = [\"symbol_id\", \"date_id\", \"time_id_group\"]\n",
    "\n",
    "    return df_lags, generated_cols, keys\n",
    "\n",
    "\n",
    "def create_feature_lag(ldf: pl.DataFrame, cols: List[str]) -> Tuple[pl.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    create_raw_feature_lag で作成したラグ列を、さらに統計量 + rolling で特徴量に拡張。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.DataFrame\n",
    "        入力DataFrame\n",
    "    cols : List[str]\n",
    "        ラグ対象カラム\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (df_lags, generated_features):\n",
    "        df_lags            : 特徴量を追加した DataFrame\n",
    "        generated_features : 生成した列名一覧\n",
    "    \"\"\"\n",
    "    generated_features = []\n",
    "    df_raw_lags, lags_cols, raw_lags_keys = create_raw_feature_lag(ldf, cols)\n",
    "\n",
    "    # symbol_id, date_id, time_id_group 単位で集計\n",
    "    df_lags, gen_cols_symdate_group = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"symbol_id\", \"date_id\", \"time_id_group\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_symdate_group)\n",
    "\n",
    "    # rolling [4, 28]\n",
    "    df_lags, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_lags,\n",
    "        cols=gen_cols_symdate_group,\n",
    "        lags=[4, 28],\n",
    "        group_keys=[\"symbol_id\"],\n",
    "        sort_keys=[\"date_id\", \"time_id_group\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    # date_id単位でラグ列を集計\n",
    "    df_lags_dateid, gen_cols_date = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"date_id\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_date)\n",
    "\n",
    "    # rolling [7]\n",
    "    df_lags_dateid, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_lags_dateid,\n",
    "        cols=gen_cols_date,\n",
    "        lags=[7],\n",
    "        group_keys=[\"date_id\"],\n",
    "        sort_keys=[\"date_id\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    # 結合\n",
    "    df_lags = df_lags.join(df_lags_dateid, on=[\"date_id\"], how=\"left\")\n",
    "\n",
    "    # symbol_id, date_id単位で集計\n",
    "    df_lags_symbol_date, gen_cols_symbol_date = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"symbol_id\", \"date_id\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_symbol_date)\n",
    "\n",
    "    # rolling [7]\n",
    "    df_lags_symbol_date, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=df_lags_symbol_date,\n",
    "        cols=gen_cols_symbol_date,\n",
    "        lags=[7],\n",
    "        group_keys=[\"symbol_id\"],\n",
    "        sort_keys=[\"symbol_id\", \"date_id\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    df_lags = df_lags.join(df_lags_symbol_date, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n",
    "\n",
    "    # date_id, time_id_group単位で集計\n",
    "    date_date_time_group, gen_cols_date_time_group = create_stat_features_by(\n",
    "        ldf=df_raw_lags,\n",
    "        cols=lags_cols,\n",
    "        key_by=[\"date_id\", \"time_id_group\"],\n",
    "        agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"]\n",
    "    )\n",
    "    generated_features.extend(gen_cols_date_time_group)\n",
    "\n",
    "    # rolling [7]\n",
    "    date_date_time_group, generated_cols, _ = add_rolling_feature(\n",
    "        ldf=date_date_time_group,\n",
    "        cols=gen_cols_date_time_group,\n",
    "        lags=[7],\n",
    "        group_keys=[\"date_id\"],\n",
    "        sort_keys=[\"date_id\", \"time_id_group\"]\n",
    "    )\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    df_lags = df_lags.join(date_date_time_group, on=[\"date_id\", \"time_id_group\"], how=\"left\")\n",
    "\n",
    "    return df_lags, generated_features\n",
    "\n",
    "\n",
    "def feature_lag_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    前日ラグ特徴量等を作成する処理のラッパ関数。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.DataFrame\n",
    "        入力のDataFrame\n",
    "    pred_cols : List[str]\n",
    "        (未使用) 予測に使うカラム\n",
    "    input_cols : List[str]\n",
    "        ラグ対象カラム\n",
    "    original_cols : List[str] | None\n",
    "        オリジナル列リスト (未使用)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf, generated_features, generated_preds):\n",
    "        ldf                : 特徴量を追加したDataFrame\n",
    "        generated_features : 生成したカラム名一覧\n",
    "        generated_preds    : 空リスト\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "    \n",
    "    # 前日特徴量(symbol_id, date_id -1, time_group_id)\n",
    "    ldf_features, generated_cols = create_feature_lag(ldf, input_cols)\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    return ldf_features, generated_features, generated_preds\n",
    "\n",
    "\n",
    "def add_feature_func(\n",
    "    df: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    全体の特徴量作成処理をまとめるメイン関数。\n",
    "    1) time_id_groupの付与\n",
    "    2) 当日のローリング特徴量\n",
    "    3) date_id, time_id 単位の統計量作成\n",
    "    4) responder_lag_featureの作成\n",
    "    5) その他のlag_featureの作成\n",
    "    6) 結果を結合して返す\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        入力DataFrame\n",
    "    pred_cols : List[str]\n",
    "        予測に使用する列（例: responder系）\n",
    "    input_cols : List[str]\n",
    "        特徴量生成に用いるカラム\n",
    "    original_cols : List[str] | None\n",
    "        元のカラムリストなど (必要に応じて拡張)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf_feature, generated_features, generated_preds):\n",
    "        ldf_feature       : すべての特徴量を追加・結合したDataFrame\n",
    "        generated_features: 生成した特徴量カラム名一覧\n",
    "        generated_preds   : 生成された予測カラム (ここでは空)\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "\n",
    "    # Lazy化 & ソート\n",
    "    ldf = df.lazy()\n",
    "    ldf = ldf.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n",
    "\n",
    "    # time_id_group (4分割)\n",
    "    ldf = ldf.with_columns(\n",
    "        pl.when(pl.col(\"time_id\") < threshold1).then(0)\n",
    "        .when(pl.col(\"time_id\") < threshold2).then(1)\n",
    "        .when(pl.col(\"time_id\") < threshold3).then(2)\n",
    "        .otherwise(3)\n",
    "        .cast(pl.Int32)\n",
    "        .alias(\"time_id_group\")\n",
    "    )\n",
    "    generated_features.append(\"time_id_group\")\n",
    "\n",
    "    # (1) 当日のローリング特徴量\n",
    "    ldf_feature, f, p = feature_today_rolling_func(\n",
    "        ldf, pred_cols, input_cols, original_cols\n",
    "    )\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "\n",
    "    # (2) date_id, time_id 単位の統計量\n",
    "    ldf_feature_today_stat, f, p = feature_today_stat_func(\n",
    "        ldf, pred_cols, input_cols, original_cols\n",
    "    )\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "\n",
    "    # 結合\n",
    "    ldf_feature = ldf_feature.join(\n",
    "        ldf_feature_today_stat, on=[\"date_id\", \"time_id\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # (3) responder_lag_feature\n",
    "    ldf_responder_lag_feature, f, p = responder_lag_feature_func(\n",
    "        ldf, pred_cols, input_cols, original_cols\n",
    "    )\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "\n",
    "    # 結合\n",
    "    ldf_feature = ldf_feature.join(\n",
    "        ldf_responder_lag_feature, on=[\"symbol_id\", \"date_id\", \"time_id_group\"], how=\"left\"\n",
    "    )\n",
    "  \n",
    "    # (4) その他の前日ラグ特徴量\n",
    "    ldf_feature_lag, f, p = feature_lag_func(\n",
    "        ldf, pred_cols, input_cols, original_cols\n",
    "    )\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "\n",
    "    # 結合\n",
    "    ldf_feature = ldf_feature.join(\n",
    "        ldf_feature_lag, on=[\"symbol_id\", \"date_id\", \"time_id_group\"], how=\"left\"\n",
    "    )\n",
    "    \n",
    "    return ldf_feature, generated_features, generated_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e644c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:50.223542Z",
     "iopub.status.busy": "2024-12-26T05:22:50.223135Z",
     "iopub.status.idle": "2024-12-26T05:22:56.220508Z",
     "shell.execute_reply": "2024-12-26T05:22:56.218954Z"
    },
    "papermill": {
     "duration": 6.00716,
     "end_time": "2024-12-26T05:22:56.222741",
     "exception": false,
     "start_time": "2024-12-26T05:22:50.215581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold 242 484 726\n",
      "weight, Float32\n",
      "feature_00, Float32\n",
      "feature_01, Float32\n",
      "feature_02, Float32\n",
      "feature_03, Float32\n",
      "feature_04, Float32\n",
      "feature_05, Float32\n",
      "feature_06, Float32\n",
      "feature_07, Float32\n",
      "feature_08, Float32\n",
      "feature_09, Int8\n",
      "feature_10, Int8\n",
      "feature_11, Int16\n",
      "feature_12, Float32\n",
      "feature_13, Float32\n",
      "feature_14, Float32\n",
      "feature_15, Float32\n",
      "feature_16, Float32\n",
      "feature_17, Float32\n",
      "feature_18, Float32\n",
      "feature_19, Float32\n",
      "feature_20, Float32\n",
      "feature_21, Float32\n",
      "feature_22, Float32\n",
      "feature_23, Float32\n",
      "feature_24, Float32\n",
      "feature_25, Float32\n",
      "feature_26, Float32\n",
      "feature_27, Float32\n",
      "feature_28, Float32\n",
      "feature_29, Float32\n",
      "feature_30, Float32\n",
      "feature_31, Float32\n",
      "feature_32, Float32\n",
      "feature_33, Float32\n",
      "feature_34, Float32\n",
      "feature_35, Float32\n",
      "feature_36, Float32\n",
      "feature_37, Float32\n",
      "feature_38, Float32\n",
      "feature_39, Float32\n",
      "feature_40, Float32\n",
      "feature_41, Float32\n",
      "feature_42, Float32\n",
      "feature_43, Float32\n",
      "feature_44, Float32\n",
      "feature_45, Float32\n",
      "feature_46, Float32\n",
      "feature_47, Float32\n",
      "feature_48, Float32\n",
      "feature_49, Float32\n",
      "feature_50, Float32\n",
      "feature_51, Float32\n",
      "feature_52, Float32\n",
      "feature_53, Float32\n",
      "feature_54, Float32\n",
      "feature_55, Float32\n",
      "feature_56, Float32\n",
      "feature_57, Float32\n",
      "feature_58, Float32\n",
      "feature_59, Float32\n",
      "feature_60, Float32\n",
      "feature_61, Float32\n",
      "feature_62, Float32\n",
      "feature_63, Float32\n",
      "feature_64, Float32\n",
      "feature_65, Float32\n",
      "feature_66, Float32\n",
      "feature_67, Float32\n",
      "feature_68, Float32\n",
      "feature_69, Float32\n",
      "feature_70, Float32\n",
      "feature_71, Float32\n",
      "feature_72, Float32\n",
      "feature_73, Float32\n",
      "feature_74, Float32\n",
      "feature_75, Float32\n",
      "feature_76, Float32\n",
      "feature_77, Float32\n",
      "feature_78, Float32\n",
      "target_cols: ['symbol_id', 'date_id', 'time_id', 'responder_0', 'responder_1', 'responder_2', 'responder_3', 'responder_4', 'responder_5', 'responder_6', 'responder_7', 'responder_8', 'weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\n",
      "parquet_files ['/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/lib-time-series-libs/')\n",
    "\n",
    "from jane_street_real import *\n",
    "\n",
    "output_base = \"datasets\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "parquet_files = [\n",
    "    \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0\"\n",
    "]\n",
    "\n",
    "# 入力変数選択（事前定義）\n",
    "id_col = \"symbol_id\"    # ID列\n",
    "date_id = \"date_id\"     # 日付列\n",
    "time_id = \"time_id\"     # 時間列\n",
    "pred_cols = [f\"responder_{i}\" for i in range(9)]  # 目的変数\n",
    "\n",
    "# 最初のファイルからカラム一覧とdtype取得\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(\"No parquet files found in the specified data path.\")\n",
    "\n",
    "target_9_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9\"\n",
    "df_test = pl.read_parquet(target_9_path, n_rows=1)\n",
    "all_cols = df_test.columns\n",
    "schema = df_test.schema  # {col_name: polars.DataType}\n",
    "\n",
    "# 除外する列\n",
    "exclude_cols = {id_col, date_id, time_id} | set(pred_cols)\n",
    "\n",
    "# input_colsを自動的に決定\n",
    "input_cols = []\n",
    "for col in all_cols:\n",
    "    if col not in exclude_cols:\n",
    "        polars_dtype = schema[col]\n",
    "        print(f\"{col}, {polars_dtype}\")\n",
    "        input_cols.append(col)\n",
    "\n",
    "# target_cols作成\n",
    "target_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "print(\"target_cols:\", target_cols)\n",
    "\n",
    "train_length = 20\n",
    "retroactive_size = 7 # rollingやwindowの最大lags数\n",
    "print(\"parquet_files\", parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c84e8a78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:56.238091Z",
     "iopub.status.busy": "2024-12-26T05:22:56.237698Z",
     "iopub.status.idle": "2024-12-26T05:22:56.244825Z",
     "shell.execute_reply": "2024-12-26T05:22:56.243739Z"
    },
    "papermill": {
     "duration": 0.017365,
     "end_time": "2024-12-26T05:22:56.247142",
     "exception": false,
     "start_time": "2024-12-26T05:22:56.229777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noutput_base = \"feature_datasets\"\\nos.makedirs(output_base, exist_ok=True)\\n\\nmake_feature_walk_forward(\\n    files=parquet_files,\\n    output_base=output_base,\\n    id_col=id_col,\\n    date_id=date_id,\\n    time_id=time_id,\\n    pred_cols=pred_cols,\\n    input_cols=input_cols,\\n    train_length=train_length,\\n    retroactive_size=retroactive_size,\\n    add_feature_func=add_feature_func\\n)\\nall_cols'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "output_base = \"feature_datasets\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "make_feature_walk_forward(\n",
    "    files=parquet_files,\n",
    "    output_base=output_base,\n",
    "    id_col=id_col,\n",
    "    date_id=date_id,\n",
    "    time_id=time_id,\n",
    "    pred_cols=pred_cols,\n",
    "    input_cols=input_cols,\n",
    "    train_length=train_length,\n",
    "    retroactive_size=retroactive_size,\n",
    "    add_feature_func=add_feature_func\n",
    ")\n",
    "all_cols\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d0fbc2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:56.262845Z",
     "iopub.status.busy": "2024-12-26T05:22:56.262464Z",
     "iopub.status.idle": "2024-12-26T05:22:56.268892Z",
     "shell.execute_reply": "2024-12-26T05:22:56.267813Z"
    },
    "papermill": {
     "duration": 0.0165,
     "end_time": "2024-12-26T05:22:56.271173",
     "exception": false,
     "start_time": "2024-12-26T05:22:56.254673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport polars as pl\\ninput_df = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0\")\\nprint(input_df.shape)\\ndel input_df\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import polars as pl\n",
    "input_df = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0\")\n",
    "print(input_df.shape)\n",
    "del input_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4840c6a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T05:22:56.291584Z",
     "iopub.status.busy": "2024-12-26T05:22:56.291142Z",
     "iopub.status.idle": "2024-12-26T05:22:56.297697Z",
     "shell.execute_reply": "2024-12-26T05:22:56.296701Z"
    },
    "papermill": {
     "duration": 0.01989,
     "end_time": "2024-12-26T05:22:56.299901",
     "exception": false,
     "start_time": "2024-12-26T05:22:56.280011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport polars as pl\\npaths = []\\nfor i in range(1, 10): # 1 to 9\\n    path = f\"/kaggle/working/feature_datasets/sprint{i}/train.parquet\"\\n    paths.append(path)\\nprint(\"paths\", paths)abs\\noutput_df = pl.scan_parquet(paths).select(\"symbol_id\")\\nprint(output_df.collect().shape)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import polars as pl\n",
    "paths = []\n",
    "for i in range(1, 10): # 1 to 9\n",
    "    path = f\"/kaggle/working/feature_datasets/sprint{i}/train.parquet\"\n",
    "    paths.append(path)\n",
    "print(\"paths\", paths)abs\n",
    "output_df = pl.scan_parquet(paths).select(\"symbol_id\")\n",
    "print(output_df.collect().shape)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.851754,
   "end_time": "2024-12-26T05:22:57.128962",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-26T05:22:43.277208",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be0d802",
   "metadata": {
    "_cell_guid": "d56e7f40-d49e-4a03-89df-31470a3737ef",
    "_uuid": "f68866a2-e910-4cd1-b613-709d600662f0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:57.647025Z",
     "iopub.status.busy": "2024-12-25T02:47:57.646626Z",
     "iopub.status.idle": "2024-12-25T02:47:57.991191Z",
     "shell.execute_reply": "2024-12-25T02:47:57.989925Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.355255,
     "end_time": "2024-12-25T02:47:57.993799",
     "exception": false,
     "start_time": "2024-12-25T02:47:57.638544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "def generate_test_data(\n",
    "    base_path: str,\n",
    "    id_col: str, \n",
    "    date_id: str,  # 例: (\"date_id\", int)\n",
    "    time_id: str,  # 例: (\"time_id\", int)\n",
    "    pred_cols: list, # 例: [(\"responder_0\", float)]\n",
    "    input_cols: list,# 例: [(\"feature_00\", float)]\n",
    "    num_partitions=4,\n",
    "    num_id=2,\n",
    "    date_per_partition=2\n",
    "):\n",
    "    \"\"\"\n",
    "    id_col, date_id, time_id, pred_cols, input_colsをもとにテストデータを作成し、\n",
    "    partition_id=0～(num_partitions-1)までのParquetファイルに出力する。\n",
    "\n",
    "    各パーティションは、以下の条件でデータを生成：\n",
    "      - id_col: 0～(num_id-1)までのIDを用意\n",
    "      - date_id: partition i は [i*date_per_partition, (i+1)*date_per_partition-1]\n",
    "      - time_id: 0～24 (25個)\n",
    "    よって1パーティションあたり: date_per_partition * num_id * 25行。\n",
    "\n",
    "    pred_cols, input_colsはタプルのリスト [(col_name, type), ...]で指定し、\n",
    "    値は日付・ID・time_idに基づき決定的に割り当てる。\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # カラム名抽出\n",
    "    id_col_name = id_col[0]\n",
    "    date_col_name = date_id[0]\n",
    "    time_col_name = time_id[0]\n",
    "\n",
    "    # 1パーティションあたりの行数\n",
    "    time_per_day = 25\n",
    "    rows_per_partition = date_per_partition * num_id * time_per_day\n",
    "\n",
    "    # 全ての列名\n",
    "    all_col_names = [id_col_name, date_col_name, time_col_name] + [c[0] for c in pred_cols] + [c[0] for c in input_cols]\n",
    "\n",
    "    for i in range(num_partitions):\n",
    "        date_start = i * date_per_partition\n",
    "        date_end = date_start + date_per_partition\n",
    "\n",
    "        # データ生成\n",
    "        # 二重三重ループで決定的に割り当て\n",
    "        date_ids_list = []\n",
    "        id_list = []\n",
    "        time_ids_list = []\n",
    "\n",
    "        for d in range(date_start, date_end):\n",
    "            for sid in range(num_id):\n",
    "                for t in range(time_per_day):\n",
    "                    date_ids_list.append(d)\n",
    "                    id_list.append(sid)\n",
    "                    time_ids_list.append(t)\n",
    "\n",
    "        # ベースとなる数値計算用\n",
    "        # responder_0やfeature_00に割り当てるための基礎値\n",
    "        # 例：val = d*10000 + sid*100 + t\n",
    "        base_vals = []\n",
    "        for d_id, s_id, t_id in zip(date_ids_list, id_list, time_ids_list):\n",
    "            val = d_id * 10000 + s_id * 100 + t_id\n",
    "            base_vals.append(val)\n",
    "\n",
    "        # DataFrame用辞書\n",
    "        data = {\n",
    "            id_col_name: id_list,\n",
    "            date_col_name: date_ids_list,\n",
    "            time_col_name: time_ids_list,\n",
    "        }\n",
    "\n",
    "        # pred_cols割り当て（例：responder_0 = base_val + index_of_col）\n",
    "        for idx, (c_name, c_type) in enumerate(pred_cols):\n",
    "            # 例: responder_0 = base_val + idx\n",
    "            c_values = [v + idx for v in base_vals]\n",
    "            data[c_name] = c_values\n",
    "\n",
    "        # input_cols割り当て（例：feature_00 = base_val/1000）\n",
    "        for idx, (c_name, c_type) in enumerate(input_cols):\n",
    "            # 例: feature_00 = base_val / 1000.0\n",
    "            c_values = [v / 1000.0 + idx for v in base_vals]\n",
    "            data[c_name] = c_values\n",
    "\n",
    "        df = pl.DataFrame(data)\n",
    "        # ソート（id_col_name, date_col_name, time_col_name順）\n",
    "        df = df.sort([id_col_name, date_col_name, time_col_name])\n",
    "\n",
    "        partition_dir = os.path.join(base_path, f\"partition_id={i}\")\n",
    "        os.makedirs(partition_dir, exist_ok=True)\n",
    "        df.write_parquet(os.path.join(partition_dir, \"part-0.parquet\"))\n",
    "\n",
    "    print(\"Test data generated without rows_per_partition, using given columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4d7bb4",
   "metadata": {
    "_cell_guid": "1283520c-2194-4f05-8452-190318a611f7",
    "_uuid": "6f344a3d-07fd-47b5-94c0-d2957fb84ecd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:58.005089Z",
     "iopub.status.busy": "2024-12-25T02:47:58.004709Z",
     "iopub.status.idle": "2024-12-25T02:47:58.013678Z",
     "shell.execute_reply": "2024-12-25T02:47:58.012548Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016983,
     "end_time": "2024-12-25T02:47:58.015794",
     "exception": false,
     "start_time": "2024-12-25T02:47:57.998811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('symbol_id', int),\n",
       " ('date_id', int),\n",
       " ('time_id', int),\n",
       " ('responder_0', float),\n",
       " ('feature_00', float)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力変数選択\n",
    "id_col = (\"symbol_id\", int)\n",
    "date_id = (\"date_id\", int)\n",
    "time_id = (\"time_id\", int)\n",
    "\n",
    "#preds_cols = [f\"responder_{i}\" for i in range(9)] # 目的変数\n",
    "pred_cols = [(\"responder_0\", float)]\n",
    "input_cols = [(\"feature_00\", float)] # 説明変数\n",
    "\n",
    "target_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a94a6b",
   "metadata": {
    "_cell_guid": "cb46278d-0a29-4f3a-8a8b-973239921d9f",
    "_uuid": "1978295c-7d27-45ba-a755-0a53e8f742f6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:58.027630Z",
     "iopub.status.busy": "2024-12-25T02:47:58.027264Z",
     "iopub.status.idle": "2024-12-25T02:47:58.175930Z",
     "shell.execute_reply": "2024-12-25T02:47:58.174572Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.157428,
     "end_time": "2024-12-25T02:47:58.178358",
     "exception": false,
     "start_time": "2024-12-25T02:47:58.020930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data generated without rows_per_partition, using given columns.\n"
     ]
    }
   ],
   "source": [
    "generate_test_data(\"test\", id_col, date_id, time_id, pred_cols, input_cols, num_partitions=2, num_id=1, date_per_partition=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61af4e6c",
   "metadata": {
    "_cell_guid": "43f495ad-8380-4271-97c3-9e8d32ed4666",
    "_uuid": "203cd0c7-f167-40be-b6e6-55415500f526",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:58.189983Z",
     "iopub.status.busy": "2024-12-25T02:47:58.189609Z",
     "iopub.status.idle": "2024-12-25T02:47:58.261744Z",
     "shell.execute_reply": "2024-12-25T02:47:58.260448Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.080541,
     "end_time": "2024-12-25T02:47:58.263979",
     "exception": false,
     "start_time": "2024-12-25T02:47:58.183438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (150, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>symbol_id</th><th>date_id</th><th>time_id</th><th>responder_0</th><th>feature_00</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td></tr><tr><td>0</td><td>0</td><td>1</td><td>1</td><td>0.001</td></tr><tr><td>0</td><td>0</td><td>2</td><td>2</td><td>0.002</td></tr><tr><td>0</td><td>0</td><td>3</td><td>3</td><td>0.003</td></tr><tr><td>0</td><td>0</td><td>4</td><td>4</td><td>0.004</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>5</td><td>20</td><td>50020</td><td>50.02</td></tr><tr><td>0</td><td>5</td><td>21</td><td>50021</td><td>50.021</td></tr><tr><td>0</td><td>5</td><td>22</td><td>50022</td><td>50.022</td></tr><tr><td>0</td><td>5</td><td>23</td><td>50023</td><td>50.023</td></tr><tr><td>0</td><td>5</td><td>24</td><td>50024</td><td>50.024</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (150, 5)\n",
       "┌───────────┬─────────┬─────────┬─────────────┬────────────┐\n",
       "│ symbol_id ┆ date_id ┆ time_id ┆ responder_0 ┆ feature_00 │\n",
       "│ ---       ┆ ---     ┆ ---     ┆ ---         ┆ ---        │\n",
       "│ i64       ┆ i64     ┆ i64     ┆ i64         ┆ f64        │\n",
       "╞═══════════╪═════════╪═════════╪═════════════╪════════════╡\n",
       "│ 0         ┆ 0       ┆ 0       ┆ 0           ┆ 0.0        │\n",
       "│ 0         ┆ 0       ┆ 1       ┆ 1           ┆ 0.001      │\n",
       "│ 0         ┆ 0       ┆ 2       ┆ 2           ┆ 0.002      │\n",
       "│ 0         ┆ 0       ┆ 3       ┆ 3           ┆ 0.003      │\n",
       "│ 0         ┆ 0       ┆ 4       ┆ 4           ┆ 0.004      │\n",
       "│ …         ┆ …       ┆ …       ┆ …           ┆ …          │\n",
       "│ 0         ┆ 5       ┆ 20      ┆ 50020       ┆ 50.02      │\n",
       "│ 0         ┆ 5       ┆ 21      ┆ 50021       ┆ 50.021     │\n",
       "│ 0         ┆ 5       ┆ 22      ┆ 50022       ┆ 50.022     │\n",
       "│ 0         ┆ 5       ┆ 23      ┆ 50023       ┆ 50.023     │\n",
       "│ 0         ┆ 5       ┆ 24      ┆ 50024       ┆ 50.024     │\n",
       "└───────────┴─────────┴─────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pl.scan_parquet(\"/kaggle/working/test/partition_id=*/part-*.parquet\", glob=True)\n",
    "      .filter(pl.col(\"symbol_id\") == 0)\n",
    "      .collect()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91a4a43",
   "metadata": {
    "_cell_guid": "c6439978-96fc-4a1b-9a91-7d79e70c1db2",
    "_uuid": "58fd0698-e4de-4725-92fd-0844c061c56a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:58.276738Z",
     "iopub.status.busy": "2024-12-25T02:47:58.276355Z",
     "iopub.status.idle": "2024-12-25T02:47:58.289307Z",
     "shell.execute_reply": "2024-12-25T02:47:58.288179Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022159,
     "end_time": "2024-12-25T02:47:58.291512",
     "exception": false,
     "start_time": "2024-12-25T02:47:58.269353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing make_walk_forward_feature.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_walk_forward_feature.py\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "import polars as pl\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# =============================================================================\n",
    "# メタ情報を作る関数: 指定のファイル一覧から min/max(date_id) を取得\n",
    "# =============================================================================\n",
    "def create_metadata_from_files(\n",
    "    files: List[str],\n",
    "    date_id: str = \"date_id\",\n",
    "):\n",
    "    \"\"\"\n",
    "    files の一覧を走査し、各ファイルから date_id の min/max を取得。\n",
    "    返り値: List[ (index, min_date, max_date) ]\n",
    "    \"\"\"\n",
    "    meta = []\n",
    "    for i, fpath in enumerate(sorted(files)):\n",
    "        # Parquet読み込み（形式固定例）\n",
    "        df_all = pl.read_parquet(fpath, columns=[date_id])\n",
    "        min_id = df_all.select(pl.col(date_id).min()).item()\n",
    "        max_id = df_all.select(pl.col(date_id).max()).item()\n",
    "        meta.append((i, min_id, max_id))\n",
    "\n",
    "    print(\"meta: partition_index, min_date, max_date =\", meta)\n",
    "    return meta\n",
    "\n",
    "def partitions_for_range(\n",
    "    meta: List[tuple],\n",
    "    start_id: int,\n",
    "    end_id: int\n",
    "):\n",
    "    \"\"\"\n",
    "    meta: [(index, min_date, max_date), ...]\n",
    "    指定した日付範囲 [start_id, end_id] と重なるファイルの index を返す\n",
    "    \"\"\"\n",
    "    needed = []\n",
    "    for (idx, mini, maxi) in meta:\n",
    "        if maxi >= start_id and mini <= end_id:\n",
    "            needed.append(idx)\n",
    "    return needed\n",
    "\n",
    "# =============================================================================\n",
    "# シンプルにParquetだけ読み書きするヘルパー\n",
    "# =============================================================================\n",
    "def _read_parquet_file(\n",
    "    fpath: str,\n",
    "    cols_to_read: List[str],\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Parquetを指定列だけ読み込む\n",
    "    \"\"\"\n",
    "    return pl.read_parquet(fpath, columns=cols_to_read)\n",
    "\n",
    "def _write_parquet_file(\n",
    "    df: pl.DataFrame,\n",
    "    out_path: str,\n",
    "    compression: str = \"zstd\",\n",
    "    compression_level: int = 9\n",
    "):\n",
    "    \"\"\"\n",
    "    Parquet書き込み\n",
    "    \"\"\"\n",
    "    df.write_parquet(\n",
    "        out_path,\n",
    "        compression=compression,\n",
    "        compression_level=compression_level\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# 1) シンプル: trainのみ出力するウォークフォワード\n",
    "# =============================================================================\n",
    "def make_feature_walk_forward(\n",
    "    # 入力\n",
    "    files: List[str],         # ファイルパス一覧\n",
    "    output_base: str,         # 出力先ディレクトリ\n",
    "    # カラム指定\n",
    "    id_col: str,              # 例: \"symbol_id\"\n",
    "    date_id: str,             # 例: \"date_id\"\n",
    "    time_id: str,             # 例: \"time_id\"\n",
    "    pred_cols: List[str],     # ターゲット系\n",
    "    input_cols: List[str],    # 入力系\n",
    "    # パラメータ\n",
    "    train_length: int,\n",
    "    retroactive_size: int,\n",
    "    # 特徴量作成関数\n",
    "    add_feature_func\n",
    "):\n",
    "    \"\"\"\n",
    "    シンプル版: valid/train_shiftを使わないウォークフォワード風処理。\n",
    "    \"\"\"\n",
    "    original_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "    print(\"original_cols:\", original_cols)\n",
    "\n",
    "    # メタ情報作成\n",
    "    meta = create_metadata_from_files(files, date_id=date_id)\n",
    "    max_date_id_global = max(m[2] for m in meta)\n",
    "\n",
    "    start_of_train = 0\n",
    "    sprint_num = 1\n",
    "\n",
    "    while True:\n",
    "        if start_of_train > max_date_id_global:\n",
    "            break\n",
    "\n",
    "        train_start = start_of_train\n",
    "        train_end = start_of_train + train_length\n",
    "        if train_end > max_date_id_global + 1:\n",
    "            train_end = max_date_id_global + 1\n",
    "\n",
    "        extended_start = max(train_start - retroactive_size, 0)\n",
    "        required_start = extended_start\n",
    "        required_end   = train_end - 1\n",
    "\n",
    "        needed_partitions = partitions_for_range(meta, required_start, required_end)\n",
    "        if len(needed_partitions) == 0:\n",
    "            break\n",
    "\n",
    "        df_current = None\n",
    "        for idx in needed_partitions:\n",
    "            fpath = files[idx]\n",
    "            print(\"read\", fpath)\n",
    "            df_part = _read_parquet_file(fpath, cols_to_read=[\n",
    "                c for c in original_cols if c in pl.read_parquet(fpath, n_rows=1).columns\n",
    "            ])\n",
    "            if df_current is None:\n",
    "                df_current = df_part\n",
    "            else:\n",
    "                df_current = pl.concat([df_current, df_part], how=\"vertical\")\n",
    "\n",
    "        if df_current is None or len(df_current) == 0:\n",
    "            break\n",
    "\n",
    "        # フィルタ\n",
    "        df_current = df_current.filter(\n",
    "            (pl.col(date_id) >= extended_start) & (pl.col(date_id) < train_end)\n",
    "        )\n",
    "\n",
    "        print(f\"Sprint {sprint_num}: extended_start={extended_start}, train_end={train_end}, rows={len(df_current)}\")\n",
    "\n",
    "        # 特徴量作成\n",
    "        df_out, gen_feat, gen_pred = add_feature_func(\n",
    "            df_current,\n",
    "            pred_cols=pred_cols,\n",
    "            input_cols=input_cols,\n",
    "            original_cols=original_cols\n",
    "        )\n",
    "\n",
    "        # train_df\n",
    "        train_df = df_out.filter(\n",
    "            (pl.col(date_id) >= train_start) & (pl.col(date_id) < train_end)\n",
    "        )\n",
    "\n",
    "        # 出力\n",
    "        sprint_dir = os.path.join(output_base, f\"sprint{sprint_num}\")\n",
    "        os.makedirs(sprint_dir, exist_ok=True)\n",
    "        train_path = os.path.join(sprint_dir, \"train.parquet\")\n",
    "\n",
    "        train_df = train_df.collect()\n",
    "        _write_parquet_file(train_df, train_path)\n",
    "        print(f\"  -> wrote {train_path}, rows={len(train_df)}\")\n",
    "\n",
    "        del df_current, df_out, train_df\n",
    "        gc.collect()\n",
    "\n",
    "        start_of_train = train_end\n",
    "        sprint_num += 1\n",
    "\n",
    "    print(\"Done (make_feature_walk_forward).\")\n",
    "    gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 2) train+valid分割 + train_shiftありのウォークフォワード\n",
    "# =============================================================================\n",
    "def make_feature_falk_forward_train_valid(\n",
    "    # 入力\n",
    "    files: List[str],\n",
    "    output_base: str,\n",
    "    # カラム指定\n",
    "    id_col: str,\n",
    "    date_id: str,\n",
    "    time_id: str,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    # パラメータ\n",
    "    train_length: int,\n",
    "    valid_length: int,\n",
    "    train_shift: int,\n",
    "    retroactive_size: int,\n",
    "    # 特徴量作成関数\n",
    "    add_feature_func\n",
    "):\n",
    "    \"\"\"\n",
    "    従来の train+valid + train_shift があるウォークフォワード版。\n",
    "    \"\"\"\n",
    "    original_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "    print(\"original_cols:\", original_cols)\n",
    "\n",
    "    meta = create_metadata_from_files(files, date_id=date_id)\n",
    "    max_date_id_global = max(m[2] for m in meta)\n",
    "\n",
    "    start_of_train = 0\n",
    "    sprint_num = 1\n",
    "\n",
    "    prev_gen_feature_str = None\n",
    "    prev_gen_preds_str   = None\n",
    "\n",
    "    while True:\n",
    "        if start_of_train > max_date_id_global:\n",
    "            break\n",
    "\n",
    "        train_start = start_of_train\n",
    "        train_end   = start_of_train + train_length\n",
    "        valid_start = train_end\n",
    "        valid_end   = train_end + valid_length\n",
    "\n",
    "        if valid_start > max_date_id_global:\n",
    "            break\n",
    "        if valid_end > max_date_id_global + 1:\n",
    "            valid_end = max_date_id_global + 1\n",
    "\n",
    "        extended_start = max(train_start - retroactive_size, 0)\n",
    "        required_start = extended_start\n",
    "        required_end   = valid_end - 1\n",
    "\n",
    "        needed_partitions = partitions_for_range(meta, required_start, required_end)\n",
    "        if len(needed_partitions) == 0:\n",
    "            break\n",
    "\n",
    "        df_current = None\n",
    "        for idx in needed_partitions:\n",
    "            fpath = files[idx]\n",
    "            df_part = _read_parquet_file(fpath, cols_to_read=[\n",
    "                c for c in original_cols if c in pl.read_parquet(fpath, n_rows=1).columns\n",
    "            ])\n",
    "            if df_current is None:\n",
    "                df_current = df_part\n",
    "            else:\n",
    "                df_current = pl.concat([df_current, df_part], how=\"vertical\")\n",
    "\n",
    "        if df_current is None or len(df_current) == 0:\n",
    "            break\n",
    "\n",
    "        # filter\n",
    "        df_current = df_current.filter(\n",
    "            (pl.col(date_id) >= extended_start) & (pl.col(date_id) < valid_end)\n",
    "        )\n",
    "        print(f\"Retroactive={extended_start}, Train=[{train_start},{train_end}), Valid=[{valid_start},{valid_end}), rows={len(df_current)}\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        df_out, generated_feature, generated_preds = add_feature_func(\n",
    "            df_current,\n",
    "            pred_cols=pred_cols,\n",
    "            input_cols=input_cols,\n",
    "            original_cols=original_cols\n",
    "        )\n",
    "\n",
    "        # 前回と比較\n",
    "        current_gen_feature_str = repr(generated_feature)\n",
    "        current_gen_preds_str   = repr(generated_preds)\n",
    "        if prev_gen_feature_str is not None:\n",
    "            if current_gen_feature_str != prev_gen_feature_str:\n",
    "                raise Exception(\"generated_feature differs from previous sprint!\")\n",
    "        if prev_gen_preds_str is not None:\n",
    "            if current_gen_preds_str != prev_gen_preds_str:\n",
    "                raise Exception(\"generated_preds differs from previous sprint!\")\n",
    "        prev_gen_feature_str = current_gen_feature_str\n",
    "        prev_gen_preds_str   = current_gen_preds_str\n",
    "\n",
    "        train_df = df_out.filter((pl.col(date_id) >= train_start) & (pl.col(date_id) < train_end))\n",
    "        valid_df = df_out.filter((pl.col(date_id) >= valid_start) & (pl.col(date_id) < valid_end))\n",
    "\n",
    "        sprint_dir = os.path.join(output_base, f\"sprint{sprint_num}\")\n",
    "        os.makedirs(sprint_dir, exist_ok=True)\n",
    "\n",
    "        train_path = os.path.join(sprint_dir, \"train.parquet\")\n",
    "        valid_path = os.path.join(sprint_dir, \"valid.parquet\")\n",
    "\n",
    "        train_df = train_df.collect()\n",
    "        valid_df = valid_df.collect()\n",
    "        _write_parquet_file(train_df, train_path)\n",
    "        _write_parquet_file(valid_df, valid_path)\n",
    "        print(f\"Sprint {sprint_num}: Train=[{train_start},{train_end}), Valid=[{valid_start},{valid_end})\")\n",
    "        print(f\"  -> train={train_path}, rows={len(train_df)}\")\n",
    "        print(f\"  -> valid={valid_path}, rows={len(valid_df)}\")\n",
    "\n",
    "        del df_current, df_out, train_df, valid_df\n",
    "        gc.collect()\n",
    "\n",
    "        start_of_train += train_shift\n",
    "        sprint_num += 1\n",
    "\n",
    "    print(\"Done (make_feature_falk_forward_train_valid).\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabffbf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:58.304326Z",
     "iopub.status.busy": "2024-12-25T02:47:58.303619Z",
     "iopub.status.idle": "2024-12-25T02:47:58.315469Z",
     "shell.execute_reply": "2024-12-25T02:47:58.314140Z"
    },
    "papermill": {
     "duration": 0.02102,
     "end_time": "2024-12-25T02:47:58.317875",
     "exception": false,
     "start_time": "2024-12-25T02:47:58.296855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing time_serise_feature.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile time_serise_feature.py\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "\n",
    "def create_stat_features_by(\n",
    "    ldf: pl.LazyFrame,\n",
    "    cols: List[str],\n",
    "    key_by: List[str],\n",
    "    agg_funcs: List[str],\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    ldf を key_by で group_by し、cols に対して指定された agg_funcs を実行する汎用関数。\n",
    "    例:\n",
    "      agg_funcs に [\"mean\",\"std\",\"min\",\"max\",\"median\",\"sum\",\"count\",\"n_unique\",\n",
    "                    \"last\",\"first\",\"skew\",\"kurtosis\",\"cv\", \"q0.25\",\"q0.75\" ...]\n",
    "      のような文字列を指定できる。\n",
    "\n",
    "    戻り値:\n",
    "      (ldf_agg, generated_cols):\n",
    "        ldf_agg : group_by(key_by).agg(...) の結果 (key_by + 各集計列 を持つ LazyFrame)\n",
    "        generated_cols : 新しく生成された列名のリスト\n",
    "\n",
    "    注意:\n",
    "      - \"q0.25\" 等のquantile形式は \"q0.x\" のフォーマットで xをfloatとして解釈し expr.quantile(x)。\n",
    "      - \"cv\" (coefficient of variation) = std/mean (mean=0に注意)\n",
    "      - skew, kurtosis は Polars のバージョンによっては使えない場合あり\n",
    "      - rolling_meanなどのウィンドウ関数はこのgroup_by集約とは別物\n",
    "\n",
    "    例:\n",
    "      aggregator_mapに含まれる文字列一覧を \"agg_funcs\" で指定\n",
    "        -> \"mean\",\"std\",\"cv\",\"q0.25\",\"skew\"など\n",
    "    \"\"\"\n",
    "\n",
    "    # Polarsでサポートする単一スカラー集約を文字列→lambda で定義\n",
    "    def parse_quantile(alias: str, cexpr: pl.Expr) -> pl.Expr:\n",
    "        # 例: alias == \"q0.25\" → 0.25\n",
    "        #     alias == \"q0.75\" → 0.75\n",
    "        m = re.match(r\"q0\\.(\\d+)\", alias)  # 例: \"q0.25\" -> group(1)==\"25\"\n",
    "        if not m:\n",
    "            # 不正形式ならそのまま col\n",
    "            return cexpr\n",
    "        float_str = \"0.\" + m.group(1)\n",
    "        q = float(float_str)\n",
    "        return cexpr.quantile(q)  # quantile(0.25 / 0.75 etc.)\n",
    "\n",
    "    aggregator_map = {\n",
    "        \"mean\":      lambda cexpr: cexpr.mean(),\n",
    "        \"std\":       lambda cexpr: cexpr.std(),\n",
    "        \"min\":       lambda cexpr: cexpr.min(),\n",
    "        \"max\":       lambda cexpr: cexpr.max(),\n",
    "        \"median\":    lambda cexpr: cexpr.median(),\n",
    "        \"sum\":       lambda cexpr: cexpr.sum(),\n",
    "        \"count\":     lambda cexpr: cexpr.count(),\n",
    "        \"n_unique\":  lambda cexpr: cexpr.n_unique(),\n",
    "        \"last\":      lambda cexpr: cexpr.last(),\n",
    "        \"first\":     lambda cexpr: cexpr.first(),\n",
    "        \"skew\":      lambda cexpr: cexpr.skew(),\n",
    "        \"kurtosis\":  lambda cexpr: cexpr.kurtosis(),\n",
    "        # 変動係数 (coefficient of variation)\n",
    "        \"cv\":        lambda cexpr: (cexpr.std() / cexpr.mean()),  # mean=0注意\n",
    "    }\n",
    "\n",
    "    agg_exprs = []\n",
    "    generated_cols: List[str] = []\n",
    "\n",
    "    for c in cols:\n",
    "        for f in agg_funcs:\n",
    "            alias_name = f\"{c}_{'_'.join(key_by)}_{f}\"\n",
    "\n",
    "            # 1) quantile系:  \"q0.25\", \"q0.50\" etc.\n",
    "            if f.startswith(\"q0.\"):\n",
    "                expr = parse_quantile(f, pl.col(c)).alias(alias_name)\n",
    "                agg_exprs.append(expr)\n",
    "                generated_cols.append(alias_name)\n",
    "                continue\n",
    "\n",
    "            # 2) aggregator_map にあるか？\n",
    "            aggregator = aggregator_map.get(f, None)\n",
    "            if aggregator is not None:\n",
    "                expr = aggregator(pl.col(c)).alias(alias_name)\n",
    "                agg_exprs.append(expr)\n",
    "                generated_cols.append(alias_name)\n",
    "            else:\n",
    "                raise Exception(f\"create_stat_features_by: not defined {f}\")\n",
    "\n",
    "    # group_by and agg\n",
    "    ldf_agg = ldf.group_by(key_by).agg(agg_exprs)\n",
    "\n",
    "    return ldf_agg, generated_cols\n",
    "\n",
    "# rolling/window系特徴量\n",
    "def add_avg_change_and_volatility(\n",
    "    ldf: pl.LazyFrame,\n",
    "    cols: List[str],\n",
    "    sort_keys: List[str],\n",
    "    group_keys: List[str],\n",
    "    n: int = 5,\n",
    "    use_log_return: bool = False,\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    LazyFrameに対して:\n",
    "      (1) t-nとの変化率 (log/ratio) ×100\n",
    "      (2) 過去n期間 rolling_std(ボラティリティ)\n",
    "    をcolsの各カラムについて計算し、(ldf_out, generated_cols)を返す。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ldf : pl.LazyFrame\n",
    "        処理対象のLazyFrame\n",
    "    cols : List[str]\n",
    "        計算対象の数値カラム ([\"feature_00\",\"feature_01\"]など)\n",
    "    n : int\n",
    "        shiftやrollingのウィンドウサイズ\n",
    "    use_log_return : bool\n",
    "        Trueならlogリターンで計算\n",
    "    sort_keys : List[str], optional\n",
    "        時系列順を保証するためにソートするカラムのリスト\n",
    "        例: [\"symbol_id\",\"date_id\",\"time_id\"]\n",
    "    group_keys : List[str], optional\n",
    "        shiftやrollingを「どのカラム単位でパーティション切る」か\n",
    "        例: [\"symbol_id\"]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ldf_out, generated_cols):\n",
    "      ldf_out       : 新しい列を追加したLazyFrame\n",
    "      generated_cols: 作成された列の名前一覧\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    - sort_keys が指定されれば ldf.sort(by=sort_keys) で並べ替えてから計算\n",
    "    - group_keys が指定されれば shift(n).over(group_keys), rolling_std(...).over(group_keys)\n",
    "      → 銘柄単位、などで独立計算が可能\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ソート\n",
    "    if sort_keys:\n",
    "        ldf = ldf.sort(by=sort_keys)\n",
    "\n",
    "    # group_keys が空でなければ partition 単位で over() を使う\n",
    "    def shift_n_expr(col: str, periods: int):\n",
    "        if group_keys:\n",
    "            return pl.col(col).shift(periods).over(group_keys)\n",
    "        else:\n",
    "            return pl.col(col).shift(periods)\n",
    "\n",
    "    def rolling_std_expr(col: str, window_size: int):\n",
    "        if group_keys:\n",
    "            return pl.col(col).rolling_std(window_size=window_size).over(group_keys)\n",
    "        else:\n",
    "            return pl.col(col).rolling_std(window_size=window_size)\n",
    "\n",
    "    generated_cols: List[str] = []\n",
    "    tmp_cols: List[str] = []\n",
    "\n",
    "    # (A) まず、一時列(1期間リターン)を作成\n",
    "    exprs_step1 = []\n",
    "    for col_name in cols:\n",
    "        tmp_return_col = f\"__tmp_return_{col_name}_{n}_{'log' if use_log_return else 'ratio'}\"\n",
    "        tmp_cols.append(tmp_return_col)\n",
    "\n",
    "        # 1期前の log 差 or ratio\n",
    "        if use_log_return:\n",
    "            # (log(col) - log(col.shift(1))) * 100\n",
    "            exprs_step1.append(\n",
    "                (\n",
    "                    (pl.col(col_name).log() - shift_n_expr(col_name, 1).log()) * 100\n",
    "                ).alias(tmp_return_col)\n",
    "            )\n",
    "        else:\n",
    "            # ((col / col.shift(1)) - 1) * 100\n",
    "            exprs_step1.append(\n",
    "                (\n",
    "                    ((pl.col(col_name) / shift_n_expr(col_name, 1)) - 1) * 100\n",
    "                ).alias(tmp_return_col)\n",
    "            )\n",
    "\n",
    "    # 一時列を先に追加\n",
    "    ldf = ldf.with_columns(exprs_step1)\n",
    "\n",
    "    # (B) 追加で (a) t-n との変化率, (b) rolling_std を計算\n",
    "    exprs_step2 = []\n",
    "    for col_name in cols:\n",
    "        avg_change_col = f\"{col_name}_avg_change_{n}\"\n",
    "        vol_col = f\"{col_name}_volatility_{n}\"\n",
    "        generated_cols.extend([avg_change_col, vol_col])\n",
    "\n",
    "        tmp_return_col = f\"__tmp_return_{col_name}_{n}_{'log' if use_log_return else 'ratio'}\"\n",
    "\n",
    "        # (a) t-n との変化率\n",
    "        if use_log_return:\n",
    "            # log リターン: (log(col) - log(col.shift(n))) * 100\n",
    "            exprs_step2.append(\n",
    "                (\n",
    "                    (pl.col(col_name).log() - shift_n_expr(col_name, n).log()) * 100\n",
    "                ).alias(avg_change_col)\n",
    "            )\n",
    "        else:\n",
    "            # ratio リターン: ((col / col.shift(n)) - 1) * 100\n",
    "            exprs_step2.append(\n",
    "                (\n",
    "                    ((pl.col(col_name) / shift_n_expr(col_name, n)) - 1) * 100\n",
    "                ).alias(avg_change_col)\n",
    "            )\n",
    "\n",
    "        # (b) rolling_std (過去 n 期間)\n",
    "        exprs_step2.append(\n",
    "            rolling_std_expr(tmp_return_col, n).alias(vol_col)\n",
    "        )\n",
    "\n",
    "    # 二段階目の計算を実行し、一時列を drop\n",
    "    ldf_out = (\n",
    "        ldf\n",
    "        .with_columns(exprs_step2)\n",
    "        .drop(tmp_cols)\n",
    "    )\n",
    "\n",
    "    return ldf_out, generated_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855d900d",
   "metadata": {
    "_cell_guid": "adfe1266-3f38-4319-8ea8-5e9ce9effc64",
    "_uuid": "ea4430b5-8fd6-47f2-b300-191a399462ad",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:58.330984Z",
     "iopub.status.busy": "2024-12-25T02:47:58.329974Z",
     "iopub.status.idle": "2024-12-25T02:47:58.340403Z",
     "shell.execute_reply": "2024-12-25T02:47:58.339313Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019119,
     "end_time": "2024-12-25T02:47:58.342558",
     "exception": false,
     "start_time": "2024-12-25T02:47:58.323439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_rolling_preds(df: pl.DataFrame, pred_cols: list[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    agg_expr = []\n",
    "    for c in pred_cols:\n",
    "        agg_expr.append(pl.col(c).first().alias(f\"{c}_prev_first\"))\n",
    "        agg_expr.append(pl.col(c).last().alias(f\"{c}_prev_last\"))\n",
    "        agg_expr.append(pl.col(c).max().alias(f\"{c}_prev_max\"))\n",
    "        agg_expr.append(pl.col(c).min().alias(f\"{c}_prev_min\"))\n",
    "        agg_expr.append(pl.col(c).std().alias(f\"{c}_prev_std\"))\n",
    "\n",
    "    return (\n",
    "        df.group_by([\"symbol_id\", \"date_id\"])\n",
    "        .agg(agg_expr)\n",
    "        .with_columns((pl.col(\"date_id\") + 1).alias(\"date_id_next\"))\n",
    "        .drop(\"date_id\")\n",
    "        .rename({\"date_id_next\": \"date_id\"})\n",
    "    )\n",
    "\n",
    "window_size = 7\n",
    "def add_feature(\n",
    "    df: pl.DataFrame, \n",
    "    pred_cols, \n",
    "    input_cols,\n",
    "    original_cols\n",
    ") -> (pl.DataFrame, pl.DataFrame):\n",
    "    # add_rolling_preds_funcを使用してprev_day_aggs相当を計算\n",
    "\n",
    "    df = df.lazy()\n",
    "    \n",
    "    df_prev = add_rolling_preds(df, pred_cols)\n",
    "    df = df.join(df_prev, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n",
    "    df = df.sort([\"symbol_id\",\"date_id\",\"time_id\"])\n",
    "\n",
    "    # pred_cols[0]でrolling_meanする例\n",
    "    first_pred = pred_cols[0]\n",
    "    rolling_col = f\"{first_pred}_prev_last\"\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col(rolling_col)\n",
    "        .rolling_mean(window_size=window_size)\n",
    "        .over(\"symbol_id\")\n",
    "        .alias(f\"{rolling_col}_{window_size}day_mean\")\n",
    "    )\n",
    "    return df, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb867a1c",
   "metadata": {
    "_cell_guid": "2ae1ade6-192a-48d4-b44e-7076d361c859",
    "_uuid": "aa78cf22-4193-49f0-887c-a37ad37cb705",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-25T02:47:58.354502Z",
     "iopub.status.busy": "2024-12-25T02:47:58.354075Z",
     "iopub.status.idle": "2024-12-25T02:48:02.036825Z",
     "shell.execute_reply": "2024-12-25T02:48:02.035178Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.691444,
     "end_time": "2024-12-25T02:48:02.039232",
     "exception": false,
     "start_time": "2024-12-25T02:47:58.347788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files ['/kaggle/working/test/partition_id=1/part-0.parquet', '/kaggle/working/test/partition_id=0/part-0.parquet']\n",
      "original_cols: ['symbol_id', 'date_id', 'time_id', 'responder_0', 'feature_00']\n",
      "meta: partition_index, min_date, max_date = [(0, 0, 2), (1, 3, 5)]\n",
      "Retroactive=0, Train=[0,2), Valid=[2,3), rows=0\n",
      "Sprint 1: Train=[0,2), Valid=[2,3)\n",
      "  -> train=test_make_feature_falk_forward_train_valid/sprint1/train.parquet, rows=0\n",
      "  -> valid=test_make_feature_falk_forward_train_valid/sprint1/valid.parquet, rows=0\n",
      "Retroactive=0, Train=[1,3), Valid=[3,4), rows=100\n",
      "Sprint 2: Train=[1,3), Valid=[3,4)\n",
      "  -> train=test_make_feature_falk_forward_train_valid/sprint2/train.parquet, rows=50\n",
      "  -> valid=test_make_feature_falk_forward_train_valid/sprint2/valid.parquet, rows=25\n",
      "Retroactive=1, Train=[2,4), Valid=[4,5), rows=100\n",
      "Sprint 3: Train=[2,4), Valid=[4,5)\n",
      "  -> train=test_make_feature_falk_forward_train_valid/sprint3/train.parquet, rows=50\n",
      "  -> valid=test_make_feature_falk_forward_train_valid/sprint3/valid.parquet, rows=25\n",
      "Retroactive=2, Train=[3,5), Valid=[5,6), rows=100\n",
      "Sprint 4: Train=[3,5), Valid=[5,6)\n",
      "  -> train=test_make_feature_falk_forward_train_valid/sprint4/train.parquet, rows=50\n",
      "  -> valid=test_make_feature_falk_forward_train_valid/sprint4/valid.parquet, rows=25\n",
      "Done (make_feature_falk_forward_train_valid).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from make_walk_forward_feature import make_feature_falk_forward_train_valid\n",
    "\n",
    "data_path = \"/kaggle/working/test\"\n",
    "parquet_files = glob.glob(os.path.join(data_path, \"partition_id=*\", \"*.parquet\"))\n",
    "\n",
    "output_base = \"test_make_feature_falk_forward_train_valid\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "test_train_length = 2\n",
    "test_valid_length = 1\n",
    "test_train_shift = 1\n",
    "test_retroactive_size = 1\n",
    "\n",
    "print(\"parquet_files\", parquet_files)\n",
    "\n",
    "make_feature_falk_forward_train_valid(\n",
    "    files=parquet_files,\n",
    "    output_base=output_base,\n",
    "    id_col=id_col[0],\n",
    "    date_id=date_id[0], \n",
    "    time_id=time_id[0],\n",
    "    pred_cols=[c[0] for c in pred_cols],\n",
    "    input_cols=[c[0] for c in input_cols],\n",
    "    train_length=test_train_length,\n",
    "    valid_length=test_valid_length,\n",
    "    train_shift=test_train_shift, \n",
    "    retroactive_size=test_retroactive_size,\n",
    "    add_feature_func=add_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003a769c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T02:48:02.053860Z",
     "iopub.status.busy": "2024-12-25T02:48:02.052649Z",
     "iopub.status.idle": "2024-12-25T02:48:02.441316Z",
     "shell.execute_reply": "2024-12-25T02:48:02.440183Z"
    },
    "papermill": {
     "duration": 0.39805,
     "end_time": "2024-12-25T02:48:02.443965",
     "exception": false,
     "start_time": "2024-12-25T02:48:02.045915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files ['/kaggle/working/test/partition_id=1/part-0.parquet', '/kaggle/working/test/partition_id=0/part-0.parquet']\n",
      "original_cols: ['symbol_id', 'date_id', 'time_id', 'responder_0', 'feature_00']\n",
      "meta: partition_index, min_date, max_date = [(0, 0, 2), (1, 3, 5)]\n",
      "read /kaggle/working/test/partition_id=1/part-0.parquet\n",
      "Sprint 1: extended_start=0, train_end=2, rows=0\n",
      "  -> wrote test_make_feature_walk_forward/sprint1/train.parquet, rows=0\n",
      "read /kaggle/working/test/partition_id=1/part-0.parquet\n",
      "read /kaggle/working/test/partition_id=0/part-0.parquet\n",
      "Sprint 2: extended_start=1, train_end=4, rows=75\n",
      "  -> wrote test_make_feature_walk_forward/sprint2/train.parquet, rows=50\n",
      "read /kaggle/working/test/partition_id=0/part-0.parquet\n",
      "Sprint 3: extended_start=3, train_end=6, rows=0\n",
      "  -> wrote test_make_feature_walk_forward/sprint3/train.parquet, rows=0\n",
      "Done (make_feature_walk_forward).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from make_walk_forward_feature import make_feature_walk_forward\n",
    "\n",
    "data_path = \"/kaggle/working/test\"\n",
    "parquet_files = glob.glob(os.path.join(data_path, \"partition_id=*\", \"*.parquet\"))\n",
    "\n",
    "output_base = \"test_make_feature_walk_forward\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "test_train_length = 2\n",
    "test_retroactive_size = 1\n",
    "\n",
    "print(\"parquet_files\", parquet_files)\n",
    "\n",
    "make_feature_walk_forward(\n",
    "    files=parquet_files,\n",
    "    output_base=output_base,\n",
    "    id_col=id_col[0],\n",
    "    date_id=date_id[0], \n",
    "    time_id=time_id[0],\n",
    "    pred_cols=[c[0] for c in pred_cols],\n",
    "    input_cols=[c[0] for c in input_cols],\n",
    "    train_length=test_train_length,\n",
    "    retroactive_size=test_retroactive_size,\n",
    "    add_feature_func=add_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01002991",
   "metadata": {
    "papermill": {
     "duration": 0.005526,
     "end_time": "2024-12-25T02:48:02.456497",
     "exception": false,
     "start_time": "2024-12-25T02:48:02.450971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Jane Street Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a1278e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T02:48:02.470046Z",
     "iopub.status.busy": "2024-12-25T02:48:02.469633Z",
     "iopub.status.idle": "2024-12-25T02:48:02.484369Z",
     "shell.execute_reply": "2024-12-25T02:48:02.483316Z"
    },
    "papermill": {
     "duration": 0.024168,
     "end_time": "2024-12-25T02:48:02.486473",
     "exception": false,
     "start_time": "2024-12-25T02:48:02.462305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jane_street_real.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jane_street_real.py\n",
    "import polars as pl\n",
    "from typing import List, Tuple\n",
    "from time_serise_feature import *\n",
    "\n",
    "target_9_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9\"\n",
    "max_time_id = pl.read_parquet(target_9_path).select(pl.col(\"time_id\").max()).item()\n",
    "segment_size = (max_time_id + 1) // 4  # 4等分のサイズを計算\n",
    "\n",
    "# 4分割の閾値を計算\n",
    "threshold1 = segment_size\n",
    "threshold2 = segment_size * 2\n",
    "threshold3 = segment_size * 3\n",
    "\n",
    "print(\"threshold\", threshold1, threshold2, threshold3)\n",
    "\n",
    "def feature_today_rolling_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n",
    "    \"\"\"\n",
    "    featureに関する特徴量を計算\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "    \n",
    "\n",
    "    # 当日特徴量\n",
    "    ## rolling特徴量\n",
    "    ldf, generated_cols = add_avg_change_and_volatility(ldf, input_cols, n = 4, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    ldf, generated_cols = add_avg_change_and_volatility(ldf, input_cols, n = 28, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    return ldf, generated_features, generated_preds\n",
    "\n",
    "def feature_today_stat_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n",
    "    \"\"\"\n",
    "    featureに関する特徴量を計算\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "    \n",
    "    # date_id, time_id単位\n",
    "    df_datetime, datetime_feature_cols = create_stat_features_by(ldf=ldf, cols=input_cols, key_by=[\"date_id\", \"time_id\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(datetime_feature_cols)\n",
    "    \n",
    "    df_datetime, datetime_feature_cols = add_avg_change_and_volatility(df_datetime, datetime_feature_cols, n = 7, group_keys=[\"date_id\"], sort_keys=[\"time_id\"])\n",
    "    generated_features.extend(datetime_feature_cols)\n",
    "    \n",
    "    return df_datetime, generated_features, generated_preds\n",
    "\n",
    "def create_raw_responder_lag(\n",
    "    df: pl.DataFrame,\n",
    "    pred_cols: List[str]\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    1) 'symbol_id', 'date_id', 'w' と pred_cols の列のみ取り出す\n",
    "    2) w * responder_6 の列を新たに作る (\"w_responder_6\")\n",
    "    3) w を削除\n",
    "    4) date_id を (date_id - 1) シフト\n",
    "    5) pred_cols + [\"w_responder_6\"] を \"_lag_1\" にリネーム\n",
    "    戻り値:\n",
    "      (lazy_frame, generated_cols):\n",
    "        lazy_frame : 上記処理結果をLazyFrameに変換したもの\n",
    "        generated_cols : リネーム後の列名(例: \"responder_6_lag_1\", \"w_responder_6_lag_1\"など)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) 必要列のみ抽出 (Eager DataFrame)\n",
    "    target_cols = [\"symbol_id\", \"date_id\", \"time_id_group\", \"weight\"] + pred_cols\n",
    "    df_lags = df.select(target_cols)\n",
    "\n",
    "    # 2) w * responder を作成\n",
    "    w_responder_cols = []\n",
    "    for c in pred_cols:\n",
    "        w_c = f\"w_{c}\"\n",
    "        w_responder_cols.append(w_c)\n",
    "        df_lags = df_lags.with_columns(\n",
    "            (pl.col(\"weight\") * pl.col(c)).alias(w_c)\n",
    "        )\n",
    "    new_pred_cols = pred_cols + w_responder_cols\n",
    "    \n",
    "    # 3) wを削除\n",
    "    df_lags = df_lags.drop([\"weight\"])\n",
    "\n",
    "    # 4) date_id = date_id + 1\n",
    "    df_lags = df_lags.with_columns(\n",
    "        (pl.col(\"date_id\") + 1).alias(\"date_id\")\n",
    "    )\n",
    "\n",
    "    # 5) 上記 new_pred_cols を \"colName_lag_1\" にリネーム\n",
    "    rename_map = {}\n",
    "    for c in new_pred_cols:\n",
    "        rename_map[c] = f\"{c}_lag_1\"\n",
    "\n",
    "    df_lags = df_lags.rename(rename_map)\n",
    "\n",
    "    # リネーム後の列名一覧\n",
    "    generated_cols = list(rename_map.values())\n",
    "    \n",
    "    keys = [\"symbol_id\", \"date_id\", \"time_id_group\"]\n",
    "    return df_lags, generated_cols, keys\n",
    "\n",
    "def create_responder_lag_feature(ldf, cols):\n",
    "    generated_features = []\n",
    "    df_raw_lags, lags_cols, raw_lags_keys = create_raw_responder_lag(ldf, cols)\n",
    "\n",
    "    # symbol_id, date_id, time_id_group 単位\n",
    "    df_lags, gen_cols_symdate_group = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"symbol_id\", \"date_id\", \"time_id_group\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_symdate_group)\n",
    "\n",
    "    ## rolling特徴量\n",
    "    ### 4 time_id_groupまで見る\n",
    "    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 4, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    ### 4 * 7 time_id_groupまで見る\n",
    "    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 28, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    # date_id単位でラグ列を集計\n",
    "    df_lags_dateid, gen_cols_date = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_date)\n",
    "\n",
    "    df_lags_dateid, generated_cols = add_avg_change_and_volatility(df_lags_dateid, gen_cols_date, n = 7, group_keys=[\"date_id\"], sort_keys=[])\n",
    "    generated_features.extend(generated_cols)\n",
    "    \n",
    "    df_lags = df_lags.join(df_lags_dateid, on=[\"date_id\"], how=\"left\")\n",
    "\n",
    "    # symbol_id, date_id単位\n",
    "    df_lags_symdate, gen_cols_symdate = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols,key_by=[\"symbol_id\", \"date_id\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_symdate)\n",
    "    \n",
    "    df_lags_symdate, generated_cols = add_avg_change_and_volatility(df_lags_symdate, gen_cols_symdate, n = 7, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    df_lags = df_lags.join(df_lags_symdate, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n",
    "\n",
    "    # date_id, time_group_id単位\n",
    "    df_lags_datetime, gen_cols_datetime = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\", \"time_id_group\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_datetime)\n",
    "   \n",
    "    df_lags_datetime, generated_cols = add_avg_change_and_volatility(df_lags_datetime, gen_cols_datetime, n = 7, group_keys=[\"date_id\"], sort_keys=[\"time_id_group\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "    \n",
    "    df_lags = df_lags.join(df_lags_datetime, on=[\"date_id\",\"time_id_group\"], how=\"left\")\n",
    "\n",
    "    return df_lags, generated_features\n",
    "\n",
    "def responder_lag_feature_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n",
    "    ## [lags.parquetの扱い]\n",
    "    ## 当日と前日のtiem_idが異なる可能性なので、単純なdate_id, time_idのlagは有用ではない\n",
    "    ## また、前日の全てのlagは当日のtime_id == 0のタイミングで提供される\n",
    "    ## すなわち、lagsが提供された場合で統計量の処理を行い、date_id - 1のresponderの統計量として扱う。\n",
    "    \n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "\n",
    "    # responder-lag特徴量\n",
    "    ldf, cols = create_responder_lag_feature(ldf, pred_cols)\n",
    "    generated_features.extend(cols)\n",
    "\n",
    "    return ldf, generated_features, generated_preds\n",
    "\n",
    "\n",
    "def create_raw_feature_lag(\n",
    "    df: pl.DataFrame,\n",
    "    pred_cols: List[str]\n",
    ") -> Tuple[pl.LazyFrame, List[str]]:\n",
    "    target_cols = [\"symbol_id\", \"date_id\", \"time_id_group\"] + pred_cols\n",
    "    df_lags = df.select(target_cols)\n",
    "\n",
    "    # 4) date_id = date_id + 1\n",
    "    df_lags = df_lags.with_columns(\n",
    "        (pl.col(\"date_id\") + 1).alias(\"date_id\")\n",
    "    )\n",
    "    rename_map = {}\n",
    "    for c in pred_cols:\n",
    "        rename_map[c] = f\"{c}_lag_1\"\n",
    "\n",
    "    df_lags = df_lags.rename(rename_map)\n",
    "\n",
    "    # リネーム後の列名一覧\n",
    "    generated_cols = list(rename_map.values())\n",
    "    \n",
    "    keys = [\"symbol_id\", \"date_id\", \"time_id_group\"]\n",
    "    return df_lags, generated_cols, keys\n",
    "\n",
    "def create_feature_lag(ldf, cols):\n",
    "    generated_features = []\n",
    "    df_raw_lags, lags_cols, raw_lags_keys = create_raw_feature_lag(ldf, cols)\n",
    "\n",
    "    # symbol_id, date_id, time_id_group 単位\n",
    "    df_lags, gen_cols_symdate_group = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"symbol_id\", \"date_id\", \"time_id_group\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_symdate_group)\n",
    "\n",
    "    ## rolling特徴量\n",
    "    ### 4 time_id_groupまで見る\n",
    "    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 4, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    ### 4 * 7 time_id_groupまで見る\n",
    "    df_lags, generated_cols = add_avg_change_and_volatility(df_lags, gen_cols_symdate_group, n = 28, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\", \"time_id_group\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    # date_id単位でラグ列を集計\n",
    "    df_lags_dateid, gen_cols_date = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\"], agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_date)\n",
    "\n",
    "    df_lags_dateid, generated_cols = add_avg_change_and_volatility(df_lags_dateid, gen_cols_date, n = 7, group_keys=[\"date_id\"], sort_keys=[])\n",
    "    generated_features.extend(generated_cols)\n",
    "    \n",
    "    df_lags = df_lags.join(df_lags_dateid, on=[\"date_id\"], how=\"left\")\n",
    "\n",
    "    # symbol_id, date_id単位\n",
    "    df_lags_symdate, gen_cols_symdate = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols,key_by=[\"symbol_id\", \"date_id\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_symdate)\n",
    "    \n",
    "    df_lags_symdate, generated_cols = add_avg_change_and_volatility(df_lags_symdate, gen_cols_symdate, n = 7, group_keys=[\"symbol_id\"], sort_keys=[\"date_id\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    df_lags = df_lags.join(df_lags_symdate, on=[\"symbol_id\", \"date_id\"], how=\"left\")\n",
    "\n",
    "    # date_id, time_group_id単位\n",
    "    df_lags_datetime, gen_cols_datetime = create_stat_features_by(ldf=df_raw_lags, cols=lags_cols, key_by=[\"date_id\", \"time_id_group\"],agg_funcs=[\"mean\", \"std\", \"skew\", \"kurtosis\", \"cv\", \"last\"])\n",
    "    generated_features.extend(gen_cols_datetime)\n",
    "   \n",
    "    df_lags_datetime, generated_cols = add_avg_change_and_volatility(df_lags_datetime, gen_cols_datetime, n = 7, group_keys=[\"date_id\"], sort_keys=[\"time_id_group\"])\n",
    "    generated_features.extend(generated_cols)\n",
    "    \n",
    "    df_lags = df_lags.join(df_lags_datetime, on=[\"date_id\",\"time_id_group\"], how=\"left\")\n",
    "\n",
    "    return df_lags, generated_features\n",
    "\n",
    "def feature_lag_func(\n",
    "    ldf: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n",
    "    \"\"\"\n",
    "    featureに関する特徴量を計算\n",
    "    \"\"\"\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "    \n",
    "    #前日特徴量(symbol_id, date_id -1, time_group_id)\n",
    "    ldf, generated_cols = create_feature_lag(ldf, input_cols)\n",
    "    generated_features.extend(generated_cols)\n",
    "\n",
    "    return ldf, generated_features, generated_preds\n",
    "\n",
    "def add_feature_func(\n",
    "    df: pl.DataFrame,\n",
    "    pred_cols: List[str],\n",
    "    input_cols: List[str],\n",
    "    original_cols: List[str] | None = None\n",
    ") -> Tuple[pl.DataFrame, List[List[str]], List[str]]:\n",
    "\n",
    "    generated_features: List[str] = []\n",
    "    generated_preds: List[str] = []\n",
    "\n",
    "    ldf = df.lazy()\n",
    "    ldf = ldf.sort([\"symbol_id\", \"date_id\", \"time_id\"])\n",
    "\n",
    "    # time_id_group (4分割)\n",
    "    ldf = ldf.with_columns(\n",
    "        pl.when(pl.col(\"time_id\") < threshold1).then(0)\n",
    "          .when(pl.col(\"time_id\") < threshold2).then(1)\n",
    "          .when(pl.col(\"time_id\") < threshold3).then(2)\n",
    "          .otherwise(3)\n",
    "          .cast(pl.Int32)\n",
    "          .alias(\"time_id_group\")\n",
    "    )\n",
    "    generated_features.extend([\"time_id_group\"])\n",
    "\n",
    "    # id, date_id, time(feature, preds付き) -> id, date_id, time(feature, preds付き)\n",
    "    ldf_feature, f, p = feature_today_rolling_func(ldf, pred_cols, input_cols, original_cols)\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "\n",
    "    # id, date_id, time(feature, preds付き) -> date_id, time_id\n",
    "    ldf_feature_today_stat, f, p = feature_today_stat_func(ldf, pred_cols, input_cols, original_cols)\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "    ldf_feature = ldf_feature.join(ldf_feature_today_stat, on=[\"date_id\", \"time_id\"], how=\"left\")\n",
    "\n",
    "    # id, date_id, time(feature, preds付き) -> [\"symbol_id\", \"date_id\", \"time_id_group\"]\n",
    "    ldf_responder_lag_feature, f, p = responder_lag_feature_func(ldf, pred_cols, input_cols, original_cols)\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "    ldf_feature = ldf_feature.join(ldf_responder_lag_feature, on=[\"symbol_id\", \"date_id\", \"time_id_group\"], how=\"left\")\n",
    "  \n",
    "    # id, date_id, time(feature, preds付き) -> \n",
    "    ldf_feature_lag, f, p = feature_lag_func(ldf, pred_cols, input_cols, original_cols)\n",
    "    generated_features.extend(f)\n",
    "    generated_preds.extend(p)\n",
    "    ldf_feature = ldf_feature.join(ldf_feature_lag, on=[\"symbol_id\", \"date_id\", \"time_id_group\"], how=\"left\")\n",
    "\n",
    "    \n",
    "    return ldf_feature, generated_features, generated_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff4f7b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T02:48:02.499229Z",
     "iopub.status.busy": "2024-12-25T02:48:02.498787Z",
     "iopub.status.idle": "2024-12-25T02:48:02.506651Z",
     "shell.execute_reply": "2024-12-25T02:48:02.505523Z"
    },
    "papermill": {
     "duration": 0.016798,
     "end_time": "2024-12-25T02:48:02.508843",
     "exception": false,
     "start_time": "2024-12-25T02:48:02.492045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport glob\\nimport polars as pl\\nimport numpy as np\\nfrom jane_street_real import *\\n\\noutput_base = \"datasets\"\\nos.makedirs(output_base, exist_ok=True)\\n\\n#data_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\"\\n#parquet_files = sorted(glob.glob(os.path.join(data_path, \"partition_id=*\", \"*.parquet\")))\\nparquet_files = [\\n    \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\"\\n]\\n\\n# 入力変数選択（事前定義）\\nid_col = \"symbol_id\"    # ID列\\ndate_id = \"date_id\"     # 日付列\\ntime_id = \"time_id\"     # 時間列\\npred_cols = [f\"responder_{i}\" for i in range(9)]  # 目的変数\\n\\n# 最初のファイルからカラム一覧とdtype取得\\nif not parquet_files:\\n    raise FileNotFoundError(\"No parquet files found in the specified data path.\")\\n\\ntarget_9_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9\"\\ndf_test = pl.read_parquet(target_9_path, n_rows=1)\\nall_cols = df_test.columns\\nschema = df_test.schema  # {col_name: polars.DataType}\\n\\n# 除外する列\\nexclude_cols = {id_col, date_id, time_id} | set(pred_cols)\\n\\n# input_colsを自動的に決定\\ninput_cols = []\\nfor col in all_cols:\\n    if col not in exclude_cols:\\n        polars_dtype = schema[col]\\n        print(f\"{col}, {polars_dtype}\")\\n        input_cols.append(col)\\n\\n# target_cols作成\\ntarget_cols = [id_col, date_id, time_id] + pred_cols + input_cols\\nprint(\"target_cols:\", target_cols)\\n\\ntrain_length = 30\\nretroactive_size = 30 # rollingやwindowの最大lags数\\nprint(\"parquet_files\", parquet_files)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from jane_street_real import *\n",
    "\n",
    "output_base = \"datasets\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "#data_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\"\n",
    "#parquet_files = sorted(glob.glob(os.path.join(data_path, \"partition_id=*\", \"*.parquet\")))\n",
    "parquet_files = [\n",
    "    \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\"\n",
    "]\n",
    "\n",
    "# 入力変数選択（事前定義）\n",
    "id_col = \"symbol_id\"    # ID列\n",
    "date_id = \"date_id\"     # 日付列\n",
    "time_id = \"time_id\"     # 時間列\n",
    "pred_cols = [f\"responder_{i}\" for i in range(9)]  # 目的変数\n",
    "\n",
    "# 最初のファイルからカラム一覧とdtype取得\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(\"No parquet files found in the specified data path.\")\n",
    "\n",
    "target_9_path = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9\"\n",
    "df_test = pl.read_parquet(target_9_path, n_rows=1)\n",
    "all_cols = df_test.columns\n",
    "schema = df_test.schema  # {col_name: polars.DataType}\n",
    "\n",
    "# 除外する列\n",
    "exclude_cols = {id_col, date_id, time_id} | set(pred_cols)\n",
    "\n",
    "# input_colsを自動的に決定\n",
    "input_cols = []\n",
    "for col in all_cols:\n",
    "    if col not in exclude_cols:\n",
    "        polars_dtype = schema[col]\n",
    "        print(f\"{col}, {polars_dtype}\")\n",
    "        input_cols.append(col)\n",
    "\n",
    "# target_cols作成\n",
    "target_cols = [id_col, date_id, time_id] + pred_cols + input_cols\n",
    "print(\"target_cols:\", target_cols)\n",
    "\n",
    "train_length = 30\n",
    "retroactive_size = 30 # rollingやwindowの最大lags数\n",
    "print(\"parquet_files\", parquet_files)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "726020b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T02:48:02.521799Z",
     "iopub.status.busy": "2024-12-25T02:48:02.521422Z",
     "iopub.status.idle": "2024-12-25T02:48:02.527929Z",
     "shell.execute_reply": "2024-12-25T02:48:02.526871Z"
    },
    "papermill": {
     "duration": 0.015695,
     "end_time": "2024-12-25T02:48:02.530227",
     "exception": false,
     "start_time": "2024-12-25T02:48:02.514532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noutput_base = \"feature_datasets\"\\nos.makedirs(output_base, exist_ok=True)\\n\\nmake_feature_walk_forward(\\n    files=parquet_files,\\n    output_base=output_base,\\n    id_col=id_col,\\n    date_id=date_id,\\n    time_id=time_id,\\n    pred_cols=pred_cols,\\n    input_cols=input_cols,\\n    train_length=train_length,\\n    retroactive_size=retroactive_size,\\n    add_feature_func=add_feature_func\\n)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "output_base = \"feature_datasets\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "make_feature_walk_forward(\n",
    "    files=parquet_files,\n",
    "    output_base=output_base,\n",
    "    id_col=id_col,\n",
    "    date_id=date_id,\n",
    "    time_id=time_id,\n",
    "    pred_cols=pred_cols,\n",
    "    input_cols=input_cols,\n",
    "    train_length=train_length,\n",
    "    retroactive_size=retroactive_size,\n",
    "    add_feature_func=add_feature_func\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.852621,
   "end_time": "2024-12-25T02:48:03.158134",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-25T02:47:54.305513",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
